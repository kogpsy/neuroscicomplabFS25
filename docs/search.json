[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Neurowissenschaft im Computerlab",
    "section": "",
    "text": "Herzlich Willkommen\nHier finden Sie das Wichtigste zum Kurs FS2025.",
    "crumbs": [
      "Herzlich Willkommen"
    ]
  },
  {
    "objectID": "index.html#ilias",
    "href": "index.html#ilias",
    "title": "Neurowissenschaft im Computerlab",
    "section": "Ilias",
    "text": "Ilias\nUnter diesen Links finden Sie die Iliasgruppen:\nILIAS-Gruppe Freitag 08.15-10.00 👉 468703-FS2025-0\nILIAS-Gruppe Freitag 10.15-12.00 👉 468703-FS2025-1",
    "crumbs": [
      "Herzlich Willkommen"
    ]
  },
  {
    "objectID": "index.html#kursvoraussetzungen",
    "href": "index.html#kursvoraussetzungen",
    "title": "Neurowissenschaft im Computerlab",
    "section": "Kursvoraussetzungen",
    "text": "Kursvoraussetzungen\nWir werden mit der Programmiersprache R und zu einem kleinen Teil mit Python arbeiten. Sie benötigen in der Veranstaltung deshalb einen eigenen Laptop (Tablets sind nicht geeignet!) mit ca. 20 GB freiem Speicherplatz und mit einer installierten (aktuellen) Version von R und RStudio (Link zum Download von R und RStudio).\nR Kenntnisse (gemäss Statistik I-IV in Psychologie) werden vorausgesetzt. Zur Auffrischung dient folgender Link (https://methodenlehre.github.io/einfuehrung-in-R/) oder für Fortgeschrittene die Bücher „Advanced R” und „R for Data Scientists” von Hadley Wickham.\nZusätzlich dient Übung 2 zur Auffrischung der Vorkenntnisse auf die im späteren Verlauf des Kurses aufgebaut wird.\nBitte installieren Sie bis zum 2. Kurstermine folgende Software:\n\nNeue Versionen von R und RStudio\nPsychoPy\nJASP",
    "crumbs": [
      "Herzlich Willkommen"
    ]
  },
  {
    "objectID": "index.html#leistungsnachweise",
    "href": "index.html#leistungsnachweise",
    "title": "Neurowissenschaft im Computerlab",
    "section": "Leistungsnachweise",
    "text": "Leistungsnachweise\nDer Kurs entspricht 5 ETCS. Dafür müssen 3 Bereiche erfüllt werden:\n\nAnwesenheit im Kurs\nbestandene Übungen\nbestandenes Abschlussquiz\n\nAlle Leistungsnachweise werden in den Veranstaltungen angekündigt. Die Termine für die Leistungsnachweise finden Sie unter Termine der Leistungsnachweise.\n\nAnwesenheit\nDie Anwesenheit im Kurs wird vorausgesetzt, an 2 Terminen darf gefehlt werden.\nDas Online-Skript erlaubt das Nacharbeiten des wichtigsten Stoffes im Eigenstudium, wir können jedoch nicht für die Vollständigkeit garantieren. Hilfestellung beim Programmieren und Verstehen der Inhalte erhalten Sie während der Kurszeiten. Aus zeitlichen Gründen können wir keine ausführliche Beantwortung von Fragen zum Kursinhalt per E-Mail anbieten. Bitte stellen Sie Ihre Fragen in der Veranstaltung - auch Ihre Mitstudierenden werden davon profitieren, oft haben mehrere Personen dieselbe Frage.\n\n\nÜbungen\nEs gibt fünf Übungen.\n\nDie Übungen werden auf der Website aufgeschaltet und in der Veranstaltung erklärt.\nDie Ergebnisse der Übungen müssen in den entsprechenden Ordner auf ILIAS hochgeladen werden. Je nach Umfang der Übung wird die Zeit bis zur Abgabe unterschiedlich ausfallen. Sie wird jedoch immer mindestens zwei Wochen betragen.\nAusser Übung 2 müssen alle Übungen abgegeben und als bestanden bewertet werden. Wird eine ungenügende Übungen abgegeben erhalten Sie eine zweite Frist für die Abgabe oder einen Zusatzauftrag.\nÜbungen dürfen alleine oder in Gruppen von max. 3 Personen erledigt werden. Alle Personen müssen die Übung abgeben. Damit wir sehen, welche Übungsabgaben zusammengehören: Nennen Sie das File mit der Aufgabe und allen Initialen der Gruppe. Z.B. uebung_3_GW_EW.R\nAlle Übungen müssen bestanden werden. Ob die Übung bestanden wurde, sehen Sie auf Ilias. Bei Nicht-Bestehen muss die Übung nochmals abgeben werden oder eine Zusatzaufgabe erledigt werden.\n\nVerwenden von LLMs für Übungen: Sie dürfen LLMs gerne als Tool nutzen, um die Übung zu bearbeiten. Es liegt in Ihrer Verantwortung, den Output von LLMs vor der Abgabe gründlich zu prüfen. Halten Sie sich an Folgendes:\n\nLLMs geben Code aus. Aber sogar wenn dieser problemlos ausgeführt werden kann, muss genau überprüft werden, ob der Code das Richtige tut. Dieses Überprüfen kann unter Umständen genau so lange dauern, wie das Lesen und Verstehen der Dokumentation.\nDas Überprüfen von Code erfordert gewisse Grundkenntnisse. Das Verwenden von LLMs ersetzt kein Lernaufwand.1\nDas direkte Verwenden von Code ohne kompetente Prüfung ist in der Forschung unethisch!\nEs dürfen keine sensiblen Daten eingegeben werden bzw. auch keine Datensätze die nicht öffentlich sind.\n\n\n\nAbschlussquiz\nDas Abschlussquiz wird gegen Ende des Semesters auf Ilias freigeschaltet und dient dazu das Gelernte zu prüfen und so Feedback zu geben, wie gut man die Lerninhalte erinnert. Sie haben die Möglichkeit das Quiz mehrmals zu wiederholen.",
    "crumbs": [
      "Herzlich Willkommen"
    ]
  },
  {
    "objectID": "index.html#datacamp",
    "href": "index.html#datacamp",
    "title": "Neurowissenschaft im Computerlab",
    "section": "DataCamp",
    "text": "DataCamp\nDataCamp ist eine Online-Lernplattform, welche sich auf Data Science und Datenanalyse konzentriert. Es bietet interaktive Kurse, Tutorials und Projekte in verschiedenen Programmiersprachen wie Python, R und SQL auf unterschiedlichen Niveaus an; sowohl für Anfänger als auch für Fortgeschrittene gibt es ein breites Angebot.\nIm Rahmen dieser Lehrveranstaltung können alle Teilnehmenden sich unter folgendem Link mit ihrer Uni Bern E-Mail Adresse (*students.unibe.ch) registrieren und die Kurse kostenlos nutzen:\n👉🏼 Einladungslink DataCamp Registration (zuerst muss ein DataCamp Zugang erstellt werden.)\nWir werden jeweils die empfohlenen Datacamp Kurse verlinken. Sie haben mit dem Link Zugriff auf alle Datacamp-Kurse bis Ende FS25.\n👉🏼 Zur Auffrischung von R-Kenntnissen eignet sich dieser Kurs: Introduction to R\n👉🏼 Als Einführung in Python eignet sich folgender Kurs: Introduction to Python",
    "crumbs": [
      "Herzlich Willkommen"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Neurowissenschaft im Computerlab",
    "section": "",
    "text": "↩︎",
    "crumbs": [
      "Herzlich Willkommen"
    ]
  },
  {
    "objectID": "forschungsprozess.html",
    "href": "forschungsprozess.html",
    "title": "1  Complab im Forschungsprozess",
    "section": "",
    "text": "Der Begriff Computerlab wird für unsere Veranstaltung genutzt, um sich auf die vielfältige Arbeit, die Neurowissenschaftler:innen an Computern oder anderen elektronischen Geräten durchführen, um neurowissenschaftliche Forschungsfragen zu beantworten. Jede Forschungsarbeit hat mehrere Phasen, welche ganz unterschiedliche Anforderungen stellen und eigenen Skills benötigen.\nIn jeder Phase des Forschungsprozesses werden verschiedenste\n\nelektronische Geräte (Computer, Eyetracker, EEG-Geräte, MRTs, etc.)\nProgramme (PsychoPy, E-Prime, etc.)\nProgrammiersprachen (R, MATLAB, Python, Java, C++, Ruby, Julia, etc.)\nspeicherbare Ergebnisse: Skripte, Datensätze, Stimuli, Grafiken…\netc.\n\nverwendet.\n\n\n\nPhasen des Forschungsprozesses\n\n\nIn diesem Kurs werden wir einige Stationen dieses Forschungsprozesses gemeinsam bearbeiten, um Einblick in das neurowissenschaftliche Arbeiten zu erhalten.\n\n\n\n\n\n\nHands-on: Complab im Forschungsprozess\n\n\n\n\nWählen Sie in 2-4er Gruppen eines der untenstehenden Paper (Open Access) aus:\n\n\nQuon et al. (2021): Quon et al. (2021): Musical components important for the Mozart K448 effect in epilepsy\nRichter et al. (2021): Richter et al. (2021): Listening to speech with a guinea pig-to-human brain-to-brain interface\nZhang et al. (2021): Zhang et al. (2021): Longitudinal effects of meditation on brain resting-state functional connectivity\n\n\nSchreiben Sie zuerst in einem Satz oben auf Ihr Blatt auf, was die Fragestellung des Papers ist.\nGehen Sie das Paper durch und schreiben Sie heraus, wo überall im “Computerlab” gearbeitet wurde, bei dieser Forschungsarbeit. (Sie können auch Vermutungen anstellen.)\n\nz.B. Datenanalyse -&gt; R/RStudio\n[15 Minuten]\n\n\n\n\n\n\nQuon, Robert J., Michael A. Casey, Edward J. Camp, Stephen Meisenhelter, Sarah A. Steimel, Yinchen Song, Markus E. Testorf, et al. 2021. “Musical Components Important for the Mozart K448 Effect in Epilepsy.” Scientific Reports 11 (1): 16490. https://doi.org/10.1038/s41598-021-95922-7.\n\n\nRichter, Claus-Peter, Petrina La Faire, Xiaodong Tan, Pamela Fiebig, David M. Landsberger, and Alan G. Micco. 2021. “Listening to Speech with a Guinea Pig-to-Human Brain-to-Brain Interface.” Scientific Reports 11 (1): 12231. https://doi.org/10.1038/s41598-021-90823-1.\n\n\nZhang, Zongpai, Wen-Ming Luh, Wenna Duan, Grace D. Zhou, George Weinschenk, Adam K. Anderson, and Weiying Dai. 2021. “Longitudinal Effects of Meditation on Brain Resting-State Functional Connectivity.” Scientific Reports 11 (1): 11361. https://doi.org/10.1038/s41598-021-90729-y.",
    "crumbs": [
      "Neurowissenschaftliches Forschen",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Complab im Forschungsprozess</span>"
    ]
  },
  {
    "objectID": "voraussetzungen.html",
    "href": "voraussetzungen.html",
    "title": "2  Voraussetzungen der Forschung",
    "section": "",
    "text": "3 Marr’s levels of explanation\nDavid Marr hat drei Ebenen vorgeschlagen, um zu klären, wie komplexe Systeme, insbesondere kognitive Systeme wie das Gehirn, Informationen verarbeiten. Die Idee ist, dass jedes informationsverarbeitende System (biologisch oder künstlich) auf diesen drei verschiedenen Ebenen analysiert werden kann. Zusammen zeigen sie, was das System tut, wie es das tut und wie es physikalisch realisiert ist.",
    "crumbs": [
      "Neurowissenschaftliches Forschen",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Voraussetzungen der Forschung</span>"
    ]
  },
  {
    "objectID": "voraussetzungen.html#levels-of-analyses",
    "href": "voraussetzungen.html#levels-of-analyses",
    "title": "2  Voraussetzungen der Forschung",
    "section": "3.1 3 Levels of analyses",
    "text": "3.1 3 Levels of analyses\n\n3.1.1 Computational level (what and why)\n\nWas macht das System?\nEs definiert das Ziel oder den Zweck des Prozesses. Diese Ebene erklärt, welches Problem das System löst und warum es wichtig ist.\nBeispiel aus dem Sehen: Das System muss die 3D-Welt aus dem 2D-Input der Retina rekonstruieren. Warum? Weil das Verständnis der räumlichen Umgebung für das Überleben wichtig ist.\nWas es zeigt: Das „große Ganze“ - die Funktion, die das System erfüllt.\n\n\n\n3.1.2 Algorithmic level (how)\n\nWie erreicht das System das Ziel?\nEs geht um die spezifischen Darstellungen und Verfahren, die zur Erreichung des Ziels eingesetzt werden. Betrachten Sie dies als die „Anweisungen“ oder „Software“ des Systems.\nBeispiel aus dem Sehen: Das Gehirn erkennt Kanten, kombiniert Merkmale und identifiziert Objekte mithilfe von Algorithmen wie Merkmalsextraktion und Mustervergleich.\nWas es zeigt: Den Prozess - die schrittweise Umwandlung vom Input zum Output.\n\n\n\n3.1.3 Implementational level\n\nWie wird der Prozess physisch umgesetzt?\nDiese Ebene beschreibt die Hardware oder biologischen Strukturen, die die Berechnungen ausführen. Beim Menschen sind dies Neuronen, Gehirnbereiche und neuronale Schaltkreise.\nBeispiel aus dem Sehen: Die Randerkennung erfolgt in der Retina und in frühen visuellen Bereichen wie dem primären visuellen Kortex (V1). Bestimmte Neuronen feuern bei bestimmten Ausrichtungen des Lichts.\nWas es zeigt: Die materielle Basis - die Mechanismen, die es möglich machen.\n\nKognitive Neurowissenschaft kann auf allen drei Ebenen tätig sein. Beispiel aus der Gesichtserkennung:\n\nDas Gehirn muss Gesichter schnell und genau erkennen, weil dies für die soziale Interaktion evolutionär wichtig ist.\nDie Gesichtserkennung kann die Extraktion von Merkmalen (Augen, Nase, Mund) und eine ganzheitliche Verarbeitung umfassen.\nDas fusiforme Gesichtsareal (FFA) spielt eine Schlüsselrolle bei der Wahrnehmung von Gesichtern.",
    "crumbs": [
      "Neurowissenschaftliches Forschen",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Voraussetzungen der Forschung</span>"
    ]
  },
  {
    "objectID": "voraussetzungen.html#grundlegendes-postulat",
    "href": "voraussetzungen.html#grundlegendes-postulat",
    "title": "2  Voraussetzungen der Forschung",
    "section": "3.2 Grundlegendes Postulat:",
    "text": "3.2 Grundlegendes Postulat:\nMentale Prozesse ergeben sich aus der Gehirnaktivität.\nGanz wichtig ist: Korrelation ≠ Kausalität. Neuroimaging-Methoden zeigen nur Korrelationen und keine Kausalität.",
    "crumbs": [
      "Neurowissenschaftliches Forschen",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Voraussetzungen der Forschung</span>"
    ]
  },
  {
    "objectID": "experiments.html",
    "href": "experiments.html",
    "title": "3  Neurowissenschaftliche Experimente",
    "section": "",
    "text": "3.1 Forschungsbereiche der Neurowissenschaften\nIn der Neurowissenschaft wird mit naturwissenschaftlichem Schwerpunkt der Aufbau und die Funktionen des Nervensystems untersucht. Neurowissenschaften sind ein sehr weites Forschungsbereich in dem unterschiedlichste und zahlreiche Themen bearbeitet werden. Die Forschungsbereiche reichen von Affektiven Neurowissenschaften, die den Zusammenhang von Gehirn und Emotionen untersuchen, über Neurochemistry, die sich u.a. mit Neurotransmittern und psychopharmakologischen Themen befasst, bis hin zu Neuroengineering, welches neuronale Systeme zu verstehen, ersetzen, reparieren oder verbessern versucht.1\nSo vielfältig, wie die Forschungsbereiche sind auch die experimentellen Ansätze und Methoden. Neurowissenschaftliche Forschung wird oft an Organismen und Tieren durchgeführt (z.B. Einzelzellableitungen in Affen). In diesem Kurs fokussieren wir uns auf neurowissenschaftliche Forschung am Menschen im Bereich der kognitiven Neurowissenschaft und Neuropsychologie. Das bedeutet, wir besprechen die Datenerhebung und -verarbeitung in verhaltenswissenschaftlichen Experimenten (teilweise auch im Zusammenhang mit bildgebenden Verfahren), welche Gehirnprozesse von Menschen untersuchen.\nKognitive Neurowissenschaften sind eng verknüpft mit Forschungsbereichen, wie beispielsweise der Psychologie, der Linguistik, künstlicher Intelligenz, Philosophie und Anthropologie:",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Neurowissenschaftliche Experimente</span>"
    ]
  },
  {
    "objectID": "experiments.html#forschungsbereiche-der-neurowissenschaften",
    "href": "experiments.html#forschungsbereiche-der-neurowissenschaften",
    "title": "3  Neurowissenschaftliche Experimente",
    "section": "",
    "text": "Das Nebenfach Neurowissenschaften an der Universität Bern fokussiert auf Aspekte der Neurowissenschaft, die für das Gebiet der Psychologie relevant sind, wie z. B. die neuronalen Grundlagen von Kognition, Emotion oder Sozialverhalten.\n\n\n\n\n\nGrafik von O. Guest (2024) modifiziert nach dem Paper von Van Rooij et al. (2023)\n\n\n\nWer sich für die Bedeutung von AI im Zusammenhang mit den Neurowissenschaften interessiert findet im Paper von Van Rooij et al. (2023) eine kritische Auseinandersetzung mit Chancen und Herausforderungen dieser Verknüpfung.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Neurowissenschaftliche Experimente</span>"
    ]
  },
  {
    "objectID": "experiments.html#besondere-herausforderungen-von-experimenten-in-den-verhaltenswissenschaften-kognitiven-neurowissenschaften",
    "href": "experiments.html#besondere-herausforderungen-von-experimenten-in-den-verhaltenswissenschaften-kognitiven-neurowissenschaften",
    "title": "3  Neurowissenschaftliche Experimente",
    "section": "3.2 Besondere Herausforderungen von Experimenten in den Verhaltenswissenschaften / kognitiven Neurowissenschaften",
    "text": "3.2 Besondere Herausforderungen von Experimenten in den Verhaltenswissenschaften / kognitiven Neurowissenschaften\nDas Erstellen und Durchführen neurowissenschaftlicher Experimente bringt viele Herausforderungen mit sich.\n\n3.2.1 Passung: Experimentalparadigmen passend zur Fragestellung\nNeurowissenschaftliche Experimente müssen exakt auf die Fragestellung zugeschnitten werden um aussagekräftige Daten zu liefern. Oft muss ein neues Paradigma erstellt werden, d.h. Forschende können kein schon bestehendes Experiment nutzen, sondern untersuchen einen Aspekt eines neuronalen Prozesses mit einer neuen Methode, einer neuen Fragestellung oder einem neuen Ansatz. Deshalb programmieren die meisten Forschenden ihre Experimentalparadigmen selbst. So können beispielsweise Instruktionen oder verwendete Stimuli, deren Grösse und Anzeigedauer präzise definiert werden. Dies erfordert breite Kenntnisse im Programmieren, der zu verwendenden Technik, wie auch der Gehirnprozesse.\n\n\n3.2.2 Präzision: Hohe räumliche und zeitliche Auflösung\nEine grosse Schwierigkeit neurowissenschaftlicher Experimente ist oft, dass eine präzise Kontrolle von räumlichen und zeitliche Eigenschaften der Experimente nötig ist um sinnvolle Daten zu erhalten. Visuelle Stimuli müssen z.B. sehr genau und immer gleich präsentiert werden können. Die zeitliche Auflösung ist gerade bei EEG Experimenten von enormer Bedeutung, da EEG eine sehr hohe zeitliche Auflösung hat. Räumliche Auflösung kann bedeuten, dass sehr präzise visuelle Darbietung möglich sein muss, sowie dass die Versuchsperson sich im Setup nicht bewegen darf, weil dies die Distanzen verschiebt (z.B. im MRT, oder der Abstand zum Bildschirm beim Eyetracking).\n\n\n3.2.3 Synchronisation: Mehrere Datenspuren\nNeurowissenschaftliche Experimente beinhalten oft die Datenerhebung auf mehreren Ebenen, z.B. wird gleichzeitig Hirnaktivität und das Drücken von Antwortbuttons aufgenommen. Das bedeutet, dass Bildschirm, MRT/EEG/Eyetracking/etc., sowie die Antwort zeitlich koregistriert/synchronisiert werden müssen, um die Daten im Nachhinein auswerten zu können. Technisch ist das oft mit grossem Aufwand verbunden und benötigt einiges an Pilotierung.\n\n\n3.2.4 Komplexität: Zu untersuchender Prozess und Störprozesse\nOft soll ein ganz spezifischer Prozess untersucht werden, aber das ist eine sehr komplexe Aufgabe, weil im menschlichen Gehirn gleichzeitig sehr viele verschiedene Prozesse ablaufen, kein Hirnareal hat nur eine einzige Aufgabe und aus ethischen Gründen ist das “Ausschalten” von Störfaktoren nicht immer möglich. Was kann man tun?\nEin Weg den Prozess sichtbar zu machen ist es zum Beispiel einen Kontrast zu rechnen, dies wird beispielsweise bei EEG und fMRI Experimenten, aber auch bei Reaktionszeitexperimenten sehr oft gemacht. Hierfür erhebt man Daten in einer Test-Bedingung in der der Prozess abgerufen wird und eine Kontroll-Bedingung, welche als “Baseline” dient. Die Baseline enthält alle “nicht interessierenden” Prozesse, die in der Test-Bedingung vorhanden sind. Durch das Vergleichen der Test- und Kontrollbedingung erhält man einen Kontrast: Also das was den interessierenden Prozess ausmacht!\nSie müssen sich beim Erstellen eines Experiments also nicht nur Gedanken dazu machen, was Sie interessiert - sondern genau so auch darüber was Sie nicht interessiert. In der Theorie tönt das einfach, in der Praxis ist das oft recht kniffelig.\n\n\n3.2.5 Ressourcenintensive Datenerhebung: Teuer und anspruchsvoll\nBildgebende Verfahren, benötigen zum Teil extrem teure Geräte, wie z.B. fMRI, und bedeuten oft hohen Aufwand, z.B. das Kleben der Elektroden beim EEG. Bei der Untersuchung von ganz bestimmten Patientengruppen hat man zudem oft nicht sehr viele Personen zur Verfügung die den Einschlusskriterien entsprechen. Oft müssen Personen auch aus dem Experiment ausgeschlossen werden, weil sie z.B. Auffälligkeiten im MRI zeigen, die nichts mit dem zu untersuchenden Prozess zu tun hat oder sie brechen während der Untersuchung ab. Gerade bei der Untersuchung klinischer Aspekte stellen sich oft Schwierigkeiten, wie beispielsweise fehlende Motivation oder Compliance von Patient:innen. Daher können oft keine sehr grossen Stichproben erhoben werden, was im Gegenzug besonders präzise Experimente erfordert.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Neurowissenschaftliche Experimente</span>"
    ]
  },
  {
    "objectID": "experiments.html#wichtige-elemente-von-experimenten",
    "href": "experiments.html#wichtige-elemente-von-experimenten",
    "title": "3  Neurowissenschaftliche Experimente",
    "section": "3.3 Wichtige Elemente von Experimenten",
    "text": "3.3 Wichtige Elemente von Experimenten\nBeim Programmieren von Experimenten lohnt es sich, sich zuerst darüber im klaren zu sein, welche Bausteine das geplante Experiment hat. Im Folgenden werden einige typische Elemente eines Verhaltensexperiments beschrieben. Oft kommen hier natürlich noch Stimulations- oder Aufnahmemethoden hinzu.\n\n3.3.1 Begrüssung und Einverständniserklärung\nHier wird die Versuchsperson begrüsst, wird über das Experiment aufgeklärt und gibt (wenn nicht vorher auf Papier schon geschehen) ihre Einverständnis zur Teilnahme am Experiment. Dies wird je nach Ethikkommission und Ethikantrag unterschiedlich gehandhabt. Wichtige Informationen sind hierbei, dass die Versuchsperson weiss worauf sie sich einlässt (Ist zum Beispiel Hirnstimulation/fMRI/etc. geplant? Wie lange dauert das Experiment ungefähr? Was soll sie tun, wenn sie abbrechen möchte?). Die Schwierigkeit ist oft, genügend Information zu geben aber die Hypothese nicht zu verraten.\n\n\n3.3.2 Instruktion\nDie Instruktion wird oft schriftlich gegeben, um diese zwischen den Versuchspersonen konstant zu halten. Es ist teilweise herausfordernd, einen Task so genau zu erklären, dass er verständlich ist, aber die Erklärung auch kurz genug zu halten, dass die Instruktion auch gelesen wird. Oft werden Übungstrials verwendet um die Instruktion zu verdeutlichen.\n\n\n3.3.3 Stimuli\nUnter Stimuli werden die gezeigten Elemente verstanden, die den Task ausmachen. Es können Töne, Bilder, Wörter, etc. als Stimuli verwendet werden.\n\n\n\n\n\n\nHands-on: Stimuli\n\n\n\nWelche Stimuli aus neurowissenschaftlichen Experimenten kennen Sie?\nTauschen Sie sich mit Ihren Mitstudierenden aus und schreiben/zeichnen Sie ein paar Beispiele vorne an die Tafel.\n[~5 Minuten]\n\n\n\n\n3.3.4 Trial\nEin Trial beschreibt ein sich wiederholender Vorgang in dem der Stimulus gezeigt wird und z.B. von der Versuchsperson eine Antwort erwartet wird. Ein Trial wird oft sehr viele Male wiederholt. Die Stimuli können zwischen den Trials variieren oder gleich bleiben. Das Timing der Trials kann konstant sein (ein Stimulus wird bspw. immer gleich lang gezeigt) oder variiert werden (unterschiedliche Anzeigedauer).\nZwischen den Trials wird ein Inter-Trial-Interval (ITI) festgelegt. Dies wird z.B. bei fMRI Experimenten dann variiert, damit (je nach Repetition Time/TR) nicht immer in derselben Schicht aufgenommen wird bei Stimuluspräsentation.\nWährend einem Trial wird die Antwort / Response der Versuchsperson aufgenommen. Bei der Aufnahme von Reaktiosnzeiten muss festgelegt werden, wann der Trial oder die Stimuluspräsentation beginnt und mit welcher Handlung sie endet. Es kann bestimmt werden, welche Antworten zulässig sind (bspw. nur bestimmte Tasten) und was passiert wenn eine richtige oder falsche Antwort gegeben wird: Gibt es z.B. ein Feedback bei falschen Antworten?\n\n\n3.3.5 Run / Block\nEin Run/ein Block bezeichnet eine Einheit mit mehreren Trials. Oft werden Bedingungen z.B. zwischen den Runs randomisiert. Zwischen den Runs sind Pausen möglich, damit sich die Versuchsperson erholen kann. Oft wird vor dem Experimentstart ein “Übungsblock” durchgeführt, um sich sicher zu sein, dass die Versuchspersonen die Aufgabe und Instruktion verstanden haben.\n\n\n3.3.6 Debriefing und Verabschiedung\nIm Debriefing wird der Versuchsperson erklärt, um was es im Experiment gegangen ist, welche Hypothesen untersucht wurden und eine eventuelle Coverstory aufgedeckt. Oft werden Personen vor dem Debriefing nach der getesteten Hypothese gefragt, um zu schauen, ob sie diese erraten hatten. Das gibt Aufschluss darüber wie sehr das Experiment dadurch verzerrt sein könnte, dass die Versuchspersonen Bescheid wissen. Wichtig ist es auch den Versuchspersonen zum Schluss zu danken.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Neurowissenschaftliche Experimente</span>"
    ]
  },
  {
    "objectID": "experiments.html#vorgehen-experiment-programmieren",
    "href": "experiments.html#vorgehen-experiment-programmieren",
    "title": "3  Neurowissenschaftliche Experimente",
    "section": "3.4 Vorgehen Experiment programmieren",
    "text": "3.4 Vorgehen Experiment programmieren\nWichtige Schritte beim Programmieren von Experimenten sind folgende (nicht unbedingt in dieser Reihenfolge, das kommt auf das Experiment an):\n\nTask auswählen\nStimuli auswählen und generieren\nTrial erstellen (Fixationskreuz? Stimulus? Antwortmöglichkeiten? Feedback? Masking?)\nTiming festlegen: Dauer Stimuluspräsentation? ITIs (Inter-Trial-Intervals)? Antwortfenster?\nDesign: Anzahl Bedingungen und Trials bestimmen (Power bedenken), within oder between Design?\nAblauf des Experiments festlegen: Gesamtdauer? Pausen nötig?\nInstruktion: klare Anweisungen, Coverstory\nEinbindung von allen technischen Geräten (z.B. EEG Recorder, MRT, Brainstimulation-Devices, Eyetracking) und Synchronisation\n\n\n3.4.1 Flowcharts\nFlowcharts, auf Deutsch Flussdiagramme, eignen sich um Prozesse zu beschreiben. Beim Programmieren kann man damit gut darstellen, was ein Programm machen soll.\n\nBei der Planung und dem Erstellen eines Experiments ist es ebenfalls hilfreich eine Flowchart zu erstellen. In einer Experiment-Flowchart sind die Elemente eines Experimentes in Boxen eingezeichnet und mit Pfeilen verbunden um sie zeitlich einzuordnen. Timing-Informationen können unter oder neben den Boxen festgehalten werden. Die Anzahl Repetitionen wird oft neben den Pfeilen eingefügt. Hilfreich ist auch zu kennzeichnen, wo welche Daten erhoben werden (button presses, EEG, etc.).\n\nWir werden in diesem Kurs immer wieder darauf eingehen, wie man ein Experiment möglichst gut planen kann um aussagekräftige Daten zu erhalten. Hier gibt es viele Möglichkeiten wie Pilotierung, Datensimulation und die adäquate Wahl der statistischen Verfahren in Bezug auf die Fragestellung.\n\nEine Flowchart eignet sich ebenfalls sehr gut, um in einem Paper/einer Arbeit darzustellen, wie der Ablauf des Experiments war.\n\n\n\n\n\n\nHands-on: Flowcharts\n\n\n\nSuchen Sie zusammen zu einem Thema Ihrer Wahl eine Flowchart.\n\nIdentifizieren Sie alle Elemente des Experiments, die Sie finden.\nGibt es Informationen zu den Stimuli?\nGibt es Informationen zum Timing?\nGibt es Informationen zur Datenerhebung?\nFehlt etwas? Wie würden Sie dies ergänzen?\n\n[~10 Minuten]\n\n\n\n\n\n\nShepherd, Gordon M. 1988. Neurobiology, 2nd Ed. Neurobiology, 2nd Ed. New York, NY, US: Oxford University Press.\n\n\nVan Rooij, Iris, Olivia Guest, Federico G Adolfi, Ronald De Haan, Antonina Kolokolova, and Patricia Rich. 2023. “Reclaiming AI as a Theoretical Tool for Cognitive Science.” Preprint. PsyArXiv. https://doi.org/10.31234/osf.io/4cbuv.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Neurowissenschaftliche Experimente</span>"
    ]
  },
  {
    "objectID": "experiments.html#footnotes",
    "href": "experiments.html#footnotes",
    "title": "3  Neurowissenschaftliche Experimente",
    "section": "",
    "text": "Forschungsbereiche der Neurowissenschaften: ↩︎",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Neurowissenschaftliche Experimente</span>"
    ]
  },
  {
    "objectID": "psychopy_experiments.html",
    "href": "psychopy_experiments.html",
    "title": "4  Verhaltensexperimente mit PsychoPy",
    "section": "",
    "text": "4.1 Umgebung",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Verhaltensexperimente mit PsychoPy</span>"
    ]
  },
  {
    "objectID": "psychopy_experiments.html#umgebung",
    "href": "psychopy_experiments.html#umgebung",
    "title": "4  Verhaltensexperimente mit PsychoPy",
    "section": "",
    "text": "4.1.1 Experiment-File erstellen und abspeichern\n\nÖffnen Sie PsychoPy und speichern Sie in einem dafür erstellten Ordner (z.B. psychopy_experiment) das Experiment-File ab (z.B. unter experiment_stroop-task).\n\n\n\n4.1.2 Builder (GUI) und Coder?\nExperimente können in PsychoPy mit dem Builder (in einem GUI) erstellt werden, der Python Code wird so automatisch für Sie generiert. Sie können sich diesen Code auch anschauen und verändern. Leider können Sie sobald Sie den Code verändert haben, diese Änderungen nicht zurück in den Builder übertragen. Im Builder-Modus können Sie aber Codestücke einfügen um einzelne Teile des Experiments in Python (oder anderen Programmiersprachen) zu programmieren und dennoch im Builder weiterarbeiten zu können.\n\nFalls Sie planen ein Online-Experiment durchzuführen, eignet sich der Builder besonders, da die Experimente direkt online durchgeführt werden können.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Verhaltensexperimente mit PsychoPy</span>"
    ]
  },
  {
    "objectID": "psychopy_experiments.html#experiment-erstellen",
    "href": "psychopy_experiments.html#experiment-erstellen",
    "title": "4  Verhaltensexperimente mit PsychoPy",
    "section": "4.2 Experiment erstellen",
    "text": "4.2 Experiment erstellen\n\n4.2.1 Stimuli\nIn PsychoPy finden Sie schon vorprogrammierte Stimulus Elemente, wie Gratings oder Rating Scales und können Texte, geometrische Figuren, Bilder und Filme einfügen. Auch komplexere Stimuluselemente wie Random Dots können sehr einfach konfiguriert werden ohne dass sie von Grund auf neu programmiert werden müssen.\n\nErstellen Sie einen Stimulus. Beachten Sie folgende Aspekte:\n\nFarbe\nGrösse\nweitere Eigenschaften, wie Bedingung/Kongruenz?\nTiming (Stimulusdauer, Stimulusende)\n\nNotieren Sie, welche Eigenschaften des Stimulus sich über die Trials hinweg verändern sollte. Dies können auch mehrere Eigenschaften sein. Diese Liste benötigen Sie später.\n\n\n\n4.2.2 Trial\n\nErgänzen Sie alle Elemente, die für einen vollständigen Trial notwendig sind:\n\nAntwort der Versuchsperson / Response (siehe auch 4.2.4)\nInter-Trial-Intervall (ITI): kann vor oder nach dem Stimulus eingefügt werden. (Die Zeit des ITI wird oft variiert. Dies müsste also auch auf die Liste oben)\nFixationskreuz?\nMask?\nFeedback?\n\n\n\n\n4.2.3 Trialschleife\nSie müssen nicht alle Trials (oder in PsychoPy: Routines) des Experiments einzeln programmieren, sondern können diese wiederholen, in dem Sie eine Trial-Schleife (loop) um den Trial herum erstellen.\n\nErstellen Sie einen loopindem Sie im Feld Flow auf Insert loop klicken.\n\nMit loopType können Sie steuern, die Bedingungen randomisiert/gemischt oder sequentiell/der Reihe nach angezeigt werden sollen.\nMit nReps können Sie angeben, wie oft jeder Stimulus wiederholt werden soll. Haben Sie also einen Stimulus mit zwei zu varierenden Eigenschaften , welche je 3 Stufen haben (also 9 Zeilen im conditions-File und nReps= 2), ergibt das 18 Trials.\n\n\nMittels diesen Schleifen können die Bedingungen implementiert werden z.B. dass sich der Stimulus bei jedem Trial verändert. Dies kann mit einer conditions-Datei spezifiziert werden, idealerweise im .csv oder .xlsx-Format.\n\nDie Endung .csv bedeutet, dass die Daten als comma separated values abgespeichert werden, also durch ein Komma getrennt. Dieses Dateiformat eignet sich besser als .xlsx, weil es mit vielen Programmen kompatibel und gut einlesbar ist.\n\nBeispielsweise wollen wir drei verschiedene Worte anzeigen (dog, cat und rabbit) und dieses Wort unterschiedlich lange anzeigen (Dauer: 1, 10 und 100 Frames). Die Versuchspersonen sollen dann den Anfangsbuchstaben des Wortes drücken, also d für dog, c für cat und r für rabbit.\n\nUm die Bedingungen (in unserem Fall: die sich verändernden Stimuluseigenschaften) zu definieren, erstellen wir eine .csvoder .xlsx-Datei (z.B. in Excel/Notepad/etc.) mit dem Namen conditions und speichern dieses im selben Ordner wie das Experiment.\n\nFügen Sie für jedes sich verändernde Element einen Variablennamen und die entsprechenden Werte ein (dies sind die Eigenschaften, die Sie sich bei Punkt 4.2.1 notiert haben). Die Variablennamen schreiben wir immer in die oberste Zeile der Datei.\nWenn wir z.B. einen Text anzeigen möchten, schreiben wir in die erste Zeile word und duration.\nIn die Spalte unter die Variablennamen schreiben wir die Werte.\nAls Beispiel könnten die Worte die wir anzeigen lassen wollen cat, dog und rabbit lauten. Dann stehen in der Spalte word, diese 3 Wörter unter dem Variablennamen. Unter dem Variablennamen duration geben wir die Anzahl Frames ein, also 1, 10 und 100. Wir wollen jedes Wort mit jeder Dauer kombinieren. Das ergibt 9 Zeilen.\nFügen Sie in jeder Zeile unter dem Variablennamen corrAns die jeweils korrekte Antwort ein.\nFügen Sie, falls vorhanden, in jeder Zeile weitere wichtige Information zum Stimulus ein.\nIm Beispiel möchten Sie z.B. später fleischfressende mit pflanzenfressenden Tieren vergleichen, deshalb eine Spalte meat. Dies verändert im Experiment nichts, dient aber am Schluss zur Auswertung, weil diese Variable auch immer in den Datensatz geschrieben wird.\n\n\nFügen Sie nun im Loop-Fenster die conditions-Datei ein.\n\n\n\n\n\n\n\n\nTipp\n\n\n\nJede Zeile in der conditions-Datei unterhalb des Variablennamens entspricht einer Bedingung (condition).\nSetzen Sie nReps auf 1 während Sie das Experiment erstellen, so sparen Sie Zeit.\n\n\nIm PsychoPy können Sie Variablen mit einem vorangestellten $einfügen.\n\nÖffnen Sie nun wieder das Stimulusfenster und passen Sie dort die Stimuluseigenschaften an. Anstatt von hard-coded values (also einmalig, fix festgelegten Werten) geben wir nun einen Variablennamen ein. Der Stimulus darf nicht auf constant gesetzt sein, sonst kann er sich nicht Trial für Trial verändern, setzen Sie ihn deshalb unbedingt auf set every repeat.\nIn unserem Beispiel fügen wir bei text die Laufvariable (verändernde Eigenschaft) ein: $word. Die Anzeigedauer des Textes soll $duration in Frames sein.\n\n\n\nLassen Sie das Experiment laufen und kontrollieren Sie, ob alles funktioniert hat.\n\n\n\n\n\n\n\nTipp\n\n\n\nMit dieser Methode können Sie auch Instruktionen, ITIs, etc. variieren lassen.\n\n\n\n\n4.2.4 Antworten aufnehmen\nIn PsychoPy muss definiert werden, wie die Antwort der Versuchsperson aufgenommen wird. Dies kann mit der Maus, der Tastatur oder anderen Devices umgesetzt werden. Die Möglichkeiten sehen Sie unter Responses.\n\nFügen Sie eine Antwortkomponente hinzu und benennen Sie diese sinnvoll.\nIn unserem Beispiel möchten wir, dass die Versuchsperson mittels Keyboard antwortet.\n\nMit Force end of Routine können Sie einstellen, ob eine Antwort den Trial beendet und mit dem nächsten fortfährt.\nDer Namen der Antwortkomponente wird später im Datensatz als Variable zu finden sein.\nWerden in einer Antwortkomponente namens key_resp mittels Tastendruck Antwort und Response Time aufgenommen, heissen die Variablen dann key_resp.keys(gedrückte Taste) und key_resp.rt (Antwortdauer).\nEntscheiden Sie, ob PsychoPy überprüfen soll, ob die richtige Antwort gegeben wurde.\nWenn Sie dies möchten, gleicht PsychoPy in unserem Beispiel die gegebene Antwort (key_resp.keys) mit der dafür eingegebenen Variable (hier corrAns) ab. Stimmen diese überein, fügt es in die Variable key_resp.corr 1 ein, wenn nicht 0).\nMit first key definieren Sie, dass der erste Tastendruck zählt.\n\n\n\n\n\n\n4.2.5 Weitere Elemente\nIn PsychoPy GUI wird Ihnen im Fenster Floweine Art Flowchart angezeigt. Hier sehen Sie, welche Elemente Ihr aktuelles Experiment enthält.\n\nFügen Sie nun alle weiteren Elemente, die Sie zu Beginn auf Ihrer Flowchart eingezeichnet hatten, z.B.\n\nBegrüssung\nEinverständnis\nInstruktion\nDebriefing, Verabschiedung\n\nLassen Sie das Experiment laufen und kontrollieren Sie, ob alles funktioniert hat.\n\n\n\n\n\n\n\nTipp\n\n\n\nBeim Programmieren lohnt es sich oft, die kleinen Schritte zwischenzutesten, weil es dann einfacher ist herauszufinden, wo genau der Fehler passiert ist.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Verhaltensexperimente mit PsychoPy</span>"
    ]
  },
  {
    "objectID": "psychopy_experiments.html#datenspeicherung",
    "href": "psychopy_experiments.html#datenspeicherung",
    "title": "4  Verhaltensexperimente mit PsychoPy",
    "section": "4.3 Datenspeicherung",
    "text": "4.3 Datenspeicherung\nWenn man die default-Einstellungen nicht ändert, speichert PsychoPy die Daten automatisch in einer trial-by-trial .csv-Datei. Das bedeutet, dass jeder Trial 1 Zeile generiert. Die .csv-Datei erhält einen Namen, der sich aus der Versuchspersonen-ID, dem Namen des Experiments, und dem aktuellen Datum inkl. Uhrzeit zusammensetzt. So ist es möglich, mit derselben Versuchspersonen-ID beliebig oft das Experiment zu wiederholen. Die .csv-Dateien werden in einem Ordner mit dem Name data abgelegt.\nIn den Fenstern der Elemente kann jeweils angegeben werden, was alles gespeichert werden soll.\n\n\n\n\n\n\nTipp\n\n\n\nBei der Wahl vom Datenfile-Namen empfiehlt es sich immer Datum und Uhrzeit anzuhängen. Dies verhindert, dass Daten überschrieben werden, wenn z.B. eine Versuchspersonen-ID falsch eingetippt oder doppelt vergeben wird.\n\n\nDas oben verwendete Beispielsexperiment ergibt folgenden Datensatz:\n\nSie sehen die Infos aus der conditions-Datei (gelb), die Zählerinformationen der Loops (hellgrau), die Timinginformationen (dunkelgrau), die Antwortinformationen (blau) und die Experimentinformationen (grün).",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Verhaltensexperimente mit PsychoPy</span>"
    ]
  },
  {
    "objectID": "psychopy_experiments.html#test-pilotierung",
    "href": "psychopy_experiments.html#test-pilotierung",
    "title": "4  Verhaltensexperimente mit PsychoPy",
    "section": "4.4 Test / Pilotierung",
    "text": "4.4 Test / Pilotierung\n\nFühren Sie das Experiment aus und schauen Sie sich den Datensatz an: Sind alle wichtigen Informationen auf jeder Zeile vorhanden?\n\nVersuchspersonen-ID\nBedingung\nStimuluseigenschaften (z.B. word)\nAntwort der Versuchsperson\nAntwortdauer der Versuchsperson\nAntwort korrekt?\n\nKönnen die Daten überschrieben werden?\nLassen Sie jemanden anderes Ihr Experiment durchführen, und geben Sie einander Feedback.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Verhaltensexperimente mit PsychoPy</span>"
    ]
  },
  {
    "objectID": "psychopy_experiments.html#verwenden-von-codekomponenten-im-builder",
    "href": "psychopy_experiments.html#verwenden-von-codekomponenten-im-builder",
    "title": "4  Verhaltensexperimente mit PsychoPy",
    "section": "4.5 Verwenden von Codekomponenten im Builder",
    "text": "4.5 Verwenden von Codekomponenten im Builder\nAuch wenn man das Experiment im Builder erstellt erfordern einige Experimentelemente das Verwenden von Codekomponenten. In diesem Abschnitt werden zwei häufige Anwendungsbeispiele besprochen: Die variable Blockdauer (z.B. für Fixationskreuze oder ITIs) und das Geben von Feedback (z.B. in einem Übungsdurchgang).\n\n\n\n\n\n\nIf-else Statements in Python\n\n\n\nIn Python können Sie für verschiedene Fälle (cases) andere Aktionen ausfüllen, indem Sie If-else Statements nutzen.\nEin If-else Statement enthält ein if (wenn), einer condition (das zutrifft), einem body (dann mach das).\nErgänzt kann dies werden mit ifelse (oder wenn)+ condition (das zutrifft) + body (dann mach das) und einem else (wenn nichts davon zutrifft) + body (dann mach das).\nWichtig: - Python ist indentation-sensibel, das bedeutet: Die Einrückung (1 Tab) muss stimmen, sonst funktioniert der Code nicht. Auch der Doppelpunkt : ist wichtig und muss an der richtigen Stelle stehen. Wenn Sie mehrere conditions verwenden möchten, müssen Sie diese in Klammern () setzen. Hier sehen Sie die Syntax eines If-else Statements:\nif (condition):\n    body\n    \nelif (condition):\n    body\n    \nelse:\n    body\n\n\n\nEinführung in Python auf Datacamp: 👉🏼 Introduction to Python\n\n\n4.5.1 Variable Dauer von Elementen\n\nFixationskreuz und ITI mit randomisierter Dauer\nUm das Experiment für die Versuchsperson unvorhersehbarer zu machen, implementieren wir vor dem eigentlichen Stimulus ein Fixationskreuz mit variabler Länge. Diese Länge soll 0.2, 0.4, 0.6, oder 0.8 Sekunden betragen.\n\nFügen Sie einen Codeblock code_fixationcross ein und definieren Sie unter Begin Routine die Variable fixationcross_duration.\n\nFügen Sie einen Textblock fixationcross ein mit dem Text + und Schriftgrösse 10. Geben Sie unter duration Ihre vorher definierte Variable ein (vergessen Sie dabei das $ nicht): $fixationcross_duration.\n\n\n\n\n\n\n\n\nHands-on: Variable ITI einbauen\n\n\n\nFügen Sie nach dem Stimulus eine ITI mit variabler Dauer hinzu.\nEinfachere Variante: Die ITI soll 10, 20, 30, 40 oder 50 Frames betragen.\nSchwierigere Variante: Die ITI soll eine Zufallszahl zwischen 0.2 und 0.8 Sekunden betragen.\n\n\n\n\n\n4.5.2 Feedback\nEs gibt Experimente, welche Feedback erfordern. Oft wird vor der Datenerhebung ein Übungsblock eingebaut, welcher Feedback enthält, so dass die Versuchspersonen wissen, ob sie den Task richtig verstanden haben.\n\nErstellen Sie zuerst eine Trialschleife mit einem Stimulus und einer Response.\nFügen Sie nach dem Stimulus und der Antwort (aber innerhalb der Trialschleife!) eine Routine feedback ein.\nFügen Sie innerhalb der Routine feedback eine Codekomponente hinzu. In dieser Komponente können Sie nun\n\nFügen Sie nun eine Textkomponente hinzu und fügen Sie beim Textfeld die Variable $response_msg ein, damit die Versuchsperson abhängig von ihrer Antwort das entsprechende Feedback erhält, welches zuvor in der Codekomponente definiert wurde.\n\n\n\n\n\n\n\n\nHands-on: Feedback geben\n\n\n\nSie können mittels einer Codekomponente auch reagieren, wenn die Versuchsperson zu schnell, zu langsam oder gar nicht antwortet.\n\nErstellen Sie einen Übungsdurchgang. Fügen Sie eine Code-Komponente hinzu und legen Sie fest, welches Feedback die Versuchsperson erhalten soll.\n\nEinfache Variante: Geben Sie der Person Feedback, ob ihre Antwort richtig oder falsch war.\nMittelschwere Variante: Geben Sie der Person Feedback, wenn Sie zu schnell oder zu langsam antwortet.\nSchwere Variante: Erstellen Sie einen Counter, welcher der Versuchsperson anzeigt, wie gut sie ist, indem für jede richtige Antwort 5 Punkte erhält, für jede falsche Antwort 5 Punkte abgezogen werden.\nFalls Sie zur Geschwindigkeit Rückmeldung geben wollen oder einen Counter bauen, können Sie etwas in dieser Art machen.\nif dots_keyboard_response.keys is None:\n    response_text = \"miss\"\n\nelif dots_keyboard_response.rt &lt;= 0.1:\n    response_text = \"too fast\"\n\nelse:\n    if (direction == \"left\" and dots_keyboard_response.keys == \"f\" or \n        direction == \"right\" and dots_keyboard_response.keys == \"j\"):\n        response_text = \"+5 points\"\n    else:\n        response_text = \"+0 points\"",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Verhaltensexperimente mit PsychoPy</span>"
    ]
  },
  {
    "objectID": "psychopy_experiments.html#weitere-wichtige-punkte",
    "href": "psychopy_experiments.html#weitere-wichtige-punkte",
    "title": "4  Verhaltensexperimente mit PsychoPy",
    "section": "4.6 Weitere wichtige Punkte",
    "text": "4.6 Weitere wichtige Punkte\n\n4.6.1 Degrees of Visual Angle\nOftmals werden Grössenangaben von Stimuli noch in Pixel oder Zentimeter, sondern in degrees of visual angle gemacht. Dies hat den Vorteil, dass die Angaben nicht vom Monitor selber oder der Entferung vom Monitor abhängig sind. Degrees of visual angle gibt die wahrgenommene Grösse des Stimulus an, und berücksichtigt die Grösse des Monitors und des Stimulus, und die Entfernung der Versuchsperson vom Monitor. Weitere Informationen dazu finden Sie auf der Website von OpenSesame. Üblicherweise entspricht ein degrees of visual angle etwa einem cm bei einer Entfernung von 57 cm vom Monitor.\n\nOpenSesame ist ein weiteres, Python-basierendes Programm für die Erstellung behavioraler Experimente.\n\nZur Umrechnung zwischen cm und degrees of visual angle finden Sie unter diesem Link mehr Information.\n\n\n4.6.2 Timing\nFrames vs. time (sec or ms): Die präziseste Art zur Steuerung des Timings von Stimuli besteht darin, sie für eine festgelegte Anzahl von Frames zu präsentieren. Bei einer Framerate von 60 Hz können Sie Ihren Stimulus nicht z. B. 120 ms lange präsentieren; die Bildperiode würde Sie auf einen Zeitraum von 116,7 ms (7 Bilder) oder 133,3 ms (8 Bilder) beschränken. Dies ist besonders wichtig für Reaktionszeit-Aufgaben und EEG-Studien, wo ein präzises Millisekunden-Timing erforderlich ist. Hier finden Sie weitere Informationen zu diesem Thema: Presening Stimuli - Psychopy.\n\nHertz ist eine Einheit die angibt, wie häufig etwas pro Sekunde passiert. Hertz kann wie Mal pro Sekunde ausgesprochen werden. 60 Hertz bedeutet also, 60 Mal pro Sekunde.\n\n\n\n4.6.3 Individualisierte Aufgabenschwierigkeit / Schwellenmessung\nIm Random Dot Experiment macht es z.B. für gewisse Fragestellungen Sinn die Aufgabenschwierigkeit für jede Person anzupassen, da sonst ceiling/floor-Effekte auftreten können.\nIn PsychoPy kann ein Staircase in einem Loop verwendet werden, um die Schwierigkeit einer Aufgabe basierend auf der Leistung der Teilnehmer dynamisch anzupassen. Sie ist besonders häufig in Experimenten zur Schwellenmessung, bei denen das Ziel darin besteht, die kleinste wahrnehmbare Reizintensität zu bestimmen. Hier finden Sie weitere Informationen zu diesem Thema: Using a Staircase - PsychoPy.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Verhaltensexperimente mit PsychoPy</span>"
    ]
  },
  {
    "objectID": "stroop_experiment.html",
    "href": "stroop_experiment.html",
    "title": "5  Stroop Paradigma",
    "section": "",
    "text": "6 Stroop Task\nDer Stroop Task wurde 1935 zum ersten Mal beschrieben (Stroop, 1935) und ist einer der meist zitierten und verwendeten neuropsychologischen Aufgaben (MacLeod, 1991). In der Neuropsychologie wird der Stroop Color and Word Test (SCWT) verwendet, um die Fähigkeit zur Inhibition kognitiver Interferenz zu messen, welche entsteht wenn zwei Stimuluseigenschaften gleichzeitig verarbeitet werden sich aber widersprechen (Scarpina & Tagini, 2017). Teilweise misst der Task auch andere kognitive Funktionen, wie visuelle Suche oder Arbeitsgedächtnis, weshalb der Vergleich von Bedingungen relevant ist (Periáñez et al., 2021).\nWährend dem Stroop Task wird ein Text mit Farbwörtern präsentiert. Im kongruenten Durchgang entsprechen die Farben des Textes dem Farbwort (das Wort “rot” wird in rot präsentiert), im inkongruenten Durchgang unterscheiden sich die Farben des Textes vom Farbwort (das Wort “rot” wird in gelber Farbe präsentiert). Die Person muss angeben in welcher Farbe das Wort abgedruckt ist. In der kongruenten Bedingung fällt dies leichter, weil das gelesene Wort auch der Farbe entspricht. In der inkongruenten Bedingung verlangsamt sich die Geschwindigkeit durch die entstehende Interferenz von Wort und Farbe, da das Wort automatisch gelesen wird. Oft wird auch noch eine neutrale Bedingung verwendet, wo nur die Farbe oder das Wort präsentiert werden.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Stroop Paradigma</span>"
    ]
  },
  {
    "objectID": "stroop_experiment.html#kurzbeschrieb-kursexperiment",
    "href": "stroop_experiment.html#kurzbeschrieb-kursexperiment",
    "title": "5  Stroop Paradigma",
    "section": "6.1 Kurzbeschrieb Kursexperiment",
    "text": "6.1 Kurzbeschrieb Kursexperiment\nIn diesem Experiment lösen die Personen zwei Bedingungen des Stroop Task, einmal geben sie die Farben der Wörter an in einer kongruenten Bedingung (Wortinhalt und Wortfarbe) stimmen überein. Einmal lösen sie die Aufgabe in einer inkongruenten Bedingung (Wortinhalt und Wortfarbe stimmen nicht überein).\nDie kongruente und inkongruente Bedingung kommen im selben Block vor. Die Instruktion lautet für beide Bedingungen gleich, da immer die Wortfarbe angegeben werden muss. Drei Farben werden verwendet: rot, blau und gelb.\nDas Stroop Kursexperiment ist folgendermassen aufgebaut:\n\n\n\n\n\n\n\nHands-on: Stroop Kursexperiment\n\n\n\nLaden Sie das Experiment herunter und testen Sie, ob es auf Ihrem Laptop läuft. Hier finden Sie die Anweisungen dazu.\n\nTesten Sie, ob das Experiment startet und ob die Übungstrials funktionieren. Kontrollieren Sie, ob es ein Datenfile abgespeichert hat und schauen Sie, ob dieses Datenfile alles Relevante enthält. Wenn alles ok ist, ist das Experiment bereit für Übung 1. Führen Sie die Testungen ausserhalb des Computerlabs durch.\nBeantworten Sie folgende Fragen zum Experiment:\n\n\nWas wurde im Experiment variiert? Wie viele unterschiedliche Trials gibt es?\nWelche Bedingungen gibt es?\nWieviele Trials werden pro Bedingung durchgeführt?\nWie lange wird der Wort-Stimulus angezeigt? Wann ist er fertig (zeit oder tasten-definiert?)?\nWie denken Sie, wird sich das Verhalten (Reaktionszeit, Richtigkeit) zwischen den Bedingungen unterscheiden?",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Stroop Paradigma</span>"
    ]
  },
  {
    "objectID": "stroop_experiment.html#credits",
    "href": "stroop_experiment.html#credits",
    "title": "5  Stroop Paradigma",
    "section": "6.2 Credits",
    "text": "6.2 Credits\nDieses Experiment wurde von Rebekka Borer programmiert.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Stroop Paradigma</span>"
    ]
  },
  {
    "objectID": "stroop_experiment.html#referenzen",
    "href": "stroop_experiment.html#referenzen",
    "title": "5  Stroop Paradigma",
    "section": "6.3 Referenzen",
    "text": "6.3 Referenzen\nMacLeod C. M. (1991). Half a century of research on the Stroop effect: an integrative review. Psychological Bulletin. 109(2), 163–203. https://doi.org/10.1037/0033-2909.109.2.163\nPeriáñez, J. A., Lubrini, G., García-Gutiérrez, A., & Ríos-Lago, M. (2021). Construct validity of the stroop color-word test: influence of speed of visual search, verbal fluency, working memory, cognitive flexibility, and conflict monitoring. Archives of Clinical Neuropsychology, 36(1), 99-111. https://doi.org/10.1093/arclin/acaa034\nScarpina, F., & Tagini, S. (2017). The stroop color and word test. Frontiers in psychology, 8, 557. https://doi.org/10.3389/fpsyg.2017.00557\nStroop, J. R. (1935). Studies of interference in serial verbal reactions. Journal of Experimental Psychology, 18(6), 643–662. https://doi.org/10.1037/",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Stroop Paradigma</span>"
    ]
  },
  {
    "objectID": "random_dot_experiment.html",
    "href": "random_dot_experiment.html",
    "title": "6  Random Dot Paradigma",
    "section": "",
    "text": "7 Random Dot Experiment\nJeden Tag treffen wir Tausende von kleinen Entscheidungen, meistens unter gewissem Zeitdruck. Viele davon sind trivial (z. B. welches Paar Socken man anzieht) und automatisch (z. B. ob man die Espresso- oder Lungo-Taste auf der Kaffeemaschine drückt). Die meisten Entscheidungen im wirklichen Leben setzen sich eigentlich aus zwei Entscheidungen zusammen: Einerseits der Entscheidung, mit dem Abwägen aufzuhören und aufgrund des aktuellen Wissenstandes zu handeln. Andererseits die Wahl oder Entscheidungshandlung selbst. Dieser sequentielle Charakter der Entscheidungsfindung ist eine grundlegende Eigenschaft des menschlichen Nervensystems und spiegelt seine Unfähigkeit wieder, Informationen sofort zu verarbeiten.\nPerzeptuelle Entscheidungen sind Entscheidungen, welche auf der Wahrnehmung, Einordnung und Integration von Sinnesreizen beruhen. Um beispielsweise eine Strasse sicher überqueren zu können, müssen wir mittels den Sinnesinformationen der Augen und Ohren sowie der Verarbeitung dieser Reize einschätzen mit welcher Geschwindigkeit ein herannahendes Auto unterwegs ist und ob wir lieber abwarten bis es vorbeigefahren ist. Innerhalb der Neurowissenschaften wird perceptual decision making untersucht, um die neuronalen Schaltkreise welche Wahrnehmungssignale kodieren, speichern und analysieren zu verstehen und mit beobachtbarem Verhalten in Verbindung zu bringen. Von Interesse ist zum Beispiel wie die Entscheidung ausfällt, wenn die sensorischen Daten undeutlich oder sogar widersprüchlich sind. Besonders spannend ist auch wie Vorwissen (prior knowledge) auf das Entscheidungsverhalten einwirkt.\nObwohl das Treffen von Entscheidungen für uns etwas sehr Vertrautes ist, ist das Wissen darum, wie das Gehirn diese Entscheidungsaufgaben löst noch sehr begrenzt. Eine einzelne Entscheidung kann schon sehr komplex sein. Um die Dynamik der Entscheidungsfindung zu verstehen, konzentrieren sich die meisten Studien deshalb auf einfache, wiederholbare Wahlprobleme mit nur zwei (binären) Antwortmöglichkeiten. Ein typisches Paradigma in neurowissenschaftlichen Studien ist das random-dot motion paradigm. Hierbei muss eine Person entscheiden in welche Richtung sich eine Punktewolke bewegt.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Random Dot Paradigma</span>"
    ]
  },
  {
    "objectID": "random_dot_experiment.html#kurzbeschrieb-kursexperiment",
    "href": "random_dot_experiment.html#kurzbeschrieb-kursexperiment",
    "title": "6  Random Dot Paradigma",
    "section": "7.1 Kurzbeschrieb Kursexperiment",
    "text": "7.1 Kurzbeschrieb Kursexperiment\nIn unserem Experiment lösen die Versuchspersonen einen Random Dot Task zweimal (in zwei Blöcken). In jedem Block erhalten sie eine andere Instruktion, die Aufgabe bleibt jedoch dieselbe: Sie müssen herausfinden in welche Richtung sich die Punktewolke bewegt. In einem Block werden sie instruiert die Aufgabe möglichst schnell zu lösen. Im anderen Block werden sie instruiert die Aufgabe möglichst richtig zu lösen. Wir werden dann analysieren, wie sich das Entscheidungsverhalten von Menschen verändert, je nachdem wie sie instruiert wurden.\nDas Random Dot Kursexperiment ist folgendermassen aufgebaut:\n\n\n\n\n\n\n\nHands-on: Random Dot Kursexperiment\n\n\n\nLaden Sie das Experiment herunter und testen Sie, ob es auf Ihrem Laptop läuft. Hier finden Sie die Anweisungen dazu.\n\nTesten Sie, ob das Experiment startet und ob die Übungstrials funktionieren. Kontrollieren Sie, ob es ein Datenfile abgespeichert hat und schauen Sie, ob dieses Datenfile alles Relevante enthält. Wenn alles ok ist, ist das Experiment bereit für Übung 1. Führen Sie die Testungen ausserhalb des Computerlabs durch.\nBeantworten Sie folgende Fragen zum Experiment:\n\n\nWas wurde im Experiment variiert? Wie viele unterschiedliche Trials gibt es?\nWelche Bedingungen gibt es?\nWieviele Trials werden pro Bedingung durchgeführt?\nWie lange wird der Dot-Stimulus angezeigt? Wann ist er fertig (zeit oder tasten-definiert?)?\nWie denken Sie, wird sich das Verhalten (Reaktionszeit, Richtigkeit) zwischen den Bedingungen unterscheiden?",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Random Dot Paradigma</span>"
    ]
  },
  {
    "objectID": "random_dot_experiment.html#credits",
    "href": "random_dot_experiment.html#credits",
    "title": "6  Random Dot Paradigma",
    "section": "7.2 Credits",
    "text": "7.2 Credits\nDieses Experiment wurde von Rebekka Borer programmiert.\n\n\n\n\nHauser, Christopher K., and Emilio Salinas. 2014. “Perceptual Decision Making.” In Encyclopedia of Computational Neuroscience, edited by Dieter Jaeger and Ranu Jung, 1–21. New York, NY: Springer New York. https://doi.org/10.1007/978-1-4614-7320-6_317-1.\n\n\nMulder, M. J., E.-J. Wagenmakers, R. Ratcliff, W. Boekel, and B. U. Forstmann. 2012. “Bias in the Brain: A Diffusion Model Analysis of Prior Probability and Potential Payoff.” Journal of Neuroscience 32 (7): 2335–43. https://doi.org/10.1523/JNEUROSCI.4156-11.2012.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Random Dot Paradigma</span>"
    ]
  },
  {
    "objectID": "intro_python.html",
    "href": "intro_python.html",
    "title": "7  Python: Einführung für Neurowissenschaften",
    "section": "",
    "text": "7.1 Wichtige Datentypen in Python (mit Vergleich zu R)",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Python: Einführung für Neurowissenschaften</span>"
    ]
  },
  {
    "objectID": "intro_python.html#wichtige-datentypen-in-python-mit-vergleich-zu-r",
    "href": "intro_python.html#wichtige-datentypen-in-python-mit-vergleich-zu-r",
    "title": "7  Python: Einführung für Neurowissenschaften",
    "section": "",
    "text": "Typ\nBeschreibung\nPython Beispiel\nR-Äquivalent\n\n\n\n\nListe\nVeränderbare Sammlung von Elementen\nx = [1, 2, 3, \"Hallo\"]\nx &lt;- list(1, 2, 3, \"Hallo\")\n\n\nTupel\nUnveränderbare Sammlung von Elementen\nx = (1, 2, 3)\nKein direktes Äquivalent\n\n\nDictionary\nSammlung von Schlüssel-Wert-Paaren\nx = {\"Name\": \"Max\", \"Alter\": 20}\nx &lt;- list(Name = \"Max\", Alter = 20)\n\n\nNumPy Array\nNumerisches Array für schnelle mathematische Operationen\nx = np.array([1, 2, 3])\nx &lt;- c(1, 2, 3)",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Python: Einführung für Neurowissenschaften</span>"
    ]
  },
  {
    "objectID": "intro_python.html#beispiele",
    "href": "intro_python.html#beispiele",
    "title": "7  Python: Einführung für Neurowissenschaften",
    "section": "7.2 Beispiele",
    "text": "7.2 Beispiele\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNote\n\n\n\nWichtig: In Python beginnt die Zählung bei 0, nicht bei 1 wie in R!\n\n\n\n\n\n\n\n\nHands-on 1: Eigene Liste und Tupel erstellen\n\n\n\n\nErstellen Sie eine Liste mit den Zahlen 10, 20, 30.\nFügen Sie der Liste die Zahl 40 hinzu: eure_liste.append(40).\nErstellen Sie ein Tupel mit den gleichen Zahlen.\nVersuchen Sie, ein Element mit append im Tupel hinzufügen. Was passiert?\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Python: Einführung für Neurowissenschaften</span>"
    ]
  },
  {
    "objectID": "intro_python.html#bibliotheken-importieren",
    "href": "intro_python.html#bibliotheken-importieren",
    "title": "7  Python: Einführung für Neurowissenschaften",
    "section": "7.3 Bibliotheken importieren",
    "text": "7.3 Bibliotheken importieren\nIn Python verwenden wir import, um externe Bibliotheken zu laden (ähnlich wie library() in R):\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nHierbei sind: - pandas für Tabellenstrukturen (wie data.frame in R) - numpy für numerische Berechnungen - matplotlib für Grafiken\n\n\n\n\n\n\nHands-on 2: Bibliotheken testen\n\n\n\nImportieren Sie numpy und erstellen Sie ein Array mit den Werten 5, 10, 15.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nHands-on 3: Listen vs Arrays verstehen\n\n\n\n\nErstellen Sie eine Python-Liste [2, 4, 6] und verdoppeln Sie sie mit * 2.\n\nErstellen Sie ein NumPy-Array mit denselben Werten und multiplizieren Sie es mit 2.\n\nWas ist der Unterschied in der Ausgabe?\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\na &lt;- c(1, 2, 3)\nprint(a * 2)\n\n[1] 2 4 6\n\n#Jede Zahl wird mathematisch multipliziert (genau wie bei NumPy arrays).\n\n\n#b &lt;- list(1, 2, 3)\n#print(b * 2)\n# Weil Listen in R keine mathematischen Operationen direkt unterstützen — sie sind nur Sammlungen von beliebigen Objekten.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Python: Einführung für Neurowissenschaften</span>"
    ]
  },
  {
    "objectID": "intro_python.html#datensätze-einlesen-und-anschauen",
    "href": "intro_python.html#datensätze-einlesen-und-anschauen",
    "title": "7  Python: Einführung für Neurowissenschaften",
    "section": "7.4 Datensätze einlesen und anschauen",
    "text": "7.4 Datensätze einlesen und anschauen\n\n\n\nR Funktion\nPython Funktion\n\n\n\n\nread.csv(\"path\")\npd.read_csv(\"path\")\n\n\nhead(df)\ndf.head()\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Python: Einführung für Neurowissenschaften</span>"
    ]
  },
  {
    "objectID": "intro_python.html#dot-notation-verstehen",
    "href": "intro_python.html#dot-notation-verstehen",
    "title": "7  Python: Einführung für Neurowissenschaften",
    "section": "7.5 Dot-Notation verstehen",
    "text": "7.5 Dot-Notation verstehen\nIn R ruft man Funktionen wie head(df) auf, wobei das Objekt als Argument übergeben wird.\nIn Python gehört die Funktion direkt zum Objekt und wird mit Punktnotation aufgerufen:\n\n\n\nR\nPython\n\n\n\n\nhead(df)\ndf.head()\n\n\nmean(df$col)\ndf[\"col\"].mean()\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nMerke: In Python sind viele Funktionen Methoden, die direkt an ein Objekt (wie ein DataFrame) gebunden sind. Deshalb funktioniert head(df) nicht!\n\n\n\n\n\n\n\n\nHands-on 4: Methoden mit Dot-Notation verwenden\n\n\n\n\nZeigen Sie die ersten 10 Zeilen von df an.\nVerwenden Sie df.describe() für einen Überblick.\nWenden Sie .mean() und .min() auf df[\"petal_width\"] an.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Python: Einführung für Neurowissenschaften</span>"
    ]
  },
  {
    "objectID": "intro_python.html#zugriff-auf-spalten-und-zeilen",
    "href": "intro_python.html#zugriff-auf-spalten-und-zeilen",
    "title": "7  Python: Einführung für Neurowissenschaften",
    "section": "7.6 Zugriff auf Spalten und Zeilen",
    "text": "7.6 Zugriff auf Spalten und Zeilen\nIn Python (mit pandas) können Sie ähnlich wie in R auf bestimmte Teile einer Tabelle zugreifen — aber mit etwas anderer Syntax.\n\n\n\n\n\n\n\n\nZiel\nPython Syntax\nR-Äquivalent\n\n\n\n\nSpalte auswählen\ndf[\"species\"]\ndf$species\n\n\nMehrere Spalten auswählen\ndf[[\"sepal_length\", \"species\"]]\ndf[, c(\"sepal_length\", \"species\")]\n\n\nErste Zeilen anzeigen\ndf.head(3)\nhead(df, 3)\n\n\nZeile nach Position\ndf.iloc[0]\ndf[1, ]\n\n\nWert aus Zeile & Spalte\ndf.iloc[0][\"sepal_length\"]\ndf[1, \"sepal_length\"]\n\n\nZeilen nach Bedingung\ndf[df[\"sepal_length\"] &gt; 6]\ndf[df$sepal_length &gt; 6, ]\n\n\n\n\n\n7.6.1 Beispiele:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\nHands-on 5: Zeilen und Spalten auswählen\n\n\n\n\nGeben Sie nur die Spalte \"petal_width\" aus.\n\nZeigen Sie die ersten 5 Zeilen des Datensatzes mit df.head().\n\nGeben Sie \"sepal_length\" und \"sepal_width\" aus.\n\nGeben Sie den Wert in Zeile 1, Spalte \"species\" aus.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Python: Einführung für Neurowissenschaften</span>"
    ]
  },
  {
    "objectID": "intro_python.html#daten-filtern-und-neue-variablen-erstellen",
    "href": "intro_python.html#daten-filtern-und-neue-variablen-erstellen",
    "title": "7  Python: Einführung für Neurowissenschaften",
    "section": "7.7 Daten filtern und neue Variablen erstellen",
    "text": "7.7 Daten filtern und neue Variablen erstellen\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nHands-on 6: Filtern und neue Spalten berechnen\n\n\n\n\nFiltern Sie die Zeilen mit sepal_length &gt; 6.\nErstellen Sie eine neue Spalte namens petal_ratio, die petal_length / petal_width ist.\nWas ist der Mittelwert der neuen Spalte?\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nHands-on 7: Recap\n\n\n\n\nFiltern Sie nur die virginica-Blumen.\nErstellen Sie eine neue Spalte: sepal_area = sepal_length * sepal_width\nWas ist der Maximalwert von petal_length bei virginica?\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Python: Einführung für Neurowissenschaften</span>"
    ]
  },
  {
    "objectID": "intro_python.html#zusammenfassung-r-vs-python",
    "href": "intro_python.html#zusammenfassung-r-vs-python",
    "title": "7  Python: Einführung für Neurowissenschaften",
    "section": "7.8 Zusammenfassung: R vs Python",
    "text": "7.8 Zusammenfassung: R vs Python\n\n\n\n\n\n\n\n\nAufgabe\nR\nPython\n\n\n\n\nPaket laden\nlibrary(ggplot2)\nimport seaborn as sns\n\n\nVektor erstellen\nc(1, 2, 3)\nnp.array([1, 2, 3])\n\n\nListe erstellen\nlist(1, 2, 3)\n[1, 2, 3]\n\n\nBenannte Liste\nlist(id = 1)\n{\"id\": 1}\n\n\nCSV einlesen\nread.csv(\"data.csv\")\npd.read_csv(\"data.csv\")\n\n\nErste Zeilen anzeigen\nhead(df)\ndf.head()\n\n\nSpalte auswählen\ndf$col\ndf[\"col\"]\n\n\nMehrere Spalten\ndf[, c(\"a\", \"b\")]\ndf[[\"a\", \"b\"]]\n\n\nZeile(n) nach Position\ndf[1, ]\ndf.iloc[0]\n\n\nWert aus Zeile und Spalte\ndf[1, \"col\"]\ndf.iloc[0][\"col\"]\n\n\nZeilen filtern\ndf[df$col &gt; 3, ]\ndf[df[\"col\"] &gt; 3]\n\n\nMittelwert berechnen\nmean(df$col)\ndf[\"col\"].mean()\n\n\nStandardabweichung\nsd(df$col)\ndf[\"col\"].std()\n\n\nNeue Spalte hinzufügen\ndf$new &lt;- df$a / df$b\ndf[\"new\"] = df[\"a\"] / df[\"b\"]\n\n\nHistogram\nhist(df$col)\nsns.histplot(df[\"col\"])\n\n\nBoxplot\nboxplot(col ~ group, data=df)\nsns.boxplot(x=\"group\", y=\"col\", data=df)\n\n\n\n\n\n\n\n\n\n💡 Lösungen zu den Hands-on Aufgaben\n\n\n\n\n\n\n7.8.1 Hands-on 1\nmeine_liste = [10, 20, 30]\nmeine_liste.append(40)\nprint(meine_liste)\n\nmein_tupel = (10, 20, 30)\n# mein_tupel.append(40)  # Fehler! Tupel unterstützen .append() nicht\n\n\n\n7.8.2 Hands-on 2\nimport numpy as np\nx = np.array([5, 10, 15])\nprint(x * 3)  # [15 30 45]\n\n\n\n7.8.3 Hands-on 3\na = [2, 4, 6]\nprint(a * 2)  # [2, 4, 6, 2, 4, 6]\n\nb = np.array([2, 4, 6])\nprint(b * 2)  # [4 8 12]\n\n\n\n7.8.4 Hands-on 4\nprint(df.head(10))\nprint(df.describe())\nprint(df[\"petal_width\"].mean())\nprint(df[\"petal_width\"].min())\n\n\n\n7.8.5 Hands-on 5\n# 1. Nur die Spalte \"petal_width\"\nprint(df[\"petal_width\"])\n\n# 2. Erste 5 Zeilen\nprint(df.head(5))\n\n# 3. Zwei Spalten anzeigen\nprint(df[[\"sepal_length\", \"sepal_width\"]])\n\n# 4. Wert aus Zeile 1, Spalte \"species\"\nprint(df.iloc[0][\"species\"])\n\n\n\n7.8.6 Hands-on 6\ngefiltert = df[df[\"sepal_length\"] &gt; 6]\ndf[\"petal_ratio\"] = df[\"petal_length\"] / df[\"petal_width\"]\nprint(df[\"petal_ratio\"].mean())\n\n\n\n7.8.7 Hands-on 7\n# 1. Nur die virginica\nvirginica = df[df[\"species\"] == \"virginica\"]\n\n# 2. Neue Spalte: sepal_area = sepal_length * sepal_width\nvirginica[\"sepal_area\"] = virginica[\"sepal_length\"] * virginica[\"sepal_width\"]\n\n# 3. Maximalwert von petal_length\nprint(virginica[\"petal_length\"].max())",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Python: Einführung für Neurowissenschaften</span>"
    ]
  },
  {
    "objectID": "loops.html",
    "href": "loops.html",
    "title": "8  Schleifen programmieren mit Python",
    "section": "",
    "text": "8.1 for-Schleifen\nDie for-Schleife wird oft genutzt, um über eine Liste oder eine andere Sequenz zu iterieren.\nIn dieser Konsole können Sie Python-Code schreiben, verändern und ausführen:\nMan kann eine for-Schleife auch mit range() verwenden:",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Schleifen programmieren mit Python</span>"
    ]
  },
  {
    "objectID": "loops.html#for-schleifen",
    "href": "loops.html#for-schleifen",
    "title": "8  Schleifen programmieren mit Python",
    "section": "",
    "text": "hirnregionen = [\"Frontallappen\", \"Okzipitallappen\", \"Temporallappen\"] \nfor hirnregion in hirnregionen: \n    print(\"Ich mag den\", hirnregion, \"!\")\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nfor i in range(5):  # Iteriert von 0 bis 4\n    print(\"Iteration Nummer:\", i)\n    \n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Schleifen programmieren mit Python</span>"
    ]
  },
  {
    "objectID": "loops.html#while-schleifen",
    "href": "loops.html#while-schleifen",
    "title": "8  Schleifen programmieren mit Python",
    "section": "8.2 while-Schleifen",
    "text": "8.2 while-Schleifen\nEine while-Schleife wird benutzt, wenn die Anzahl der Iterationen nicht im Voraus bekannt ist, sondern von einer Bedingung abhängt.\nzaehler = 0\nwhile zaehler &lt; 3:\n    print(\"Dies ist Schleifeniteration:\", zaehler)\n    zaehler = zaehler + 1  # Erhöht den Zähler, um eine Endlosschleife zu vermeiden\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\nHands-on 1-3: for und while-Schleifen erstellen\n\n\n\n\nGeben Sie die Zahlen von 1 bis 10 mit einer for-Schleife aus.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nGeben Sie “Python macht Spaß!” fünfmal mit einer while-Schleife aus.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nErstellen Sie eine Schleife, die nur gerade Zahlen von 1 bis 20 ausgibt.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Schleifen programmieren mit Python</span>"
    ]
  },
  {
    "objectID": "loops.html#flowcharts",
    "href": "loops.html#flowcharts",
    "title": "8  Schleifen programmieren mit Python",
    "section": "8.3 Flowcharts",
    "text": "8.3 Flowcharts\nBeispiel Flowchart:\ni = 1\nwhile i &lt;= 100:\n    print(i)\n    if i == 39:\n        i = 61\n    else:\n        i = i + 1\n\n\n\nhttps://de.wikipedia.org/wiki/Programmablaufplan#/media/Datei:Flowchart_de.svg\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\nHands-on 4: Flowchart erstellen\n\n\n\nErstellen Sie eine Flowchart für die for-Schleife in Aufgabe 3.\nSie können diese auf https://app.diagrams.net/ erstellen.\n\n\n\n\n\n\n\n\nHands-on 5: Fortgeschrittene Übung: Donuts-Essen\n\n\n\nErstellen Sie eine Flowchart für den folgenden Code:\ndonuts = 5\nwhile donuts &gt; 0:\n    print(\"Ich esse einen Donut. Lecker!\")\n    donuts = donuts - 1\n    if donuts == 1:\n        print(\"Oh nein! Nur noch ein Donut übrig!\")\n    elif donuts == 0:\n        print(\"Keine Donuts mehr... Zeit, neue zu kaufen!\")\nHier können Sie ausprobieren, was der Code macht. Passt das zu Ihrer Flowchart?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Schleifen programmieren mit Python</span>"
    ]
  },
  {
    "objectID": "loops.html#wichtig-endlosschleifen-vermeiden",
    "href": "loops.html#wichtig-endlosschleifen-vermeiden",
    "title": "8  Schleifen programmieren mit Python",
    "section": "8.4 Wichtig: Endlosschleifen vermeiden",
    "text": "8.4 Wichtig: Endlosschleifen vermeiden\nSchleifen müssen immer eine Bedingung haben, die sie beendet. Sonst könnte folgendes passieren:\ni = 0\nwhile i &lt; 1:\n    i = i - 1\n    print(i)  # Diese Schleife läuft endlos!\nHier fehlt eine Bedingung, die i wieder größer macht, sodass die Schleife stoppt.\nProbieren Sie es aus (wenn Sie einen Crash Ihres Browsertabs nicht scheuen…?).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Schleifen programmieren mit Python</span>"
    ]
  },
  {
    "objectID": "loops.html#fazit",
    "href": "loops.html#fazit",
    "title": "8  Schleifen programmieren mit Python",
    "section": "8.5 Fazit",
    "text": "8.5 Fazit\nSchleifen sind ein mächtiges Werkzeug um wiederkehrende Aufgaben effizient zu lösen. Schleifen werden fast überall benutzt: Experimente programmieren, Daten einlesen, Daten bearbeiten, Grafiken erstellen, usw.\n\n\n\n\n\n\n\nHands-on Lösungen\n\n\n\n\n\n\n8.5.1 Hands-on 1\nfor i in range(10):\n    print(i + 1)\nAlternativ:\nzahlen = [1,2,3,4,5,6,7,8,9,10]\nfor z in zahlen:\n    print(z)\nOder:\nzahlen = 1\nwhile zahlen &lt;= 10:\n    print(zahlen)\n    zahlen = zahlen + 1\n\n\n8.5.2 Hands-on 2\ni = 0\nwhile i &lt; 5:\n    print(\"Python macht Spaß!\")\n    i = i + 1\n\n\n8.5.3 Hands-on 3\nn = 2\nwhile n &lt;= 20:\n    print(n)\n    n = n + 2\n\n\n8.5.4 Hands-on 4\n\n\n\n\n\n\n\n8.5.5 Hands-on 5",
    "crumbs": [
      "Experimentieren & Programmieren",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Schleifen programmieren mit Python</span>"
    ]
  },
  {
    "objectID": "uebung_1.html",
    "href": "uebung_1.html",
    "title": "Übung 1",
    "section": "",
    "text": "Auftrag\nFühren Sie selbst und mit 2 weiteren Personen das Stroop und das Random Dot Experiment durch. Laden Sie anschliessend die 6 Datensätze auf Ilias hoch. Die beiden Experimente dauern zusammen ca. 30 Minuten ( abhängig von den Versuchspersonen).\nFühren Sie das Experiment mit der Einstellung Run durch und nicht im Pilot-Mode!\nWichtig:\nDie erhobenen Daten werden wir dann in den kommenden Sitzungen verwenden, achten Sie also auf gute Datenqualität.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "Übung 1"
    ]
  },
  {
    "objectID": "uebung_1.html#vorgehen",
    "href": "uebung_1.html#vorgehen",
    "title": "Übung 1",
    "section": "Vorgehen",
    "text": "Vorgehen\n\nLaden Sie die 2 Experimente herunter und testen Sie, ob Sie einwandfrei laufen. Die Experimente befinden sich auf Github. Sie können sie unter den untenstehenden Links downloaden. Klicken Sie dafür auf den ZIP-Ordner, und dann auf View Raw oder auf das Icon mit ... und dort auf Download. Sie müssen das File dann evtl. entzippen, bevor Sie das Experiment starten können. Bei Problemen finden Sie unten einen Abschnitt Troubleshooting. Wenn das nichts hilft, können Sie sich bei der nächsten Veranstaltung an uns wenden.\n\nStroop Experiment\nRandom Dot Experiment\n\nFühren Sie selber die beiden Experimente durch.\n\nStellen Sie sicher, dass hier ein vollständiger Datensatz abgespeichert wird. Testen Sie erst dann zusätzliche Personen.\n\nLassen Sie 2 weitere Personen die beiden Experimente ausführen (jede Person soll beide Experimente ausführen).\n\nDie Personen müssen zwischen 18 und 60 Jahren alt sein.\nDie Personen sollten eine normale oder korrigiert-zu-normale (Brille/Kontaktlinsen) Sehstärke haben.\nKeine Mitstudierenden aus dem Computerlab testen.\nAchten Sie darauf, dass die Personen die Aufgaben konzentriert und ohne Ablenkung lösen können.\n\nLaden Sie die 6 Datensätze auf ILIAS hoch.\n\nLaden Sie die 6 .csv/.xlsx-Files mit den erhobenen Datensätzen auf Ilias unter Übung 1 hoch.\n\n\nDie Datensätze finden Sie im Experimentordner (dort wo das .psyexp-File des Experiments gespeichert ist) einen Ordner data. In diesem Ordner sind mehrere Dateien vorhanden. Sie müssen nur die .csv/.xlsx-Files pro Person hochladen, die anderen Files werden nicht benötigt. Die Datei muss einige KB gross sein, sonst hat etwas nicht geklappt (z.B. wenn die Datei nur 2 KB gross ist).",
    "crumbs": [
      "Experimentieren & Programmieren",
      "Übung 1"
    ]
  },
  {
    "objectID": "uebung_1.html#abgabetermin",
    "href": "uebung_1.html#abgabetermin",
    "title": "Übung 1",
    "section": "Abgabetermin",
    "text": "Abgabetermin\n14. März 2024 23:55",
    "crumbs": [
      "Experimentieren & Programmieren",
      "Übung 1"
    ]
  },
  {
    "objectID": "uebung_1.html#trouble-shooting",
    "href": "uebung_1.html#trouble-shooting",
    "title": "Übung 1",
    "section": "Trouble shooting",
    "text": "Trouble shooting\nBitte die Fehlermeldung im Fenster genau durchlesen. Dort finden Sie Hinweise darauf, was schief gelaufen ist.\nDas Experiment startet nicht.\n\nUnter Einstellungen (Radsymbol) den Reiter Basic auswählen. Bei Use PsychoPy version die neuste PsychoPy Version 2024.2.4 auswählen.\nUnter Einstellungen (Radsymbol) den Reiter Input auswählen. Bei Keyboard backend (statt ioHub) PsychToolbox auswählen.\n\nDas Experiment startet zwar, der Bildschirm ist aber dann einfach für eine kurze Zeit grau und das Fenster schliesst sich wieder.\n\nZugriffsrechte gegeben? (Bei Windows: Als Administrator starten, bei MacOS: Zugriffsrechte erteilen)\nUnter Einstellungen (Radsymbol) den Reiter Input auswählen. Keyboard Backend auf PsychToolbox statt ioHub setzen.",
    "crumbs": [
      "Experimentieren & Programmieren",
      "Übung 1"
    ]
  },
  {
    "objectID": "datawrangling.html",
    "href": "datawrangling.html",
    "title": "9  Daten importieren und vorverarbeiten",
    "section": "",
    "text": "9.1 Datenformate\nBevor mit einem Datensatz gearbeitet wird, empfiehlt es sich den Datensatz anzuschauen und Folgendes zu identifizieren:",
    "crumbs": [
      "Data wrangling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Daten importieren und vorverarbeiten</span>"
    ]
  },
  {
    "objectID": "datawrangling.html#datenformate",
    "href": "datawrangling.html#datenformate",
    "title": "9  Daten importieren und vorverarbeiten",
    "section": "",
    "text": "In welchem Dateiformat ist der Datensatz gespeichert? (z.B. in .csv, .xlsx oder anderen?)\nIn welchem Datenformat ist der Datensatz geordnet? (long oder wide oder mixed?)\nGibt es ein data dictionary mit Erklärungen zu den Variablen?",
    "crumbs": [
      "Data wrangling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Daten importieren und vorverarbeiten</span>"
    ]
  },
  {
    "objectID": "datawrangling.html#set-up",
    "href": "datawrangling.html#set-up",
    "title": "9  Daten importieren und vorverarbeiten",
    "section": "9.2 Set up",
    "text": "9.2 Set up\n\n\n\n\n\n\nHands-on: Vorbereitung\n\n\n\n\nÖffnen Sie RStudio.\nErstellen Sie ein neues RStudio-Project.\n\nKlicken Sie dafür auf File &gt; New Project\nBenennen Sie das Project complab_fs25 und speichern Sie es an einem sinnvollen Ort auf Ihrem Computer. Wählen Sie, dass dafür ein neuer Ordner erstellt werden soll.\n\nErstellen Sie in diesem Projekt-Ordner einen Ordner namens data.\nKopieren Sie in den data-Ordner Ihre erhobenen Daten des Stroop Experiments. Falls Sie noch keine Daten erhoben haben, dann laden Sie hier einen Beispiels-Datensatz herunter und speichern Sie ihn im data-Ordner.\nErstellen Sie ein neues RScript oder ein RNotebook (.Rmd-File: File &gt; New File &gt; RNotebook) und speichern Sie dieses unter intro_datawrangling im Projekt-Ordner.\n\n\n\n\n\n\n\n\n\nTipp: Namensgebung für Files und Variablen\n\n\n\nWenn Sie Filenamen auswählen, achten Sie darauf dass diese machine-readable sind:\n\nkeine Lücken (verwenden Sie stattdessen den camelCase, den snake_case oder -)\nkeine ä, ö, ü oder andere Sonderzeichen verwenden",
    "crumbs": [
      "Data wrangling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Daten importieren und vorverarbeiten</span>"
    ]
  },
  {
    "objectID": "datawrangling.html#packages-installieren-und-laden",
    "href": "datawrangling.html#packages-installieren-und-laden",
    "title": "9  Daten importieren und vorverarbeiten",
    "section": "9.3 Packages installieren und laden",
    "text": "9.3 Packages installieren und laden\nFür das Bearbeiten der Daten verwenden eignen sich Funktionen aus dem Package {tidyverse}, eine Sammlung von verschiedenen, für data science sehr geeigneten R Packages. Funktionen aus dem {tidyverse} ermöglichen und vereinfachen viele Schritte der Datenverarbeitung. Im Folgenden werden die wichtigsten und häufigst verwendeten Funktionen beschrieben. Das {tidyverse} können Sie direkt in R herunterladen:\n\nMehr Informationen zum {tidyverse} finden Sie hier.\n\n\n# Download und Installieren des Packages (nur einmal ausführen)\ninstall.packages(\"tidyverse\")\n\nEin Package muss nur einmal heruntergeladen und installiert werden, danach ist es lokal auf dem Computer gespeichert. Aber: Jedes Mal wenn R geöffnet wird, müssen Packages wieder neu geladen werden.\n\n# Package laden (bei jedem Öffnen von R zu Beginn des Skripts ausführen)\nlibrary(\"tidyverse\") \n\nSobald ein Package installiert ist, können die Funktionen auch verwendet werden ohne, dass das ganze Package mit library() geladen wird, indem die Funktion mit dem Package-Namen zusammen aufgerufen wird: packagename::packagefunction(). Dies macht Sinn, wenn verschiedene Packages dieselben Namen für verschiedene Funktionen nutzen und es so zu Konflikten kommt oder wenn nur eine Funktion aus einem Package verwendet werden soll und alle anderen sowieso nicht gebraucht werden.",
    "crumbs": [
      "Data wrangling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Daten importieren und vorverarbeiten</span>"
    ]
  },
  {
    "objectID": "datawrangling.html#daten-importieren-in-r-read.csv",
    "href": "datawrangling.html#daten-importieren-in-r-read.csv",
    "title": "9  Daten importieren und vorverarbeiten",
    "section": "9.4 Daten importieren in R: read.csv()",
    "text": "9.4 Daten importieren in R: read.csv()\nEinen Datensatz in .csv-Format kann mit der Funktion read.csv() importiert werden. Teilweise muss innerhalb der Klammer zusätzlich der Separator mit sep = \",\" angegeben werden, also mit welchem Zeichen die Spalten getrennt sind. Normalerweise ist dies , in .csv (comma separated values), es kann aber auch ;, . oder eine Lücke  sein.\n\n# Daten laden und anschauen\nd_stroop &lt;- read.csv(\"data/stroop_example.csv\", sep = \",\")\nglimpse(d_stroop)\n\n\n\n\n\n\n\nHands-on: Daten einlesen\n\n\n\nLesen Sie den Stroop Datensatz in Ihrem data-Ordner ein und schauen Sie ihn dann mit den Funktionen glimpse() und head() an.\n\nWelche Variablen sind wichtig für die weitere Auswertung?\nWelche braucht es wahrscheinlich nicht mehr?\nFinden Sie Versuchspersonenidentifikation? / Reaktionszeit? / Antwort der Versuchsperson?\n\n\n\n\n\n\n\n\n\nTipp: Daten anderer Formate einlesen\n\n\n\nFalls Sie eine Excel-Datei einlesen möchten, können Sie dies mit der read_excel()-Funktion aus dem Package readxl() tun: readxl::read_excel().\nFalls Sie nicht wissen, mit welcher Funktion Sie Ihre Daten einlesen können, können Sie dies in RStudio ausprobieren indem Sie beim Reiter Environment auf Import Dataset klicken und dort Ihren Datensatz auswählen oder über File &gt; Import Dataset. Sie können dort diverse Einstellungen tätigen. In der R Console können Sie dann den Code sehen, der zum Einlesen verwendet wurde und die dortige Funktion in Ihren Code kopieren.",
    "crumbs": [
      "Data wrangling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Daten importieren und vorverarbeiten</span>"
    ]
  },
  {
    "objectID": "datawrangling.html#verwenden-der-pipe-oder",
    "href": "datawrangling.html#verwenden-der-pipe-oder",
    "title": "9  Daten importieren und vorverarbeiten",
    "section": "9.5 Verwenden der Pipe: |> oder %>%",
    "text": "9.5 Verwenden der Pipe: |&gt; oder %&gt;%\nIn R können Sie die Pipe verwenden um mehrere Datenverarbeitungsschritte aneinander zu hängen. Damit sparen Sie sich aufwändige Zwischenschritte und vermeiden das Erstellen von immer neuen Datensätzen. Statt zwei einzelne Datenverarbeitungsschritte zu machen wie oben, können mehrere Schritte (hier Daten einlesen und anzeigen) zusammengefasst werden, in dem nach Zeilenende eine Pipe eingefügt wird:\n\nWann Pipes ungeeignet sind wird hier beschrieben.\n\n\nd_stroop &lt;- read.csv(\"data/stroop_example.csv\", sep = \",\") |&gt;\n    glimpse()\n\nDie Base R Pipe |&gt; und die magritter Pipe %&gt;%_ unterscheiden sich in Details, in diesem Kapitel spielt es jedoch keine Rolle, welche Pipe Sie verwenden.\n\n\n\n\n\n\nTipp\n\n\n\nAchtung: Wenn wir zu Beginn ein &lt;- oder = verwenden, wird alles was nach der Pipe kommt wird ebenfalls im Datensatz verändert. Wird z.B. der Code …\n\nd_stroop &lt;- read.csv(\"data/stroop_example.csv\", sep = \",\") |&gt;\n    head()\n\n…eingegeben, besteht der Datensatz d_stroop dann nur noch aus 6 Zeilen, weil die Funktion head() den Datensatz auf die ersten 6 Zeilen kürzt.\nWird die Pipe ohne &lt;- oder = verwendet, bleibt der Datensatz unverändert:\n\nd_stroop |&gt;\n    head()",
    "crumbs": [
      "Data wrangling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Daten importieren und vorverarbeiten</span>"
    ]
  },
  {
    "objectID": "datawrangling.html#daten-filtern-filter",
    "href": "datawrangling.html#daten-filtern-filter",
    "title": "9  Daten importieren und vorverarbeiten",
    "section": "9.6 Daten filtern: filter()",
    "text": "9.6 Daten filtern: filter()\nMit filter() können bestimmte Beobachtungen oder Untergruppen ausgewählt werden. Hierfür muss in der Funktion filter(.data, filter, ...) der Datensatz, die betreffende Variable, sowie eine Bedingung eingegeben werden. Es wird die ganze Zeile im Datensatz behalten in der die Variable der Bedingung entspricht.\nBeispiele:\n\n# nur die Trials mit den rot angezeigten Wörtern behalten\nd_stroop_filtered &lt;- filter(d_stroop, color == \"red\")\n\n# dasselbe mit der Pipe\nd_filtered &lt;- d_stroop |&gt; filter(color == \"red\")\n\n# nur Trials die ohne blau angezeigten Wörter behalten\nd_filtered &lt;- d_stroop |&gt; filter(color != \"blue\")\n\n# nur Übungstrials mit einer Antwortszeit unter oder gleich gross wie 1 Sekunde behalten\nd_filtered &lt;- d_stroop |&gt; filter(respPractice.rt &lt;= 1)\n\n# nur Übungstrials mit Antwortzeiten zwischen 1 und 2 Sekunden behalten\nd_filtered &lt;- d_stroop |&gt; filter(respPractice.rt &gt; 1 & respPractice.rt &lt; 2)\n\n# mehrere Filter verwenden\nd_filtered &lt;- d_stroop |&gt; \n    filter(color == \"red\") |&gt;\n    filter(respPractice.rt &lt;= 1)\n\nIn unserem Datensatz möchten wir nur die gültigen Experimentdaten behalten, die Color-To-Key (ctk) Bedingung sowie die Practice Trials möchten wir ausschliessen.\nDie Variable trials_test.thisN enthält die Trialnummer, sie enthält nur Einträge, während der gültigen Trials. Wir können dies nutzen und alle Zeilen behalten in welchen die Zelle der Variable trials_test.thisN nicht leer ist:\n\nd_stroop &lt;- d_stroop |&gt; \n    filter(!is.na(trials_test.thisN)) \n\n\n\n\n\n\n\nHands-on: Daten filtern\n\n\n\nErstellen Sie einen neuen Datensatz namens d_stroop_correct und filtern Sie diesen so dass er nur Trials mit richtigen Antworten enthält. Schauen Sie in der Variable keyResp_test_run.corr, ob tatsächlich nur noch richtige Antworten übrig geblieben sind.\nAchtung: Arbeiten Sie in den weiteren Schritten nicht mit diesem Datensatz weiter, da wir die falschen Antworten in einem nächsten Schritt noch im Datensatz brauchen.",
    "crumbs": [
      "Data wrangling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Daten importieren und vorverarbeiten</span>"
    ]
  },
  {
    "objectID": "datawrangling.html#variablen-auswählen-select",
    "href": "datawrangling.html#variablen-auswählen-select",
    "title": "9  Daten importieren und vorverarbeiten",
    "section": "9.7 Variablen auswählen: select()",
    "text": "9.7 Variablen auswählen: select()\nEin komplexer Datensatz mit sehr vielen Variablen wird oft für die Analyse aus Gründen der Einfachheit oder Anonymisierung reduziert. Das bedeutet, dass man sich die nötigen Variablen herausliest, und nur mit diesem reduzierten Datensatz weiterarbeitet. Hierzu eignet sich die Funktion select() sehr gut: Mit select(.data, variablenname, ...) können die zu behaltenden Variablen ausgewählt werden. Wird ein ! vor einen Variablennamen gesetzt, wird die Variable nicht behalten.\nMit select() können wir auch die Variablen sortieren und umbenennen, damit unser Datensatz so strukturiert ist, wie wir ihn gebrauchen können.\nBeispiele:\n\n# Variable word und color behalten ohne Pipe\nd_simpler &lt;- select(d_stroop, word, color)\n\n# Variable word und color behalten mit Pipe\nd_simpler &lt;- d_stroop |&gt; select(word, color)\n\n# alle Variablen ausser word behalten\nd_simpler &lt;- d_stroop |&gt; select(!word)\n\n# Variablennamen verändern\nd_simpler &lt;- d_stroop |&gt; select(newvariablename = word)\n\nSollen mehrere Variablen am Stück ausgewählt werden, kann man die erste Variable in der Reihe (z.B. word) und die letzte in der Reihe (z.B. congruent) als word:congruent eingeben, dann werden auch alle dazwischen liegenden Variablen ausgewählt.\n\n\n\n\n\n\nHands-on: Variablen auswählen\n\n\n\nSchauen Sie sich Ihren Datensatz an, welche Variablen benötigen Sie für die weitere Analysen?\nErstellen Sie einen neuen Datensatz d_stroop_clean in welchem Sie die entsprechenden Variablen auswählen und umbennen, wenn Sie Ihnen zu lange/kompliziert erscheinen.\nUntenstehend finden Sie ein Beispiel, wie der Datensatz danach aussehen könnte.\n\n\n\n\nRows: 120\nColumns: 10\n$ id         &lt;chr&gt; \"sub-154989\", \"sub-154989\", \"sub-154989\", \"sub-154989\", \"su…\n$ experiment &lt;chr&gt; \"stroop_test\", \"stroop_test\", \"stroop_test\", \"stroop_test\",…\n$ trial      &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ word       &lt;chr&gt; \"rot\", \"rot\", \"blau\", \"gelb\", \"rot\", \"blau\", \"blau\", \"gelb\"…\n$ color      &lt;chr&gt; \"red\", \"red\", \"blue\", \"yellow\", \"yellow\", \"yellow\", \"red\", …\n$ corrAns    &lt;chr&gt; \"r\", \"r\", \"b\", \"g\", \"g\", \"g\", \"r\", \"r\", \"b\", \"g\", \"b\", \"b\",…\n$ congruent  &lt;int&gt; 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,…\n$ response   &lt;chr&gt; \"r\", \"r\", \"b\", \"g\", \"g\", \"g\", \"r\", \"r\", \"b\", \"g\", \"b\", \"b\",…\n$ rt         &lt;dbl&gt; 1.0639791, 0.7370255, 1.1883303, 1.2007897, 1.6688681, 1.58…\n$ accuracy   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…",
    "crumbs": [
      "Data wrangling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Daten importieren und vorverarbeiten</span>"
    ]
  },
  {
    "objectID": "datawrangling.html#neue-variablen-generieren-und-verändern-mutate-und-case_when",
    "href": "datawrangling.html#neue-variablen-generieren-und-verändern-mutate-und-case_when",
    "title": "9  Daten importieren und vorverarbeiten",
    "section": "9.8 Neue Variablen generieren und verändern: mutate() und case_when()",
    "text": "9.8 Neue Variablen generieren und verändern: mutate() und case_when()\nMit der mutate(.data, …) Funktion können im Datensatz neue Variablen generiert oder bestehende verändert werden.\nBeispiel:\n\n# Neue Variablen erstellen\nd_new &lt;- d_stroop_clean |&gt;\n    mutate(num_variable = 1.434,\n           chr_variable = \"1.434\",\n           sumofxy_variable = rt + 1,\n           copy_variable = word)\n\n# Bestehende Variablen verändern\nd_new &lt;- d_new |&gt;\n    mutate(num_variable = num_variable * 1000) # z.B. um Sekunden zu Millisekunden zu machen\n\nMit case_when() kann eine neue Variable erstellt werden in Abhängigkeit von Werten anderer Variablen. Damit kann z.B. eine Variable accuracy erstellt werden, die den Wert correct hat, wenn die Aufgabe richtig gelöst wurde (z.B. Bedingung rot und Tastendruck r) und sonst den Wert error hat.\nBeispiel:\n\nd_condvariable &lt;- d_stroop_clean |&gt;\n    mutate(cond_variable = case_when(rt &gt; 0.8 ~ \"higher\",\n                                     rt &lt;= 0.8 ~ \"lower\",\n                                     .default = NA))\n\n\n\n\n\n\n\nHands-on: Variablen generieren und verändern\n\n\n\n\nErstellen Sie im Datensatz d_stroop_clean eine neue Variable mit dem Namen researcher, den Ihren Namen enthält.\nErstellen Sie zudem eine Variable accuracy_check, mit correct für korrekte und error für inkorrekte Trials. Kontrollieren Sie mit der Variable keyResp_test_run.corr (oder Ihrem neuen Variablennamen, wenn Sie diese umbenannt haben) im Datensatz, ob Sie die Aufgabe richtig gelöst haben.\nÄndern Sie die Trialnummer, so dass sie nicht mehr mit 0 beginnt, sondern mit 1.",
    "crumbs": [
      "Data wrangling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Daten importieren und vorverarbeiten</span>"
    ]
  },
  {
    "objectID": "datawrangling.html#variablenklasse-verändern-as.factor-as.numeric",
    "href": "datawrangling.html#variablenklasse-verändern-as.factor-as.numeric",
    "title": "9  Daten importieren und vorverarbeiten",
    "section": "9.9 Variablenklasse verändern: as.factor(), as.numeric(), …",
    "text": "9.9 Variablenklasse verändern: as.factor(), as.numeric(), …\nVariablen können verschiedene Klassen haben, sie können z.B. kategoriale (factor, character) oder numerische (integer, numeric, double) Informationen enthalten. Beim Einlesen “rät” R, welche Klasse eine Variable hat. Teilweise möchten wir dies ändern. Wenn wir eine Variable zu einem Faktor machen möchten, verwenden wir as.factor(). Dies macht z.B. Sinn, wenn die Versuchspersonennummer als Zahl eingelesen wurde. Um von einem Faktor zu einer numerischen Variable zu kommen, verwenden wir as.numeric().\n\n# Die Variable \"congruent\" zu einem Faktor machen\nd_stroop_clean |&gt; \n    mutate(congruent = as.factor(congruent))\n\n\n\n\n\n\n\nHands-on: Variablenklassen\n\n\n\nSchauen Sie sich den Datensatz mit glimpse() oder mit View() an. Welche Klassen enthält Ihr Datensatz und was bedeuten Sie?",
    "crumbs": [
      "Data wrangling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Daten importieren und vorverarbeiten</span>"
    ]
  },
  {
    "objectID": "datawrangling.html#daten-gruppieren-und-zusammenfassen-group_by-und-summarise",
    "href": "datawrangling.html#daten-gruppieren-und-zusammenfassen-group_by-und-summarise",
    "title": "9  Daten importieren und vorverarbeiten",
    "section": "9.10 Daten gruppieren und zusammenfassen: group_by() und summarise()",
    "text": "9.10 Daten gruppieren und zusammenfassen: group_by() und summarise()\nMit diesen beiden Funktionen können wir mit wenig Code den Datensatz gruppieren und zusammenfassen.\n\n# Nach Wörter gruppieren\nd_stroop_clean |&gt; group_by(word) |&gt;\n    summarise(mean_rt = mean(rt),\n              sd_rt = sd(rt))\n\n# A tibble: 3 × 3\n  word  mean_rt sd_rt\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 blau     1.23 0.490\n2 gelb     1.26 0.447\n3 rot      1.16 0.552\n\n\n\n\n\n\n\n\nHands-on: Daten zusammenfassen\n\n\n\nErstellen Sie einen neuen Datensatz d_stroop_summary\n\nGruppieren Sie den Datensatz für Wortfarbe und Kongruenz.\nFassen Sie für diese Gruppen die durchschnittliche Antwortzeit und Accuracy sowie die Standardabweichungen zusammen.",
    "crumbs": [
      "Data wrangling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Daten importieren und vorverarbeiten</span>"
    ]
  },
  {
    "objectID": "datawrangling.html#datensätze-speichern-write.csv",
    "href": "datawrangling.html#datensätze-speichern-write.csv",
    "title": "9  Daten importieren und vorverarbeiten",
    "section": "9.11 Datensätze speichern: write.csv()",
    "text": "9.11 Datensätze speichern: write.csv()\n\nwrite.csv(d_stroop_clean, file = \"data/dataset_stroop_clean.csv\", row.names = FALSE)\n\n\n\n\n\n\n\nHands-on: Datensätze speichern\n\n\n\nSpeichern Sie einen neuen Datensatz mit den vorverarbeiteten Daten.",
    "crumbs": [
      "Data wrangling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Daten importieren und vorverarbeiten</span>"
    ]
  },
  {
    "objectID": "datawrangling.html#data-wrangling-workflow-implementieren",
    "href": "datawrangling.html#data-wrangling-workflow-implementieren",
    "title": "9  Daten importieren und vorverarbeiten",
    "section": "9.12 Data wrangling workflow implementieren",
    "text": "9.12 Data wrangling workflow implementieren\n\n\n\n\n\n\nHands-on: Data wrangling workflow\n\n\n\nErstellen Sie nun ein Projekt für das Random-Dot Experiment und führen Sie die gelernten data wrangling Schritte selbstständig durch.\n\n\n\nZu den gelernten Funktionen finden Sie hier Grafiken die evtl. helfen, sich die Funktions-Namen zu merken.",
    "crumbs": [
      "Data wrangling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Daten importieren und vorverarbeiten</span>"
    ]
  },
  {
    "objectID": "datawrangling_automatisiert.html",
    "href": "datawrangling_automatisiert.html",
    "title": "10  Automatisiertes Preprocessing",
    "section": "",
    "text": "10.1 Setup\n# Package laden (bei jedem Öffnen von R zu Beginn des Skripts ausführen)\nlibrary(\"tidyverse\")",
    "crumbs": [
      "Data wrangling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Automatisiertes Preprocessing</span>"
    ]
  },
  {
    "objectID": "datawrangling_automatisiert.html#stroop-experiment-data-wrangling",
    "href": "datawrangling_automatisiert.html#stroop-experiment-data-wrangling",
    "title": "10  Automatisiertes Preprocessing",
    "section": "10.2 Stroop-Experiment data wrangling",
    "text": "10.2 Stroop-Experiment data wrangling\n\n# Daten vorverarbeiten\ndata_stroop &lt;- read_csv(\"data/stroop_example2.csv\")\nglimpse(data_stroop)\n\nd_stroop &lt;- read_csv(\"data/stroop_example2.csv\") |&gt;\n    filter(!is.na(trials_test.thisN)) |&gt;\n    mutate(trial = trials_test.thisN + 1,\n           condition = case_when(congruent == 1 ~ \"congruent\",\n                                 congruent == 0 ~ \"incongruent\")) |&gt;\n    select(id = participant, \n           trial,\n           word, \n           color,\n           congruent,\n           condition,\n           resp = keyResp_test_run.keys, \n           corr = keyResp_test_run.corr, \n           rt = keyResp_test_run.rt)",
    "crumbs": [
      "Data wrangling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Automatisiertes Preprocessing</span>"
    ]
  },
  {
    "objectID": "datawrangling_automatisiert.html#daten-mit-eigener-funktion-einlesen",
    "href": "datawrangling_automatisiert.html#daten-mit-eigener-funktion-einlesen",
    "title": "10  Automatisiertes Preprocessing",
    "section": "10.3 Daten mit eigener Funktion einlesen",
    "text": "10.3 Daten mit eigener Funktion einlesen\n\nread_stroop &lt;- function(path){\n    # Code kopiert von oben\n    d_stroop &lt;- read_csv(path) |&gt;\n    filter(!is.na(trials_test.thisN)) |&gt;\n    mutate(trial = trials_test.thisN + 1,\n           condition = case_when(congruent == 1 ~ \"congruent\",\n                                 congruent == 0 ~ \"incongruent\")) |&gt;\n    select(id = participant, \n           trial,\n           word, \n           color, \n           congruent, \n           condition,\n           resp = keyResp_test_run.keys, \n           corr = keyResp_test_run.corr, \n           rt = keyResp_test_run.rt)\n    # ---------------------\n    return(d_stroop)\n}\n\nd_stroop_fun &lt;- read_stroop(path = \"data/stroop_example2.csv\")\n\nRows: 161 Columns: 92\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (26): wordCTK, colorCTK, corrAnsCTK, wordPractice, colorPractice, corrAn...\ndbl (65): congruentCTK, congruentPractice, congruent, trials_CTK.thisRepN, t...\nlgl  (1): notes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\n\n\n\n\nHands-on 1: Eigene Funktion schreiben\n\n\n\nSchreiben Sie eine Funktion, die nur reaction times von &lt;0.5 wählt, und den Prozent von korrekten Antworten ausgibt.\n\nfast_correct &lt;- function(data){\n    ___ # Code einfügen\n}\n\nfast_correct(d_stroop)",
    "crumbs": [
      "Data wrangling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Automatisiertes Preprocessing</span>"
    ]
  },
  {
    "objectID": "datawrangling_automatisiert.html#einlesen-automatisieren",
    "href": "datawrangling_automatisiert.html#einlesen-automatisieren",
    "title": "10  Automatisiertes Preprocessing",
    "section": "10.4 Einlesen Automatisieren",
    "text": "10.4 Einlesen Automatisieren\nWir benötigen eine Liste aller Datenfiles. Die Funktion list.files() gibt einer Liste aller Dokumente in einem Ordner zurück. Mit dem Argument pattern = ... kann spezifiziert werden, welche Buchstaben(folgen) der Filenamen entahlten soll.\n\nlist.files(path = 'data/raw')\n\n  [1] \"sub-001_random_dot.csv\"  \"sub-001_stroop_test.csv\"\n  [3] \"sub-002_random_dot.csv\"  \"sub-002_stroop_test.csv\"\n  [5] \"sub-003_random_dot.csv\"  \"sub-003_stroop_test.csv\"\n  [7] \"sub-004_random_dot.csv\"  \"sub-004_stroop_test.csv\"\n  [9] \"sub-005_random_dot.csv\"  \"sub-005_stroop_test.csv\"\n [11] \"sub-006_random_dot.csv\"  \"sub-006_stroop_test.csv\"\n [13] \"sub-007_random_dot.csv\"  \"sub-007_stroop_test.csv\"\n [15] \"sub-008_random_dot.csv\"  \"sub-008_stroop_test.csv\"\n [17] \"sub-009_random_dot.csv\"  \"sub-009_stroop_test.csv\"\n [19] \"sub-010_random_dot.csv\"  \"sub-010_stroop_test.csv\"\n [21] \"sub-011_random_dot.csv\"  \"sub-011_stroop_test.csv\"\n [23] \"sub-012_random_dot.csv\"  \"sub-012_stroop_test.csv\"\n [25] \"sub-013_random_dot.csv\"  \"sub-013_stroop_test.csv\"\n [27] \"sub-014_random_dot.csv\"  \"sub-014_stroop_test.csv\"\n [29] \"sub-015_random_dot.csv\"  \"sub-015_stroop_test.csv\"\n [31] \"sub-016_random_dot.csv\"  \"sub-016_stroop_test.csv\"\n [33] \"sub-017_random_dot.csv\"  \"sub-017_stroop_test.csv\"\n [35] \"sub-018_random_dot.csv\"  \"sub-018_stroop_test.csv\"\n [37] \"sub-019_random_dot.csv\"  \"sub-019_stroop_test.csv\"\n [39] \"sub-020_random_dot.csv\"  \"sub-020_stroop_test.csv\"\n [41] \"sub-021_random_dot.csv\"  \"sub-021_stroop_test.csv\"\n [43] \"sub-022_random_dot.csv\"  \"sub-022_stroop_test.csv\"\n [45] \"sub-023_random_dot.csv\"  \"sub-023_stroop_test.csv\"\n [47] \"sub-024_random_dot.csv\"  \"sub-024_stroop_test.csv\"\n [49] \"sub-025_random_dot.csv\"  \"sub-025_stroop_test.csv\"\n [51] \"sub-026_random_dot.csv\"  \"sub-026_stroop_test.csv\"\n [53] \"sub-027_random_dot.csv\"  \"sub-027_stroop_test.csv\"\n [55] \"sub-028_random_dot.csv\"  \"sub-028_stroop_test.csv\"\n [57] \"sub-029_random_dot.csv\"  \"sub-029_stroop_test.csv\"\n [59] \"sub-030_random_dot.csv\"  \"sub-030_stroop_test.csv\"\n [61] \"sub-031_random_dot.csv\"  \"sub-031_stroop_test.csv\"\n [63] \"sub-032_random_dot.csv\"  \"sub-032_stroop_test.csv\"\n [65] \"sub-033_random_dot.csv\"  \"sub-033_stroop_test.csv\"\n [67] \"sub-034_random_dot.csv\"  \"sub-034_stroop_test.csv\"\n [69] \"sub-035_random_dot.csv\"  \"sub-035_stroop_test.csv\"\n [71] \"sub-036_random_dot.csv\"  \"sub-036_stroop_test.csv\"\n [73] \"sub-037_random_dot.csv\"  \"sub-037_stroop_test.csv\"\n [75] \"sub-038_random_dot.csv\"  \"sub-038_stroop_test.csv\"\n [77] \"sub-039_random_dot.csv\"  \"sub-040_random_dot.csv\" \n [79] \"sub-040_stroop_test.csv\" \"sub-041_random_dot.csv\" \n [81] \"sub-041_stroop_test.csv\" \"sub-042_random_dot.csv\" \n [83] \"sub-042_stroop_test.csv\" \"sub-043_random_dot.csv\" \n [85] \"sub-043_stroop_test.csv\" \"sub-044_random_dot.csv\" \n [87] \"sub-044_stroop_test.csv\" \"sub-045_random_dot.csv\" \n [89] \"sub-045_stroop_test.csv\" \"sub-046_random_dot.csv\" \n [91] \"sub-046_stroop_test.csv\" \"sub-047_random_dot.csv\" \n [93] \"sub-047_stroop_test.csv\" \"sub-048_random_dot.csv\" \n [95] \"sub-048_stroop_test.csv\" \"sub-049_random_dot.csv\" \n [97] \"sub-049_stroop_test.csv\" \"sub-050_random_dot.csv\" \n [99] \"sub-050_stroop_test.csv\" \"sub-051_random_dot.csv\" \n[101] \"sub-051_stroop_test.csv\" \"sub-052_random_dot.csv\" \n[103] \"sub-052_stroop_test.csv\" \"sub-053_random_dot.csv\" \n[105] \"sub-053_stroop_test.csv\" \"sub-054_random_dot.csv\" \n[107] \"sub-054_stroop_test.csv\" \"sub-055_random_dot.csv\" \n[109] \"sub-055_stroop_test.csv\" \"sub-056_random_dot.csv\" \n[111] \"sub-057_random_dot.csv\"  \"sub-057_stroop_test.csv\"\n[113] \"sub-058_random_dot.csv\"  \"sub-058_stroop_test.csv\"\n[115] \"sub-059_random_dot.csv\"  \"sub-059_stroop_test.csv\"\n[117] \"sub-060_random_dot.csv\"  \"sub-060_stroop_test.csv\"\n[119] \"sub-061_random_dot.csv\"  \"sub-061_stroop_test.csv\"\n[121] \"sub-062_random_dot.csv\"  \"sub-062_stroop_test.csv\"\n[123] \"sub-063_random_dot.csv\"  \"sub-063_stroop_test.csv\"\n[125] \"sub-064_random_dot.csv\"  \"sub-064_stroop_test.csv\"\n[127] \"sub-065_random_dot.csv\"  \"sub-065_stroop_test.csv\"\n[129] \"sub-066_random_dot.csv\"  \"sub-066_stroop_test.csv\"\n[131] \"sub-067_random_dot.csv\"  \"sub-067_stroop_test.csv\"\n[133] \"sub-068_random_dot.csv\"  \"sub-068_stroop_test.csv\"\n[135] \"sub-069_random_dot.csv\"  \"sub-069_stroop_test.csv\"\n[137] \"sub-070_random_dot.csv\"  \"sub-070_stroop_test.csv\"\n[139] \"sub-071_random_dot.csv\"  \"sub-071_stroop_test.csv\"\n[141] \"sub-072_random_dot.csv\"  \"sub-072_stroop_test.csv\"\n[143] \"sub-073_random_dot.csv\"  \"sub-073_stroop_test.csv\"\n[145] \"sub-074_random_dot.csv\"  \"sub-074_stroop_test.csv\"\n[147] \"sub-075_random_dot.csv\"  \"sub-075_stroop_test.csv\"\n[149] \"sub-076_random_dot.csv\"  \"sub-076_stroop_test.csv\"\n[151] \"sub-077_random_dot.csv\"  \"sub-077_stroop_test.csv\"\n[153] \"sub-078_random_dot.csv\"  \"sub-078_stroop_test.csv\"\n[155] \"sub-079_random_dot.csv\"  \"sub-079_stroop_test.csv\"\n[157] \"sub-080_random_dot.csv\"  \"sub-080_stroop_test.csv\"\n[159] \"sub-081_random_dot.csv\"  \"sub-082_random_dot.csv\" \n[161] \"sub-082_stroop_test.csv\" \"sub-083_random_dot.csv\" \n[163] \"sub-083_stroop_test.csv\" \"sub-084_random_dot.csv\" \n[165] \"sub-084_stroop_test.csv\" \"sub-085_random_dot.csv\" \n[167] \"sub-085_stroop_test.csv\" \"sub-086_random_dot.csv\" \n[169] \"sub-086_stroop_test.csv\" \"sub-087_random_dot.csv\" \n[171] \"sub-087_stroop_test.csv\" \"sub-088_random_dot.csv\" \n[173] \"sub-088_stroop_test.csv\" \"sub-089_random_dot.csv\" \n[175] \"sub-089_stroop_test.csv\" \"sub-090_random_dot.csv\" \n[177] \"sub-090_stroop_test.csv\" \"sub-091_random_dot.csv\" \n[179] \"sub-091_stroop_test.csv\" \"sub-092_random_dot.csv\" \n[181] \"sub-092_stroop_test.csv\" \"sub-093_random_dot.csv\" \n[183] \"sub-093_stroop_test.csv\" \"sub-094_stroop_test.csv\"\n[185] \"sub-095_random_dot.csv\"  \"sub-095_stroop_test.csv\"\n[187] \"sub-096_random_dot.csv\"  \"sub-096_stroop_test.csv\"\n[189] \"sub-097_random_dot.csv\"  \"sub-097_stroop_test.csv\"\n[191] \"sub-098_random_dot.csv\"  \"sub-098_stroop_test.csv\"\n[193] \"sub-099_random_dot.csv\"  \"sub-099_stroop_test.csv\"\n[195] \"sub-100_random_dot.csv\"  \"sub-100_stroop_test.csv\"\n[197] \"sub-101_random_dot.csv\"  \"sub-101_stroop_test.csv\"\n[199] \"sub-102_random_dot.csv\"  \"sub-102_stroop_test.csv\"\n[201] \"sub-103_random_dot.csv\"  \"sub-103_stroop_test.csv\"\n[203] \"sub-104_random_dot.csv\"  \"sub-104_stroop_test.csv\"\n[205] \"sub-105_random_dot.csv\"  \"sub-105_stroop_test.csv\"\n[207] \"sub-106_random_dot.csv\"  \"sub-106_stroop_test.csv\"\n[209] \"sub-107_random_dot.csv\"  \"sub-107_stroop_test.csv\"\n[211] \"sub-108_random_dot.csv\"  \"sub-108_stroop_test.csv\"\n[213] \"sub-109_random_dot.csv\"  \"sub-109_stroop_test.csv\"\n[215] \"sub-110_random_dot.csv\"  \"sub-110_stroop_test.csv\"\n[217] \"sub-111_random_dot.csv\"  \"sub-111_stroop_test.csv\"\n[219] \"sub-112_random_dot.csv\"  \"sub-112_stroop_test.csv\"\n[221] \"sub-113_random_dot.csv\"  \"sub-113_stroop_test.csv\"\n[223] \"sub-114_random_dot.csv\"  \"sub-114_stroop_test.csv\"\n[225] \"sub-115_random_dot.csv\"  \"sub-115_stroop_test.csv\"\n[227] \"sub-116_random_dot.csv\"  \"sub-116_stroop_test.csv\"\n[229] \"sub-117_random_dot.csv\"  \"sub-117_stroop_test.csv\"\n[231] \"sub-118_random_dot.csv\"  \"sub-118_stroop_test.csv\"\n[233] \"sub-119_random_dot.csv\"  \"sub-119_stroop_test.csv\"\n[235] \"sub-120_random_dot.csv\"  \"sub-120_stroop_test.csv\"\n[237] \"sub-121_random_dot.csv\"  \"sub-121_stroop_test.csv\"\n[239] \"sub-122_random_dot.csv\"  \"sub-122_stroop_test.csv\"\n[241] \"sub-123_random_dot.csv\"  \"sub-123_stroop_test.csv\"\n[243] \"sub-124_random_dot.csv\"  \"sub-124_stroop_test.csv\"\n[245] \"sub-125_random_dot.csv\"  \"sub-125_stroop_test.csv\"\n[247] \"sub-126_random_dot.csv\"  \"sub-126_stroop_test.csv\"\n[249] \"sub-127_random_dot.csv\"  \"sub-127_stroop_test.csv\"\n[251] \"sub-128_random_dot.csv\"  \"sub-128_stroop_test.csv\"\n[253] \"sub-129_random_dot.csv\"  \"sub-129_stroop_test.csv\"\n[255] \"sub-130_random_dot.csv\"  \"sub-130_stroop_test.csv\"\n[257] \"sub-131_random_dot.csv\"  \"sub-131_stroop_test.csv\"\n[259] \"sub-132_random_dot.csv\"  \"sub-132_stroop_test.csv\"\n[261] \"sub-133_random_dot.csv\"  \"sub-133_stroop_test.csv\"\n[263] \"sub-134_random_dot.csv\"  \"sub-134_stroop_test.csv\"\n[265] \"sub-135_random_dot.csv\"  \"sub-135_stroop_test.csv\"\n[267] \"sub-136_random_dot.csv\"  \"sub-136_stroop_test.csv\"\n[269] \"sub-137_random_dot.csv\"  \"sub-137_stroop_test.csv\"\n[271] \"sub-138_random_dot.csv\"  \"sub-138_stroop_test.csv\"\n[273] \"sub-139_random_dot.csv\"  \"sub-139_stroop_test.csv\"\n[275] \"sub-140_random_dot.csv\"  \"sub-140_stroop_test.csv\"\n[277] \"sub-141_random_dot.csv\"  \"sub-141_stroop_test.csv\"\n[279] \"sub-142_random_dot.csv\"  \"sub-142_stroop_test.csv\"\n[281] \"sub-143_random_dot.csv\"  \"sub-143_stroop_test.csv\"\n[283] \"sub-144_random_dot.csv\"  \"sub-144_stroop_test.csv\"\n[285] \"sub-145_random_dot.csv\"  \"sub-145_stroop_test.csv\"\n[287] \"sub-146_random_dot.csv\"  \"sub-146_stroop_test.csv\"\n[289] \"sub-147_random_dot.csv\"  \"sub-147_stroop_test.csv\"\n[291] \"sub-148_random_dot.csv\"  \"sub-148_stroop_test.csv\"\n[293] \"sub-149_random_dot.csv\"  \"sub-149_stroop_test.csv\"\n[295] \"sub-150_random_dot.csv\"  \"sub-150_stroop_test.csv\"\n[297] \"sub-151_random_dot.csv\"  \"sub-151_stroop_test.csv\"\n[299] \"sub-152_random_dot.csv\"  \"sub-152_stroop_test.csv\"\n[301] \"sub-153_random_dot.csv\"  \"sub-153_stroop_test.csv\"\n[303] \"sub-154_random_dot.csv\"  \"sub-154_stroop_test.csv\"\n[305] \"sub-155_random_dot.csv\"  \"sub-155_stroop_test.csv\"\n[307] \"sub-156_random_dot.csv\"  \"sub-156_stroop_test.csv\"\n[309] \"sub-157_random_dot.csv\"  \"sub-157_stroop_test.csv\"\n[311] \"sub-158_random_dot.csv\"  \"sub-158_stroop_test.csv\"\n[313] \"sub-159_random_dot.csv\"  \"sub-159_stroop_test.csv\"\n[315] \"sub-160_random_dot.csv\"  \"sub-160_stroop_test.csv\"\n[317] \"sub-161_random_dot.csv\"  \"sub-161_stroop_test.csv\"\n[319] \"sub-162_random_dot.csv\"  \"sub-162_stroop_test.csv\"\n[321] \"sub-163_random_dot.csv\"  \"sub-163_stroop_test.csv\"\n[323] \"sub-164_random_dot.csv\"  \"sub-164_stroop_test.csv\"\n[325] \"sub-165_random_dot.csv\"  \"sub-165_stroop_test.csv\"\n[327] \"sub-166_random_dot.csv\"  \"sub-167_random_dot.csv\" \n[329] \"sub-167_stroop_test.csv\" \"sub-168_random_dot.csv\" \n[331] \"sub-168_stroop_test.csv\" \"sub-169_random_dot.csv\" \n[333] \"sub-169_stroop_test.csv\" \"sub-170_random_dot.csv\" \n[335] \"sub-170_stroop_test.csv\" \"sub-171_random_dot.csv\" \n[337] \"sub-171_stroop_test.csv\" \"sub-172_random_dot.csv\" \n[339] \"sub-172_stroop_test.csv\" \"sub-173_random_dot.csv\" \n[341] \"sub-173_stroop_test.csv\" \"sub-174_random_dot.csv\" \n[343] \"sub-174_stroop_test.csv\" \"sub-175_random_dot.csv\" \n[345] \"sub-175_stroop_test.csv\" \"sub-176_random_dot.csv\" \n[347] \"sub-176_stroop_test.csv\" \"sub-177_random_dot.csv\" \n[349] \"sub-177_stroop_test.csv\" \"sub-178_random_dot.csv\" \n[351] \"sub-178_stroop_test.csv\" \"sub-179_random_dot.csv\" \n[353] \"sub-179_stroop_test.csv\" \"sub-180_random_dot.csv\" \n[355] \"sub-180_stroop_test.csv\" \"sub-181_random_dot.csv\" \n[357] \"sub-181_stroop_test.csv\" \"sub-182_random_dot.csv\" \n[359] \"sub-182_stroop_test.csv\" \"sub-183_random_dot.csv\" \n[361] \"sub-183_stroop_test.csv\" \"sub-184_random_dot.csv\" \n[363] \"sub-184_stroop_test.csv\" \"sub-185_random_dot.csv\" \n[365] \"sub-185_stroop_test.csv\" \"sub-186_random_dot.csv\" \n[367] \"sub-186_stroop_test.csv\" \"sub-187_random_dot.csv\" \n[369] \"sub-187_stroop_test.csv\" \"sub-188_random_dot.csv\" \n[371] \"sub-188_stroop_test.csv\" \"sub-189_random_dot.csv\" \n[373] \"sub-189_stroop_test.csv\" \"sub-190_random_dot.csv\" \n[375] \"sub-190_stroop_test.csv\" \"sub-191_random_dot.csv\" \n[377] \"sub-191_stroop_test.csv\" \"sub-192_random_dot.csv\" \n[379] \"sub-192_stroop_test.csv\" \"sub-193_random_dot.csv\" \n[381] \"sub-193_stroop_test.csv\" \"sub-194_random_dot.csv\" \n[383] \"sub-194_stroop_test.csv\" \"sub-195_random_dot.csv\" \n[385] \"sub-195_stroop_test.csv\" \"sub-196_random_dot.csv\" \n[387] \"sub-196_stroop_test.csv\" \"sub-197_random_dot.csv\" \n[389] \"sub-197_stroop_test.csv\" \"sub-198_random_dot.csv\" \n[391] \"sub-198_stroop_test.csv\" \"sub-199_random_dot.csv\" \n[393] \"sub-199_stroop_test.csv\" \"sub-200_random_dot.csv\" \n[395] \"sub-200_stroop_test.csv\" \"sub-201_random_dot.csv\" \n[397] \"sub-201_stroop_test.csv\" \"sub-202_random_dot.csv\" \n[399] \"sub-202_stroop_test.csv\" \"sub-203_random_dot.csv\" \n[401] \"sub-203_stroop_test.csv\" \"sub-204_random_dot.csv\" \n[403] \"sub-204_stroop_test.csv\" \"sub-205_random_dot.csv\" \n[405] \"sub-205_stroop_test.csv\" \"sub-206_random_dot.csv\" \n[407] \"sub-206_stroop_test.csv\" \"sub-207_random_dot.csv\" \n[409] \"sub-207_stroop_test.csv\" \"sub-208_random_dot.csv\" \n[411] \"sub-208_stroop_test.csv\" \"sub-209_random_dot.csv\" \n[413] \"sub-209_stroop_test.csv\" \"sub-210_random_dot.csv\" \n[415] \"sub-210_stroop_test.csv\" \"sub-211_random_dot.csv\" \n[417] \"sub-211_stroop_test.csv\" \"sub-212_random_dot.csv\" \n[419] \"sub-212_stroop_test.csv\" \"sub-213_random_dot.csv\" \n[421] \"sub-213_stroop_test.csv\" \"sub-214_random_dot.csv\" \n[423] \"sub-214_stroop_test.csv\" \"sub-215_random_dot.csv\" \n[425] \"sub-215_stroop_test.csv\" \"sub-216_random_dot.csv\" \n[427] \"sub-216_stroop_test.csv\" \"sub-217_random_dot.csv\" \n[429] \"sub-217_stroop_test.csv\" \"sub-218_random_dot.csv\" \n[431] \"sub-218_stroop_test.csv\" \"sub-219_random_dot.csv\" \n[433] \"sub-219_stroop_test.csv\" \"sub-220_random_dot.csv\" \n[435] \"sub-220_stroop_test.csv\" \"sub-221_random_dot.csv\" \n[437] \"sub-221_stroop_test.csv\" \"sub-222_random_dot.csv\" \n[439] \"sub-222_stroop_test.csv\" \"sub-223_random_dot.csv\" \n[441] \"sub-223_stroop_test.csv\" \"sub-224_random_dot.csv\" \n[443] \"sub-224_stroop_test.csv\" \"sub-225_random_dot.csv\" \n[445] \"sub-225_stroop_test.csv\" \"sub-226_random_dot.csv\" \n[447] \"sub-226_stroop_test.csv\" \"sub-227_random_dot.csv\" \n[449] \"sub-227_stroop_test.csv\" \"sub-228_random_dot.csv\" \n[451] \"sub-228_stroop_test.csv\" \"sub-229_random_dot.csv\" \n[453] \"sub-229_stroop_test.csv\" \"sub-230_random_dot.csv\" \n[455] \"sub-230_stroop_test.csv\" \"sub-231_random_dot.csv\" \n[457] \"sub-231_stroop_test.csv\" \"sub-232_random_dot.csv\" \n[459] \"sub-232_stroop_test.csv\" \"sub-233_random_dot.csv\" \n[461] \"sub-233_stroop_test.csv\" \"sub-234_random_dot.csv\" \n[463] \"sub-234_stroop_test.csv\" \"sub-235_random_dot.csv\" \n[465] \"sub-235_stroop_test.csv\" \"sub-236_random_dot.csv\" \n[467] \"sub-236_stroop_test.csv\" \"sub-237_random_dot.csv\" \n[469] \"sub-237_stroop_test.csv\" \"sub-238_random_dot.csv\" \n[471] \"sub-238_stroop_test.csv\" \"sub-239_random_dot.csv\" \n[473] \"sub-240_random_dot.csv\" \n\nlist.files(path = 'data/raw', pattern = 'stroop')\n\n  [1] \"sub-001_stroop_test.csv\" \"sub-002_stroop_test.csv\"\n  [3] \"sub-003_stroop_test.csv\" \"sub-004_stroop_test.csv\"\n  [5] \"sub-005_stroop_test.csv\" \"sub-006_stroop_test.csv\"\n  [7] \"sub-007_stroop_test.csv\" \"sub-008_stroop_test.csv\"\n  [9] \"sub-009_stroop_test.csv\" \"sub-010_stroop_test.csv\"\n [11] \"sub-011_stroop_test.csv\" \"sub-012_stroop_test.csv\"\n [13] \"sub-013_stroop_test.csv\" \"sub-014_stroop_test.csv\"\n [15] \"sub-015_stroop_test.csv\" \"sub-016_stroop_test.csv\"\n [17] \"sub-017_stroop_test.csv\" \"sub-018_stroop_test.csv\"\n [19] \"sub-019_stroop_test.csv\" \"sub-020_stroop_test.csv\"\n [21] \"sub-021_stroop_test.csv\" \"sub-022_stroop_test.csv\"\n [23] \"sub-023_stroop_test.csv\" \"sub-024_stroop_test.csv\"\n [25] \"sub-025_stroop_test.csv\" \"sub-026_stroop_test.csv\"\n [27] \"sub-027_stroop_test.csv\" \"sub-028_stroop_test.csv\"\n [29] \"sub-029_stroop_test.csv\" \"sub-030_stroop_test.csv\"\n [31] \"sub-031_stroop_test.csv\" \"sub-032_stroop_test.csv\"\n [33] \"sub-033_stroop_test.csv\" \"sub-034_stroop_test.csv\"\n [35] \"sub-035_stroop_test.csv\" \"sub-036_stroop_test.csv\"\n [37] \"sub-037_stroop_test.csv\" \"sub-038_stroop_test.csv\"\n [39] \"sub-040_stroop_test.csv\" \"sub-041_stroop_test.csv\"\n [41] \"sub-042_stroop_test.csv\" \"sub-043_stroop_test.csv\"\n [43] \"sub-044_stroop_test.csv\" \"sub-045_stroop_test.csv\"\n [45] \"sub-046_stroop_test.csv\" \"sub-047_stroop_test.csv\"\n [47] \"sub-048_stroop_test.csv\" \"sub-049_stroop_test.csv\"\n [49] \"sub-050_stroop_test.csv\" \"sub-051_stroop_test.csv\"\n [51] \"sub-052_stroop_test.csv\" \"sub-053_stroop_test.csv\"\n [53] \"sub-054_stroop_test.csv\" \"sub-055_stroop_test.csv\"\n [55] \"sub-057_stroop_test.csv\" \"sub-058_stroop_test.csv\"\n [57] \"sub-059_stroop_test.csv\" \"sub-060_stroop_test.csv\"\n [59] \"sub-061_stroop_test.csv\" \"sub-062_stroop_test.csv\"\n [61] \"sub-063_stroop_test.csv\" \"sub-064_stroop_test.csv\"\n [63] \"sub-065_stroop_test.csv\" \"sub-066_stroop_test.csv\"\n [65] \"sub-067_stroop_test.csv\" \"sub-068_stroop_test.csv\"\n [67] \"sub-069_stroop_test.csv\" \"sub-070_stroop_test.csv\"\n [69] \"sub-071_stroop_test.csv\" \"sub-072_stroop_test.csv\"\n [71] \"sub-073_stroop_test.csv\" \"sub-074_stroop_test.csv\"\n [73] \"sub-075_stroop_test.csv\" \"sub-076_stroop_test.csv\"\n [75] \"sub-077_stroop_test.csv\" \"sub-078_stroop_test.csv\"\n [77] \"sub-079_stroop_test.csv\" \"sub-080_stroop_test.csv\"\n [79] \"sub-082_stroop_test.csv\" \"sub-083_stroop_test.csv\"\n [81] \"sub-084_stroop_test.csv\" \"sub-085_stroop_test.csv\"\n [83] \"sub-086_stroop_test.csv\" \"sub-087_stroop_test.csv\"\n [85] \"sub-088_stroop_test.csv\" \"sub-089_stroop_test.csv\"\n [87] \"sub-090_stroop_test.csv\" \"sub-091_stroop_test.csv\"\n [89] \"sub-092_stroop_test.csv\" \"sub-093_stroop_test.csv\"\n [91] \"sub-094_stroop_test.csv\" \"sub-095_stroop_test.csv\"\n [93] \"sub-096_stroop_test.csv\" \"sub-097_stroop_test.csv\"\n [95] \"sub-098_stroop_test.csv\" \"sub-099_stroop_test.csv\"\n [97] \"sub-100_stroop_test.csv\" \"sub-101_stroop_test.csv\"\n [99] \"sub-102_stroop_test.csv\" \"sub-103_stroop_test.csv\"\n[101] \"sub-104_stroop_test.csv\" \"sub-105_stroop_test.csv\"\n[103] \"sub-106_stroop_test.csv\" \"sub-107_stroop_test.csv\"\n[105] \"sub-108_stroop_test.csv\" \"sub-109_stroop_test.csv\"\n[107] \"sub-110_stroop_test.csv\" \"sub-111_stroop_test.csv\"\n[109] \"sub-112_stroop_test.csv\" \"sub-113_stroop_test.csv\"\n[111] \"sub-114_stroop_test.csv\" \"sub-115_stroop_test.csv\"\n[113] \"sub-116_stroop_test.csv\" \"sub-117_stroop_test.csv\"\n[115] \"sub-118_stroop_test.csv\" \"sub-119_stroop_test.csv\"\n[117] \"sub-120_stroop_test.csv\" \"sub-121_stroop_test.csv\"\n[119] \"sub-122_stroop_test.csv\" \"sub-123_stroop_test.csv\"\n[121] \"sub-124_stroop_test.csv\" \"sub-125_stroop_test.csv\"\n[123] \"sub-126_stroop_test.csv\" \"sub-127_stroop_test.csv\"\n[125] \"sub-128_stroop_test.csv\" \"sub-129_stroop_test.csv\"\n[127] \"sub-130_stroop_test.csv\" \"sub-131_stroop_test.csv\"\n[129] \"sub-132_stroop_test.csv\" \"sub-133_stroop_test.csv\"\n[131] \"sub-134_stroop_test.csv\" \"sub-135_stroop_test.csv\"\n[133] \"sub-136_stroop_test.csv\" \"sub-137_stroop_test.csv\"\n[135] \"sub-138_stroop_test.csv\" \"sub-139_stroop_test.csv\"\n[137] \"sub-140_stroop_test.csv\" \"sub-141_stroop_test.csv\"\n[139] \"sub-142_stroop_test.csv\" \"sub-143_stroop_test.csv\"\n[141] \"sub-144_stroop_test.csv\" \"sub-145_stroop_test.csv\"\n[143] \"sub-146_stroop_test.csv\" \"sub-147_stroop_test.csv\"\n[145] \"sub-148_stroop_test.csv\" \"sub-149_stroop_test.csv\"\n[147] \"sub-150_stroop_test.csv\" \"sub-151_stroop_test.csv\"\n[149] \"sub-152_stroop_test.csv\" \"sub-153_stroop_test.csv\"\n[151] \"sub-154_stroop_test.csv\" \"sub-155_stroop_test.csv\"\n[153] \"sub-156_stroop_test.csv\" \"sub-157_stroop_test.csv\"\n[155] \"sub-158_stroop_test.csv\" \"sub-159_stroop_test.csv\"\n[157] \"sub-160_stroop_test.csv\" \"sub-161_stroop_test.csv\"\n[159] \"sub-162_stroop_test.csv\" \"sub-163_stroop_test.csv\"\n[161] \"sub-164_stroop_test.csv\" \"sub-165_stroop_test.csv\"\n[163] \"sub-167_stroop_test.csv\" \"sub-168_stroop_test.csv\"\n[165] \"sub-169_stroop_test.csv\" \"sub-170_stroop_test.csv\"\n[167] \"sub-171_stroop_test.csv\" \"sub-172_stroop_test.csv\"\n[169] \"sub-173_stroop_test.csv\" \"sub-174_stroop_test.csv\"\n[171] \"sub-175_stroop_test.csv\" \"sub-176_stroop_test.csv\"\n[173] \"sub-177_stroop_test.csv\" \"sub-178_stroop_test.csv\"\n[175] \"sub-179_stroop_test.csv\" \"sub-180_stroop_test.csv\"\n[177] \"sub-181_stroop_test.csv\" \"sub-182_stroop_test.csv\"\n[179] \"sub-183_stroop_test.csv\" \"sub-184_stroop_test.csv\"\n[181] \"sub-185_stroop_test.csv\" \"sub-186_stroop_test.csv\"\n[183] \"sub-187_stroop_test.csv\" \"sub-188_stroop_test.csv\"\n[185] \"sub-189_stroop_test.csv\" \"sub-190_stroop_test.csv\"\n[187] \"sub-191_stroop_test.csv\" \"sub-192_stroop_test.csv\"\n[189] \"sub-193_stroop_test.csv\" \"sub-194_stroop_test.csv\"\n[191] \"sub-195_stroop_test.csv\" \"sub-196_stroop_test.csv\"\n[193] \"sub-197_stroop_test.csv\" \"sub-198_stroop_test.csv\"\n[195] \"sub-199_stroop_test.csv\" \"sub-200_stroop_test.csv\"\n[197] \"sub-201_stroop_test.csv\" \"sub-202_stroop_test.csv\"\n[199] \"sub-203_stroop_test.csv\" \"sub-204_stroop_test.csv\"\n[201] \"sub-205_stroop_test.csv\" \"sub-206_stroop_test.csv\"\n[203] \"sub-207_stroop_test.csv\" \"sub-208_stroop_test.csv\"\n[205] \"sub-209_stroop_test.csv\" \"sub-210_stroop_test.csv\"\n[207] \"sub-211_stroop_test.csv\" \"sub-212_stroop_test.csv\"\n[209] \"sub-213_stroop_test.csv\" \"sub-214_stroop_test.csv\"\n[211] \"sub-215_stroop_test.csv\" \"sub-216_stroop_test.csv\"\n[213] \"sub-217_stroop_test.csv\" \"sub-218_stroop_test.csv\"\n[215] \"sub-219_stroop_test.csv\" \"sub-220_stroop_test.csv\"\n[217] \"sub-221_stroop_test.csv\" \"sub-222_stroop_test.csv\"\n[219] \"sub-223_stroop_test.csv\" \"sub-224_stroop_test.csv\"\n[221] \"sub-225_stroop_test.csv\" \"sub-226_stroop_test.csv\"\n[223] \"sub-227_stroop_test.csv\" \"sub-228_stroop_test.csv\"\n[225] \"sub-229_stroop_test.csv\" \"sub-230_stroop_test.csv\"\n[227] \"sub-231_stroop_test.csv\" \"sub-232_stroop_test.csv\"\n[229] \"sub-233_stroop_test.csv\" \"sub-234_stroop_test.csv\"\n[231] \"sub-235_stroop_test.csv\" \"sub-236_stroop_test.csv\"\n[233] \"sub-237_stroop_test.csv\" \"sub-238_stroop_test.csv\"\n\n\nUm die Files einzulesen, reichen nur die Namen der Dateien nicht aus. Dazu benötigen wir die kompletten Pfade.\n\nfiles &lt;- list.files(path = 'data/raw/', pattern = 'stroop') %&gt;% \n    paste('data/raw/', ., sep = '')\n\n\nHier wird die Pipe des magritter-Packages verwendet (%&gt;%) statt die Base-R Pipe (|&gt;). Mit %&gt;% haben wir die Möglichkeit mit dem . zu bestimmen wo die weitergeleiteten Inhalte der Pipe eingefügt werden (nach data/). Informationen zu den Unterschieden der Pipes finden Sie hier.\n\n\n10.4.1 Alle Files von Hand einlesen\nJedes Daten File wird einzeln eingelesen. Anschliessend müssen alle Files zusammengefügt werden. Diese Lösung ist einfach zu verstehen, ist bei vielen Dokumenten aber zu aufwändig.\n\nfile1 &lt;- files[1]\nfile2 &lt;- files[2]\nfile3 &lt;- files[3]\n\nd1 &lt;- read_stroop(file1)\n\nRows: 161 Columns: 110\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (21): wordCTK, colorCTK, corrAnsCTK, keyResp_CTK.keys, wordPractice, col...\ndbl (77): congruentCTK, thisN, thisTrialN, thisRepN, keyResp_CTK.corr, keyRe...\nlgl (12): keyResp_CTK.duration, respPractice.duration, keyResp_test_run.dura...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nd2 &lt;- read_stroop(file2)\n\nRows: 161 Columns: 111\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (21): wordCTK, colorCTK, corrAnsCTK, keyResp_CTK.keys, wordPractice, col...\ndbl (78): congruentCTK, thisN, thisTrialN, thisRepN, keyResp_CTK.corr, keyRe...\nlgl (12): keyResp_CTK.duration, respPractice.duration, keyResp_test_run.dura...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nd3 &lt;- read_stroop(file3)\n\nRows: 161 Columns: 111\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (21): wordCTK, colorCTK, corrAnsCTK, keyResp_CTK.keys, wordPractice, col...\ndbl (78): congruentCTK, thisN, thisTrialN, thisRepN, keyResp_CTK.corr, keyRe...\nlgl (12): keyResp_CTK.duration, respPractice.duration, keyResp_test_run.dura...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nd_hand &lt;- bind_rows(d1, d2, d3)\n\n\n\n10.4.2 Alle Files mit for-Loop einlesen\nDas Einlesen kann mit einem for-Loop automatisiert werden. Der Loop iteriert über alle Daten Files. Als erstes muss ein leerer Data Frame d_loop erstellt werden. Bei jeder Iteration des Loops wird ein Daten File eingelesen und dem erstellten Data Frame d_loop angehängt.\n\nd_loop &lt;- tibble()\n\nfor (file in files){\n    d_tmp &lt;- read_stroop(file)\n    d_loop &lt;- bind_rows(d_loop, d_tmp)\n}\n\n\n\n10.4.3 Alle Files mit der Funktion map() einlesen\nmap() wendet eine Funktion auf alle Elemente eines Vektors an. Der Vektor files enthält die Pfade zu den Daten Files. Mit map() können wir also unsere selbst erstellte Funktion read_stroop() auf jeden Pfad anwenden. Im Anschluss müssen die Dataframes noch verbunden werden.\n\nd_map1 &lt;- files |&gt;\n    map(read_stroop) %&gt;%\n    bind_rows()\n\nDie Funktion map_dfr() macht das gleiche wie map() fügt aber zusätzlich die einzelnen Data Frames automatisch zusammen.\n\nd_map2 &lt;- files |&gt;\n    map_dfr(read_stroop)\n\n\n\n\n\n\n\n\nHands-on 2: map Funktion anweden\n\n\n\nBenutzen Sie die Funktion map() um unsere Funktion fast_correct() gleichzeitig auf d1, d2 und d3 anzuwenden.\nTIPP: map() braucht als Argument eine Liste!",
    "crumbs": [
      "Data wrangling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Automatisiertes Preprocessing</span>"
    ]
  },
  {
    "objectID": "datawrangling_automatisiert.html#kompletter-stroop-code-an-einem-ort",
    "href": "datawrangling_automatisiert.html#kompletter-stroop-code-an-einem-ort",
    "title": "10  Automatisiertes Preprocessing",
    "section": "10.5 Kompletter Stroop Code an einem Ort",
    "text": "10.5 Kompletter Stroop Code an einem Ort\n\nread_stroop &lt;- function(path){\n    d_stroop &lt;- read_csv(path) |&gt;\n    filter(!is.na(trials_test.thisN)) |&gt;\n    mutate(trial = trials_test.thisN + 1,\n           condition = case_when(congruent == 1 ~ \"congruent\",\n                                 congruent == 0 ~ \"incongruent\")) |&gt;\n    select(id = participant, \n           trial,\n           word, \n           color, \n           congruent, \n           condition,\n           resp = keyResp_test_run.keys, \n           corr = keyResp_test_run.corr, \n           rt = keyResp_test_run.rt)\n    d_stroop\n}\n\nd &lt;- list.files(path = 'data/raw/', pattern = 'stroop') %&gt;% \n    paste('data/raw/', ., sep = '') |&gt;\n    map_dfr(read_stroop)\n\nd |&gt; write.csv(file = \"data/clean/dataset_stroop_clean.csv\", row.names = FALSE) # neuer Datensatz in anderen Ordner speichern um Verdoppelung zu vermeiden\n\n\nAchten Sie darauf, den neu erstellten Datensatz nicht in den raw-Ordner zu speichern. Sonst wird er (weil er stroop im Namen hat) beim nächsten Ausführen der Funktion read_stroop ebenfalls eingelesen, was einen Fehler verursacht.\n\n\n\n\n\n\n\nHands-on Lösungen\n\n\n\n\n\n\n10.5.1 Hands-on 1\n\nfast_correct &lt;- function(data){\n  d &lt;- data %&gt;% \n    filter(rt&lt;0.5)\n  p &lt;- mean(d$corr) * 100   \n  return(p)\n}\n\nfast_correct(d_stroop)\n\n[1] 100\n\n\n\n\n10.5.2 Hands-on 2\n\ndata_list &lt;- list(d1, d2, d3)\n\ndata_list |&gt; \n    map(fast_correct)\n\n[[1]]\n[1] 89.65517\n\n[[2]]\n[1] 100\n\n[[3]]\n[1] 100",
    "crumbs": [
      "Data wrangling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Automatisiertes Preprocessing</span>"
    ]
  },
  {
    "objectID": "datawrangling_automatisiert.html#footnotes",
    "href": "datawrangling_automatisiert.html#footnotes",
    "title": "10  Automatisiertes Preprocessing",
    "section": "",
    "text": "Für das Entzippen mit Windows machen Sie einen Rechtsklick auf den Ordner mit dem Reissverschluss und wählen Sie Entpacken nach und geben Sie den Ordner an, in dem Sie alle Ihre RProject-Ordner speichern. Für das Entzippen mit Mac speichern Sie den heruntergeladenen Ordner in den Ordner, in dem Sie alle Ihre RProject-Ordner speichern und Doppelklicken Sie danach auf den Ordner. Nur entzippte Ordner können einwandfrei verwendet werden!↩︎",
    "crumbs": [
      "Data wrangling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Automatisiertes Preprocessing</span>"
    ]
  },
  {
    "objectID": "uebung_2.html",
    "href": "uebung_2.html",
    "title": "Übung 2",
    "section": "",
    "text": "Auftrag\nArbeiten Sie die Datenvorverarbeitungsschritte im Kapitel Daten importieren und vorverarbeiten durch.",
    "crumbs": [
      "Data wrangling",
      "Übung 2"
    ]
  },
  {
    "objectID": "uebung_2.html#abgabetermin",
    "href": "uebung_2.html#abgabetermin",
    "title": "Übung 2",
    "section": "Abgabetermin",
    "text": "Abgabetermin\nBei dieser Übung muss nichts abgegeben werden.\nBeim Termin vom 14. März 2024 werden wir jedoch stark auf dieses Vorwissen aufbauen.",
    "crumbs": [
      "Data wrangling",
      "Übung 2"
    ]
  },
  {
    "objectID": "uebung_2.html#trouble-shooting",
    "href": "uebung_2.html#trouble-shooting",
    "title": "Übung 2",
    "section": "Trouble shooting",
    "text": "Trouble shooting\nSie dürfen gerne in den Pausen der vorherigen Termine auf uns zukommen, falls etwas nicht klappt oder Sie eine Frage haben.\nFalls Sie zusätzliche Übungsaufgaben wünschen, finden Sie hier weitere Aufgaben:\n\nWalk-through mit Lösungen\nÜbungsaufgaben\n\nSie können auch Kurse auf DataCamp besuchen, z.B.\n👉🏼 Introduction to R",
    "crumbs": [
      "Data wrangling",
      "Übung 2"
    ]
  },
  {
    "objectID": "data_visualization_1.html",
    "href": "data_visualization_1.html",
    "title": "11  Datenvisualisierung mit",
    "section": "",
    "text": "11.1 Daten\nDie wichtigste Komponente einer Grafik sind die Daten. Bevor eine Grafik erstellt wird, müssen die Eigenschaften des Datensatzes bekannt sein.\n# Einlesen des Datensatzes\nd &lt;- read.csv(\"data/DatasaurusDozen.csv\") %&gt;%\n    mutate(condition = as.factor(condition)) # Variable condition zu Faktor konvertieren\n\n# Datensatz anschauen\nglimpse(d)\n\nRows: 1,846\nColumns: 3\n$ condition &lt;fct&gt; away, away, away, away, away, away, away, away, away, away, …\n$ value1    &lt;dbl&gt; 32.33111, 53.42146, 63.92020, 70.28951, 34.11883, 67.67072, …\n$ value2    &lt;dbl&gt; 61.411101, 26.186880, 30.832194, 82.533649, 45.734551, 37.11…",
    "crumbs": [
      "Datenvisualisieren",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Datenvisualisierung mit</span>"
    ]
  },
  {
    "objectID": "data_visualization_1.html#daten",
    "href": "data_visualization_1.html#daten",
    "title": "11  Datenvisualisierung mit",
    "section": "",
    "text": "Der verwendete Datensatz stammt von Matejka and Fitzmaurice (2017).\n\n\n\n11.1.1 Datenformat\nAm einfachsten ist das Plotten mit ggplot(), wenn die Daten im long-Format vorliegen. Das bedeutet:\n\nJede Variable die gemessen/erhoben wird, hat eine Spalte (z.B. Versuchspersonennummer, Reaktionszeit, Taste).\nJede Messung hat eine Zeile. (In unserem PsychoPy-Experiment entspricht dies einer Zeile pro Trial.)\n\nDie hier eingelesenen Daten sind schon im long-Format.\n\nFalls die Daten im wide-Format abgespeichert sind, lohnt es sich diese umzuformatieren z.B. mit pivot_longer().\n\n\n\n11.1.2 Variablen\nFür die Grafik ist es relevant, welches Skalenniveau die zu visualisierenden Variablen haben. Je nach Anzahl Variablen und den entsprechenden Skalenniveaus eignen sich andere Grafik-Formate. Eine häufige Schwierigkeit beim Visualisieren der Daten ist, dass die Daten nicht das für den gewählten Plot passenden Skalenniveaus haben.\n\n\n\nCC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=724035\n\n\n\n\n\n\n\n\nHands-on: Datensatz anschauen\n\n\n\nSchauen Sie sich den Datensatz an.\n\nWie viele unterschiedliche Variablen gibt es?\nWie heissen die Variablen?\nWelches Skalenniveau haben sie?\n\n\n\n\n\n11.1.3 Subsetting\nWenn nur ein gewisser Teil der Daten visualisiert werden soll, muss der Datensatz gefiltert werden. Der aktuelle Datensatz enthält beispielsweise verschiedene Bedingungen, jeweils mit Werten für Variable value1 und value2. Folgende 13 Bedingungen sind enthalten:\n\nunique(d$condition)\n\n [1] away       bullseye   circle     dino       dots       h_lines   \n [7] high_lines slant_down slant_up   star       v_lines    wide_lines\n[13] x_shape   \n13 Levels: away bullseye circle dino dots h_lines high_lines ... x_shape\n\n\nFürs erste entscheiden wir uns für die Bedingung away.\n\nd_away &lt;- d %&gt;%\n    filter(condition == \"away\")\n\nWir können für diese Bedingung zusätzlich summary statistics berechnen, hier Mittelwert und Standardabweichung.\n\nd_away_summary &lt;- d_away %&gt;%\n    summarise(mean_value1 = mean(value1),\n              sd_value1 = sd(value1),\n              mean_value2 = mean(value2),\n              sd_value2 = sd(value2))\n\nglimpse(d_away_summary)\n\nRows: 1\nColumns: 4\n$ mean_value1 &lt;dbl&gt; 54.2661\n$ sd_value1   &lt;dbl&gt; 16.76982\n$ mean_value2 &lt;dbl&gt; 47.83472\n$ sd_value2   &lt;dbl&gt; 26.93974\n\n\nDiese Werte geben einen Anhaltspunkt, in welchem Bereich sich die Werte bewegen werden.\n\n\n11.1.4 Plot\nIn den folgenden Beispielen werden die Daten der Bedingung away verwendet. Als erstes Argument wird der Funktion ggplot() der Datensatz übergeben (data = data_away).\n\nggplot(data = d_away)",
    "crumbs": [
      "Datenvisualisieren",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Datenvisualisierung mit</span>"
    ]
  },
  {
    "objectID": "data_visualization_1.html#mapping",
    "href": "data_visualization_1.html#mapping",
    "title": "11  Datenvisualisierung mit",
    "section": "11.2 Mapping",
    "text": "11.2 Mapping\nDas mapping beschreibt, welche Variable auf der X- und Y-Achse abgetragen werden sollen. Es wird also definiert, wie die Variablen auf die Formen (aesthetics) gemappt werden sollen. Am einfachsten wird dies zu Beginn in festgelegt (das mapping kann aber auch in der Funktion geom_ selbst definiert werden). Weitere Variablen könnten als Argumente z.B. unter group = ... oder color = ... eingefügt werden.\n\nggplot(data = d_away,\n       mapping = aes(x = value1,\n                     y = value2)) \n\n\n\n\n\n\n\n\nDie Grafik verfügt nun über Achsen, diese werden automatisch mit den Variablennamen beschriftet. Da noch keine Formen (geoms) hinzugefügt wurde ist die Grafik in der Mitte aber leer.",
    "crumbs": [
      "Datenvisualisieren",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Datenvisualisierung mit</span>"
    ]
  },
  {
    "objectID": "data_visualization_1.html#geom-formen",
    "href": "data_visualization_1.html#geom-formen",
    "title": "11  Datenvisualisierung mit",
    "section": "11.3 Geom / Formen",
    "text": "11.3 Geom / Formen\nAls dritte Komponente wird in ggplot() die Form mit geom_ hinzugefügt. Jede Form, die eingefügt wird, benötigt Angaben zum mapping. Falls kein mapping angegeben wird, wird dieses aus der ggplot()-Funktion in der ersten Zeile übernommen.\nEs stehen viele verschiedene Formen zur Auswahl. Beispielsweise werden mit geom_point() Punkte erstellt, mit geom_line() Linien, mit geom_boxplot Boxplots, usw. Bei der Wahl der passenden Form kommt es einerseits auf die Daten an. Sind die Daten z.B. Faktoren oder numerische Werte (siehe auch Skalenniveau oben)? Wie viele Variablen werden gleichzeitig in die Grafik eingebunden? Andererseits ist es wichtig, was mit der Grafik gezeigt werden soll: Unterschiede? Gemeinsamkeiten? Veränderungen über Zeit?\nGeome zur Visualisierung von Datenpunkten und Verläufen:\n\nPunkte / Scatterplots - geom_point()\nLinien - geom_line()\n\nGeome zur Visualisierung von zusammenfassenden Werten:\n\nHistogramme - geom_histogram()\nMittelwerte und Standardabweichungen - geom_pointrange()\nDichteplots - geom_density()\nBoxplots - geom_boxplot()\nViolinplots - geom_violin()\n\n\nEs gibt auch weitere, sehr informative Arten der Visualisierung, wie heat maps oder shift functions, auf die wir in dieser Veranstaltung nicht eingehen.\n\n\n\n\n\n\n\nHands-on: Geoms\n\n\n\nWelche geoms eignen sich für welches Skalenniveau und welche Variablenanzahl?\nTipps:\n\nSchauen Sie sich den Datensatz mit glimpse(), head() oder summary() an.\nSchauen Sie sich die verschiedenen Formen von Plots hier an. \n\n👉 `{ggplot2}-Cheatsheet zum Herunterladen\n\n\n\n11.3.1 Kombinieren von mehreren geoms in einer Grafik\nTeilweise werden in Visualisierungen mehrere geoms kombiniert. In vielen Fällen macht es beispielsweise Sinn, nicht nur die Rohwerte oder Werte für jedes Subjekt, sondern in derselben Grafik auch zusammenfassende Masse, z.B. einen Boxplot, zu visualisieren.\n\nWeiterführende Info zum Kombinieren von Plots finden Sie hier.\n\nVerwenden verschiedener geoms in einem Plot:\n\nggplot(data = d_away, \n       mapping = aes(x = condition,\n                     y = value2)) +\n    geom_boxplot(width = 0.3) +\n    geom_jitter(width = 0.1) \n\n\n\n\n\n\n\n\nKombiniert werden können aber nicht nur verschiedene Formen, sondern auch mehrere Datensätze. Dies kann in ggplot() einfach umgesetzt werden indem mehrere Geoms übereinandergelegt werden und nicht das mapping aus der ggplot()-Funktion genutzt wird. Stattdessen wird für jedes geom ein separater Datensatz und ein separates mapping spezifiziert.\n\nggplot(data = d_away, \n       mapping = aes(x = condition,\n                     y = value2)) +\n    geom_jitter(width = 0.1) + # verwendet Datensatz \"d_away\"\n    geom_point(data = d_away_summary, # verwendet Datensatz \"d_away_summary\"\n               aes(x = \"away\", y = mean_value1), # condition ist nicht im Datensatz enthalten, deshalb hier hardcoded\n               color = \"red\", # Punkt rot einfärben\n               size = 3) # Punkt vergrössern",
    "crumbs": [
      "Datenvisualisieren",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Datenvisualisierung mit</span>"
    ]
  },
  {
    "objectID": "data_visualization_1.html#beschriftungen-und-themes",
    "href": "data_visualization_1.html#beschriftungen-und-themes",
    "title": "11  Datenvisualisierung mit",
    "section": "11.4 Beschriftungen und Themes",
    "text": "11.4 Beschriftungen und Themes\nSchönere und informativere Plots lassen sich gestalten, wenn wir einen Titel hinzufügen, die Achsenbeschriftung anpassen und das theme verändern:\n\nggplot(data = d_away,\n       mapping = aes(x = value1,\n                     y = value2)) +\n    geom_point() +\n    labs(title = \"Ein etwas schönerer Plot\", \n         subtitle = \"Verteilung der Rohwerte\",\n        x = \"Wert 1  [a.u.]\",\n        y = \"Wert 2 [a.u.]\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\nAuch theme_classic() oder theme_bw() ergeben schlichte aber schöne Plots.\n\n\n\n\n\n\n\nHands-on\n\n\n\nErstellen Sie eine Grafik.\n\nFügen Sie mit labs() passende Beschriftungen hinzu. Gibt es noch weitere, oben nicht verwendete Optionen?\nWechseln Sie das theme. Welches gefällt Ihnen am besten?\n\ntheme_bw()\ntheme_classic()\ntheme_dark()\n… (schreiben Sie theme_ und drücken Sie Tab, um weitere Vorschläge zu sehen.)",
    "crumbs": [
      "Datenvisualisieren",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Datenvisualisierung mit</span>"
    ]
  },
  {
    "objectID": "data_visualization_1.html#daten-plotten-tipps-und-tricks",
    "href": "data_visualization_1.html#daten-plotten-tipps-und-tricks",
    "title": "11  Datenvisualisierung mit",
    "section": "11.5 Daten plotten: Tipps und Tricks",
    "text": "11.5 Daten plotten: Tipps und Tricks\n\n\n\n\n\n\nHands-on: Informative Grafik erstellen\n\n\n\nIm Folgenden können Sie den Datensatz mit Grafiken erkunden.\nSie können entweder in Ihrem RScript / RNotebook weiterarbeiten oder Sie können ein GUI (graphical user interface) verwenden, dass für Sie den Code schreibt.\n\nWelche geom_s/Formen eignen sich gut für diesen Datensatz?\nWelche Abbildungen können alle 3 Variablen des Datensatzes berücksichtigen?\nWie kann man Bedingungen miteinander vergleichen?\nWie können Grösse und Farbe der geom_s bestimmt werden?\nWie passt man Schriftgrössen an?\nKönnen Sie eine Grafik speichern?\nLassen Sie sich den Code direkt ins RScript / RNotebook einfügen und verändern Sie den Code dort weiter.\n\n\n\n\n11.5.1 Daten plotten mit esquisser()\nUm in RStudio ein GUI für das Datenvisualisieren zu verwenden, kann das Package {esquisse} genutzt werden.\n\nInstallieren Sie das Package {esquisse} mit install.packages(\"esquisse\") in der Konsole oder über Tools &gt; Install packages...\nGeben Sie in Ihrer Konsole esquisse::esquisser() ein und wählen Sie dann unter Import Data den schon eingelesenen Datensatz DatasaurusDozen.csv aus.\n\n\nEin weiteres R-basiertes Visualisierungstool in welchem der Code per GUI erstellt wird, ist trelliscopejs\n\n\n\n11.5.2 Mehrere Plots in einer Grafik darstellen\nDies können Sie mit dem Package patchwork sehr einfach machen. Sie finden oben oder hier ein Beispiel.\n\nWenn Sie das Package {patchwork} zum ersten Mal nutzen, können Sie es in der Konsole mit install.packages(\"patchwork\") installieren.\n\n\n\n11.5.3 Grafik abspeichern\nEine Grafik lässt sich abspeichern unter dem Reiter Plots &gt; Export oder mit der Funktion ggsave().\n\n\n11.5.4 Inspiration\n\nGrafiken für verschiedene Datenarten: From Data to Viz\nSimple bis crazy Chartideen: R Charts: Ggplot\nFarben für Grafiken: R Charts: Colors, noch mehr Farben\n\n\n\n11.5.5 Weiterführende Ressourcen zur Datenvisualisierung mit ggplot()\n\nDokumentation von ggplot2\nKurzweilige, kompakte und sehr informative Informationen und Videos über das Erstellen von Grafiken in ggplot finden Sie hier: Website PsyTeachR: Data Skills for reproducible research\nHier ist der Start der PsyTeachR Videoliste von Lisa deBruine, dort finden sich auch hilfreiche Kurzvideos zu Themen von Daten einlesen bis zu statistischen Analysen. Beispielsweise zu Basic Plots, Common Plots und Plot Themes and Customization\nEinführung in R von Andrew Ellis und Boris Mayer\n\n\n\n\n\nMatejka, Justin, and George Fitzmaurice. 2017. “Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics Through Simulated Annealing.” In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, 1290–94. Denver Colorado USA: ACM. https://doi.org/10.1145/3025453.3025912.",
    "crumbs": [
      "Datenvisualisieren",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Datenvisualisierung mit</span>"
    ]
  },
  {
    "objectID": "data_visualization_1.html#footnotes",
    "href": "data_visualization_1.html#footnotes",
    "title": "11  Datenvisualisierung mit",
    "section": "",
    "text": "Für das Entzippen mit Windows machen Sie einen Rechtsklick auf den Ordner mit dem Reissverschluss und wählen Sie Entpacken nach und geben Sie den Ordner an, in dem Sie alle Ihre RProject-Ordner speichern. Für das Entzippen mit Mac speichern Sie den heruntergeladenen Ordner in den Ordner, in dem Sie alle Ihre RProject-Ordner speichern und Doppelklicken Sie danach auf den Ordner. Nur entzippte Ordner können einwandfrei verwendet werden!↩︎",
    "crumbs": [
      "Datenvisualisieren",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Datenvisualisierung mit</span>"
    ]
  },
  {
    "objectID": "data_visualization_2.html",
    "href": "data_visualization_2.html",
    "title": "12  Grundlagen der Datenvisualisierung",
    "section": "",
    "text": "12.1 Diagnostik: Daten untersuchen\nDatensätze können sehr komplex sein, deshalb ist die Visualisierung der Daten ein hilfreicher erster Schritt. Mit Hilfe von Visualisierungen können Aussagen über die Qualität der Daten gemacht werden, z.B. über:\nDiagnostische Grafiken dienen dazu, rasch an Informationen zu können und Probleme in Datensätzen zu entdecken. Die Grafiken müssen daher nicht ästhetisch ansprechend oder für Aussenstehende verständlich sein. Im Sinne der Reproduzierbarkeit lohnt es sich aber, auch diese Visualisierungen gut zu dokumentieren.\nIm Folgenden schauen wir uns Beispiele für diagnostische Grafiken an.",
    "crumbs": [
      "Datenvisualisieren",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Grundlagen der Datenvisualisierung</span>"
    ]
  },
  {
    "objectID": "data_visualization_2.html#diagnostik-daten-untersuchen",
    "href": "data_visualization_2.html#diagnostik-daten-untersuchen",
    "title": "12  Grundlagen der Datenvisualisierung",
    "section": "",
    "text": "Fehlende Werte\nAufgabenschwierigkeit\nExtreme Datenpunkte (Ausreisser)\nZeitverläufe\nVerteilung der Daten\n\n\n\n\n12.1.1 Fehlende Werte\nHierbei ist es wichtig, vor allem systematisch fehlende Datenpunkte zu entdecken: Fehlt bei einer Person die Hälfte der Antworten? Möchten wir diese ausschliessen?\nDiese können mit dem Package {naniar} relativ schnell sichtbar gemacht werden. Es gibt keine missings in unserem Fall.\n\nnaniar::vis_miss(d)\n\n\n\n\n\n\n\n\n\nBevor Sie das Package verwenden können, müssen Sie dies erst herunterladen. Sie können dies unter dem Reiter Tools &gt; Install Packages ... tun oder in der Konsole mit install.packages(\"naniar\").\n\nFür die Analyse schliessen wir alle Reaktionszeiten unter 100ms und über 8 Sekunden aus.\n\n# zu schnelle und zu langsame Antworten ausschliessen\nd &lt;- d |&gt;\n    filter(rt &gt; 0.099 & rt &lt; 8)\n\nWir berechnen nun für die kommenden Grafiken die Anzahl Trials pro Person, die accuracy, sowie die mittlere Reaktionszeit für die Bedingungen congruent und incongruent (wie im Kapitel Aggregierte Statistiken beschrieben).\n\n# Daten gruppieren:  Anzahl Trials, Accuracy und mittlere Reaktionszeit berechnen\nacc_rt_individual &lt;- d |&gt;\n    group_by(id, condition) |&gt;\n    summarise(\n        N = n(),\n        ncorrect = sum(corr),\n        accuracy = mean(corr),\n        median_rt = median(rt)\n    )\nacc_rt_individual\n\n# A tibble: 468 × 6\n# Groups:   id [234]\n   id      condition       N ncorrect accuracy median_rt\n   &lt;fct&gt;   &lt;fct&gt;       &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 sub-001 congruent      60       56    0.933     0.487\n 2 sub-001 incongruent    60       55    0.917     0.535\n 3 sub-002 congruent      60       60    1         0.544\n 4 sub-002 incongruent    60       54    0.9       0.619\n 5 sub-003 congruent      60       59    0.983     0.704\n 6 sub-003 incongruent    60       59    0.983     0.708\n 7 sub-004 congruent      60       60    1         0.606\n 8 sub-004 incongruent    60       49    0.817     0.778\n 9 sub-005 congruent      60       58    0.967     0.616\n10 sub-005 incongruent    60       35    0.583     0.863\n# ℹ 458 more rows\n\n\nNachdem wir Trials ohne Antwort ausgeschlossen haben, interessiert es uns, wie viele Trials jede Versuchsperson gelöst hat:\n\n# Plot: Anzahl Trials pro Bedingung für jede Versuchsperson \nacc_rt_individual |&gt; \n    ggplot(aes(x = id, y = N)) +\n    geom_point() +\n    facet_wrap(~ condition) +\n    geom_hline(yintercept = 40) + # Horizontale Linie einfügen\n    theme_minimal()\n\n\n\n\n\n\n\n\nWir könnten alle Personen ausschliessen, die weniger als 40 gültige Trials hatten (in unserem Beispiel nicht nötig, da alle genügend Trials haben).\n\n\n\n\n\n\nCode: Daten ausschliessen\n\n\n\n\n\n\n# Datensatz mit allen Ids, welche zuwenig Trials hatten\nn_exclusions &lt;- acc_rt_individual |&gt;\n    filter(N &lt; 40) \n\n# Aus dem Hauptdatensatz diese Ids ausschliessen\nd &lt;- d |&gt;\n    filter(!id %in% n_exclusions$id) \n\n# Check\nd_acc_rt_individual &lt;- d |&gt;\n    group_by(id, condition) |&gt;\n    summarise(\n        N = n(),\n        ncorrect = sum(corr),\n        accuracy = mean(corr),\n        median_rt = median(rt)\n    )\n\nd_acc_rt_individual |&gt; \n    ggplot(aes(x = id, y = N)) +\n    geom_point() +\n    facet_wrap(~ condition) +\n    geom_hline(yintercept = 40) + # Horizontale Linie einfügen\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n12.1.2 Extreme Datenpunkte (Ausreisser)\nWir können Visualisierungen auch verwenden, um extreme Datenpunkte zu identifizieren. Dafür teilen wir hier die Accuracywerte in 3 Gruppen ein und plotten diese:\n\n# Trials nach accuracy einteilen\nd_acc_rt_individual_grouped &lt;- d_acc_rt_individual %&gt;% \n  mutate(\n    performance = case_when(\n      accuracy &gt; 0.75 ~ \"good\",\n      accuracy &lt; 0.40 ~ \"bad\",\n      TRUE ~ \"ok\") %&gt;% \n      factor(levels = c(\"good\", \"ok\", \"bad\")))\n\n# Outlier visualisieren\nd_acc_rt_individual_grouped %&gt;% \n    ggplot(aes(x = id, y = accuracy, color = performance, shape = performance)) +\n    geom_point(size = 2, alpha = 0.6) + \n    geom_point(data = filter(d_acc_rt_individual_grouped, performance != \"OK\"), \n               alpha = 0.9) + \n    facet_grid(~condition) +\n    scale_color_manual(values = c(\"gray40\", \"steelblue\", \"red\")) +\n    geom_hline(yintercept = 0.33, linetype='dotted', col = 'black')+\n    annotate(\"text\", x = \"sub-100\", y = 0.33, label = \"chance level\", vjust = -1, size = 3) +\n    theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\nDasselbe könnte man für die Reaktionszeiten machen. Informationen dazu, wie Ausreisser in Reaktionszeiten gefunden und visualisiert werden können, finden Sie hier.\n\n\n\n12.1.3 Werte löschen\nEs können einzelne Trials oder auch die gesamten Daten einer Versuchsperson ausgeschlossen werden, weil z.B. die Accuracy zu tief war, fehlende Werte bestanden, etc.\nHierbei ist wichtig:\n\nDatenpunkte werden nie aus den Rohdaten gelöscht, sondern nur aus dem aktuell geladenen Datensatz, welcher für die Analysen verwendet und neu abgespeichert wurde. So können wir uns immer noch umentscheiden und verlieren nicht die Information, welche Daten gefehlt haben.\nDadurch, dass die Datenverarbeitung in R mit reproduzierbarem Code geschrieben ist, können wir immer überprüfen, ob ein Fehler in unserer Datenverarbeitung zu den missings geführt hat und diesen evtl. korrigieren.\nEs macht nicht immer Sinn die Trials mit missing data zu löschen! Dies muss von Fall zu Fall entschieden werden. Wenn Versuchspersonen zum Beispiel teilweise zu lange brauchten um eine Aufgabe zu lösen, dann ist das eine wichtige Information, wird diese rausgelöscht, wird die Leistung der Versuchsperson systematisch überschätzt.\n\nIn unserem Beispiel macht es Sinn, die drei Versuchspersonen, die extrem tiefe Accuracy hatten (wahrscheinlich weil sie die Aufgabe falsch verstanden hatten) vor den Analysen aus dem Datensatz auszuschliessen.Zuerst schauen wir uns an, welche Versuchspersonen sehr tiefe accuracy-Werte hatten:\n\nacc_rt_individual |&gt;\n    filter(accuracy &lt; 0.33) # 0.33 ist das chance level\n\n# A tibble: 3 × 6\n# Groups:   id [3]\n  id      condition       N ncorrect accuracy median_rt\n  &lt;fct&gt;   &lt;fct&gt;       &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 sub-009 incongruent    60        4   0.0667     1.25 \n2 sub-180 incongruent    60        1   0.0167     0.653\n3 sub-201 incongruent    60        1   0.0167     0.629\n\n\nDann schliessen wir diese 3 Personen aus. Hierbei empfiehlt es sich, den Code so zu kommentieren, dass man später direkt sieht, weshalb eine Person ausgeschlossen wurde.\n\n# Aussschluss dieser Personen\nd &lt;- d |&gt;\n    filter(id != \"sub-009\") |&gt; # Ausschluss wegen zu tiefer Accuracy (&lt; 0.33)\n    filter(id != \"sub-180\") |&gt; # Ausschluss wegen zu tiefer Accuracy (&lt; 0.33)\n    filter(id != \"sub-201\") # Ausschluss wegen zu tiefer Accuracy (&lt; 0.33)\n\n\n\n12.1.4 Verlaufseffekte: Ermüdung und Lernen\nVerlaufseffekte sind relevant, wenn man z.B. Ermüdungs- oder Lerneffekte ausschliessen möchte. Sie könnten aber auch inhaltlich interessant sein, z.B. in einem Trainingskontext, wenn man sich dafür interessiert, wie viele Trials nötig sind, damit sich jemand in einer Aufgabe verbessert.\nIn unserem Experiment möchten wir sicher sein, dass die Performanz sich über die Zeit hinweg nicht zu stark verändert. Hierzu können wir beispielsweise die accuracy plotten:\n\nd_acc_rt_trial &lt;- d |&gt;\n    group_by(condition, trial) |&gt;\n    summarise(\n        accuracy = mean(corr),\n        median_rt = median(rt)\n        )\n\nd_acc_rt_trial |&gt;\n    ggplot(aes(x = trial, y = accuracy, color = condition)) +\n    geom_point(size = 2, alpha = 0.8) +\n    geom_line() +\n    scale_color_manual(values = c(congruent = \"skyblue3\",\n                                 incongruent = \"tomato3\")) +\n    facet_wrap(~ condition) +\n    theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\nOder wir können uns die Reaktionszeiten über die Zeit hinweg anschauen.\n\nd_acc_rt_trial |&gt;\n    ggplot(aes(x = trial, y = median_rt, color = condition)) +\n    geom_point(size = 2, alpha = 0.8) +\n    geom_line() +\n    scale_color_manual(values = c(congruent = \"skyblue3\",\n                                 incongruent = \"tomato3\")) +\n    facet_wrap(~ condition) +\n    theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\nDas tun wir hier für 3 Versuchspersonen.\n\n# Plot: Reaktionszeit über die Trials hinweg (für 3 Versuchspersonen)\nd |&gt;\n    filter(id %in% c(\"sub-001\", \"sub-100\", \"sub-150\")) |&gt;\n    ggplot(aes(x = trial, y = rt, color = condition)) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    geom_point(alpha = 0.5) +\n    scale_color_manual(values = c(congruent = \"skyblue3\",\n                                 incongruent = \"tomato3\")) +\n    facet_wrap(~ id) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n12.1.5 Aufgabenschwierigkeit und Performanz der Versuchspersonen\nBevor wir die Daten analysieren, möchten wir wissen, ob die Personen die Aufgabe einigermassen gut lösen konnten (und wollten). In unserem Experiment erwarten wir eine Genauigkeit (accuracy) über dem Rateniveau von 33%. Wir plotten hierfür die accuracy für jede Person und Bedingung.\n\n# Plot accuracy per person and condition\np1 &lt;- d_acc_rt_individual |&gt; \n  ggplot(aes(x = condition, y = accuracy, color = condition)) +\n    geom_jitter(size = 3, alpha = 0.4, \n                width = 0.2, height = 0) +\n    geom_boxplot(width = 0.1, alpha = 0, color = \"black\") +\n    scale_color_manual(values = c(congruent = \"skyblue3\",\n                                 incongruent = \"tomato3\")) +\n    labs(x = \"Congruency\",\n         y = \"Proportion correct\",\n         title = \"Accuracy\",\n         subtitle = \"per Person and Condition\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\np2 &lt;- d_acc_rt_individual |&gt; \n  ggplot(aes(x = condition, y = median_rt, color = condition)) +\n    geom_jitter(size = 3, alpha = 0.4, \n                width = 0.2, height = 0) +\n    geom_boxplot(width = 0.1, alpha = 0, color = \"black\") +\n    scale_color_manual(values = c(congruent = \"skyblue3\",\n                                 incongruent = \"tomato3\")) +\n    labs(x = \"Congruency\",\n         y = \"Median Response Time [s]\",\n         title = \"Median Response Time\",\n         subtitle = \"per Person and Condition\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\nlibrary(patchwork)\n\np1 + p2\n\n\n\n\n\n\n\n\nUnd wir interessieren uns, wie sich die accuracy zwischen den Bedingungen unterscheidet. Das zeigt uns, ob die Instruktion eine Wirkung hatte. Dafür fügen wir Linien ein, die die accuracy- Werte pro Versuchsperson verbindet:\n\np3 &lt;- d_acc_rt_individual |&gt; \n    ggplot(aes(x = condition, y = accuracy, color = condition, group = id)) +\n    geom_line(color = \"grey40\", alpha = 0.5) +\n    geom_jitter(size = 3, alpha = 0.8, \n                width = 0, height = 0) +\n    scale_color_manual(values = c(congruent = \"skyblue3\",\n                                 incongruent = \"tomato3\")) +\n    labs(x = \"Congruency\",\n         y = \"Proportion correct\",\n         title = \"Accuracy\",\n         subtitle = \"per Person and Condition\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\np4 &lt;- d_acc_rt_individual |&gt; \n    ggplot(aes(x = condition, y = median_rt, color = condition, group = id)) +\n    geom_line(color = \"grey40\", alpha = 0.5) +\n    geom_jitter(size = 3, alpha = 0.8, \n                width = 0, height = 0) +\n    scale_color_manual(values = c(congruent = \"skyblue3\",\n                                 incongruent = \"tomato3\")) +\n    labs(x = \"Congruency\",\n         y = \"Median Response Time [s]\",\n         title = \"Median Response Time\",\n         subtitle = \"per Person and Condition\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\np3 + p4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on: Datenqualität\n\n\n\nBesprechen Sie miteinander, was Sie nun über unsere Daten wissen.\n\nHaben die Versuchspersonen die Aufgaben lösen können?\nWar die Aufgabe zu einfach, zu schwierig?\nDenken Sie, die Personen waren motiviert?\nWelche Datensätze / Trials möchten wir ausschliessen? (Achtung: Dies müsste eigentlich vor dem Anschauen der Daten entschieden werden, um zu verhindern, dass man Datenpunkte ausschliesst, welche die Hypothese nicht bestätigen.)\nWie gut eignen sich die Daten, um die Forschungsfrage zu beantworten?\nWas könnte bei einem nächsten Experiment besser gemacht werden?",
    "crumbs": [
      "Datenvisualisieren",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Grundlagen der Datenvisualisierung</span>"
    ]
  },
  {
    "objectID": "data_visualization_2.html#analyse-daten-zusammenfassen-und-explorieren",
    "href": "data_visualization_2.html#analyse-daten-zusammenfassen-und-explorieren",
    "title": "12  Grundlagen der Datenvisualisierung",
    "section": "12.2 Analyse: Daten zusammenfassen und explorieren",
    "text": "12.2 Analyse: Daten zusammenfassen und explorieren\nGrafiken können einerseits eine Ergänzung zur statistischen Datenanalyse sein, wie auch die Resultate der Analysen (bspw. geschätzte Parameterwerte) visualisieren. Sie haben den Vorteil, dass Informationen über Daten oder Analyseergebnisse gleichzeitig ersichtlich sind, sie können also vom Betrachtenden direkt verglichen werden.\nWir möchten die Daten hinsichtlich der Forschungsfragen visualisieren. Die Grafiken müssen vor allem präzise und informativ sein. Um Schlüsse aus Daten ziehen zu können, müssen diese zusammengefasst werden. Dazu eignen sich Lagemasse oder Masse der zentralen Tendenz, also beispielsweise der Mittelwert, Median oder Modus. Gleichzeitig ist es wichtig, dass auch Verteilungsmasse berichtet werden, wie Standardabweichungen oder Standardfehler. Wir können auch mit Werte aus statistischen Modellen, wie Parameterschätzungen und Konfidenzintervalle, grafisch darstellen.\nMit Hilfe von Visualisierungen können z.B. Aussagen können gemacht werden über:\n\nVerteilung der Daten\nZusammenhänge von Variablen (Korrelationen, Zeitverläufe)\nVergleiche und Unterschiede von Gruppen / Bedingungen\n\n\n12.2.1 Verteilung der Rohdaten\nDaten von neurowissenschaftlichen Studien können wichtige Informationen enthalten, die ohne Grafiken übersehen werden können (Rousselet, Pernet, and Wilcox (2017)). Das Visualisieren kann Muster zum Vorschein bringen, die durch statistische Auswertungen nicht sichtbar sind. Die Wichtigkeit von Datenvisualisierung für das Entdecken von Mustern in den Daten zeigte Francis Anscombe 1973 mit dem Anscombe’s Quartet. Dies diente als Inspiration für das Erstellen des “künstlichen” Datensatzes DatasaurusDozen, welchen wir in der letzten Veranstaltung visualisiert haben. Verschiedene Rohwerte, können dieselben Mittelwerte, Standardabweichungen und Korrelationen ergeben. Nur wenn man die Rohwerte plottet erkennt man, wie unterschiedlich die Datenpunkte verteilt sind.\nDies wird ersichtlich, wenn wir die Mittelwerte und Standardabweichungen für jede Gruppe berechnen und plotten:\n\n# load DatasaurusDozen dataset\ndino_data &lt;- read.csv(\"data/DatasaurusDozen.csv\") %&gt;%\n    mutate(condition = as.factor(condition))\n\n# Plot mean and standard deviation for value 1 per condition \ndino_data |&gt;   \n    group_by(condition) |&gt;\n    summarise(mean_value1 = mean(value1),\n              sd_value1 = sd(value1)) |&gt;\n    ggplot(mapping = aes(x = mean_value1,\n                     y = condition)) +\n    geom_point() +\n    geom_errorbar(aes(xmin = mean_value1 - sd_value1, \n                      xmax = mean_value1 + sd_value1), \n                  width = 0.2) +\n    theme_minimal()\n\n\n\n\n\n\n\n\nUnd dann die Rohwerte visualisieren:\n\n# Plot raw values\ndino_data |&gt; \n    ggplot(aes(x = value1, y = value2)) +\n    geom_point(size = 1) +\n    facet_wrap(~condition) +\n    theme_minimal()\n\n\n\n\n\n\n\n\nHier sehen Sie das Ganze animiert:\n\n\n\nDatensatz und Visualisierung von Matejka and Fitzmaurice (2017)\n\n\n\n\n12.2.2 Zentrale Tendenz und Verteilungsmasse\nMasse der zentralen Tendenz sind beispielsweise der Mittelwert, der Median und Modus. Wenn wir uns dafür interessieren, wie sich die accuracy in Bezug auf alle Teilnehmenden verhält, schauen wir uns die zentrale Tendenz über alle Personen hinweg an. Es sollte nie nur die zentrale Tendenz, sondern immer auch ein passendes Verteilungsmass berichtet werden.\nWie oben schon gezeigt können wir dies z.B. mit Boxplots umsetzen Diese zeigen uns den Median und die Quartile sowie Ausreisser an. Eine andere Möglichkeit Verteilungen anzuzeigen sind die Violinplots. Hier wurden mit geom_jitter() auch die Mittelwerte der einzelnen Personen im Plot eingefügt.\n\n# Boxplot\np_boxplot &lt;- d_acc_rt_individual |&gt; \n  ggplot(aes(x = condition, y = accuracy, color = condition)) +\n    geom_jitter(alpha = 0.25, width = 0.2) +\n    geom_boxplot(alpha = 0, width = 0.2, color = \"black\") +\n    scale_fill_manual(values = c(congruent = \"skyblue3\",\n                                 incongruent = \"tomato3\")) +\n    labs(title = \"Boxplot\",\n         x = \"Congruency\",\n         y = \"Proportion correct\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\n# Violin Plot\np_violin &lt;- d_acc_rt_individual |&gt; \n  ggplot(aes(x = condition, y = accuracy, color = condition)) +\n    geom_jitter(alpha = 0.5, width = 0.2) +\n    geom_violin(alpha = 0, width = 0.2, color = \"black\") +\n    scale_fill_manual(values = c(congruent = \"skyblue3\",\n                                 incongruent = \"tomato3\")) +\n    labs(title = \"Violin Plot\",\n         x = \"Congruency\",\n         y = \"Proportion correct\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\np_boxplot + p_violin\n\n\n\n\n\n\n\n\nDas Package {ggridges} bietet die Möglichkeit die Verteilungen zu plotten. Mehr Informationen hierzu finden Sie hier in der Dokumentation.\n\nlibrary(ggridges)\np5 &lt;- d_acc_rt_individual |&gt; \n    ggplot(aes(x = accuracy, y = condition, fill = condition)) + geom_density_ridges2(scale = 0.5, alpha = 0.5) +\n    scale_fill_manual(values = c(congruent = \"skyblue3\",\n                                 incongruent = \"tomato3\")) +\n    labs(title = \"Accuracy\",\n         x = \"Proportion corect\",\n         y = \"Congruency\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\np6 &lt;- d_acc_rt_individual |&gt; \n    ggplot(aes(x = median_rt, y = condition, fill = condition)) + geom_density_ridges2(scale = 1, alpha = 0.5) +\n    scale_fill_manual(values = c(congruent = \"skyblue3\",\n                                 incongruent = \"tomato3\")) +\n    labs(title = \"Median Response Time\",\n         x = \"Median Response Time [s]\",\n         y = \"Congruency\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\np5 + p6\n\n\n\n\n\n\n\n\n\n\n12.2.3 Aggregierte Statistiken\nWenn wir Mittelwerte und Standardfehler angeben möchten können wir dies wie folgt tun. Wichtig ist hier, dass wir within-subject Standardfehler berechnen. Genaueres zu den Unterschieden zwischen within- und between-subject Standardfehlern finden Sie hier. Wir verwenden das Package {Rmisc}. Achtung: Da dieses jedoch wegen der Namen der Funktionen oft Namens-Konflikte auslöst, laden wir Rmisc nicht mit der library()-Funktion, sondern stellen es einfach vor die benötigte Funktion, z.B. so Rmisc::summarySEwithin().\n\nd_acc_within &lt;- d |&gt;\n    Rmisc::summarySEwithin(measurevar = \"corr\",\n                               withinvars = \"condition\",\n                               idvar = \"id\",\n                               na.rm = FALSE,\n                               conf.interval = 0.95)\n\np7 &lt;- d_acc_within |&gt;\n    ggplot(aes(x = condition, y = corr, group = 1)) +\n    geom_line() +\n    geom_errorbar(width = .1, aes(ymin = corr-se, ymax = corr+se)) +\n    geom_point(size = 3) +\n    labs(title = \"Accuracy\",\n         x = \"Congruency\",\n         y = \"Accuracy\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\n\nd_rt_within &lt;- d |&gt;\n    Rmisc::summarySEwithin(measurevar = \"rt\",\n                               withinvars = \"condition\",\n                               idvar = \"id\",\n                               na.rm = FALSE,\n                               conf.interval = 0.95)\n\np8 &lt;- d_rt_within |&gt;\n    ggplot(aes(x = condition, y = rt, group = 1)) +\n    geom_line() +\n    geom_errorbar(width = .1, aes(ymin = rt-se, ymax = rt+se)) +\n    geom_point(size = 3) +\n    labs(title = \"Median Response Time\",\n         x = \"Congruency\",\n         y = \"Median Response Time [s]\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\n\np7 + p8\n\n\n\n\n\n\n\n\n\nHier finden Sie weitere Code-Beispiele für das Plotten von Verteilungsmassen.\nHier finden Sie Informationen, wie Reaktionszeiten zusammengefasst und visualisiert werden könnten.\n\n\n\n12.2.4 Visualisieren von Modellschätzungen\nWenn für die statistische Analyse ein Modell geschätzt wurde, kann auch dies visualisiert werden. Auf diese Form der Visualisierung wird in diesem Kapitel aber nicht eingegangen.\n\nsjPlot: Package zum Plotten von Fixed Effects\nsee: Package zum Visualisieren von Statistischen Modellen",
    "crumbs": [
      "Datenvisualisieren",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Grundlagen der Datenvisualisierung</span>"
    ]
  },
  {
    "objectID": "data_visualization_2.html#kommunikation-forschungsergebnisse-visualisieren",
    "href": "data_visualization_2.html#kommunikation-forschungsergebnisse-visualisieren",
    "title": "12  Grundlagen der Datenvisualisierung",
    "section": "12.3 Kommunikation: Forschungsergebnisse visualisieren",
    "text": "12.3 Kommunikation: Forschungsergebnisse visualisieren\nKommunikation der Ergebnisse findet vor allem in den wissenschaftlichen Artikeln, Postern oder Präsentationen statt. Bei Visualisierungen die der Kommunkation dienen sind folgende Merkmale wichtig:\n\n12.3.1 Beschriftungen\nDie genaue Beschriftung und deren Lesbarkeit ist für diese Form von Grafiken zentral. Achten Sie sich auf Folgendes:\n\nDie Achsenbeschriftungen enthalten die verwendete Variable in Klartext (nicht den R Variablennamen) und wenn zutreffend auch die Masseinheit (z.B. Response Time [ms]). Beschriftungen können Sie einfügen mit labs().\n\n\np_boxplot +\nlabs(title = \"Der Titel der Grafik\", \n     subtitle = \"Der Subtitel der Grafik\",\n     x = \"hier kommt Label x [Masseinheit]\", \n     y = \"hier kommt Label y [Masseinheit]\",\n     caption = \" Hier kommt eine Caption\")\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's fill values.\n\n\n\n\n\n\n\n\n\n\nFarben / Formen usw. werden in einer Legende den Gruppen zugeordnet (Ausnahme: wenn Daten von einzelnen Personen geplottet werden, wird die Versuchspersonennummer nicht aufgeführt).\nMasse der zentralen Tendenz und Varianzmasse werden beschrieben (z.B. Standardfehler oder Standardabweichung?)\n\n\n\n12.3.2 Fünf Merkmale einer guten Grafik\nEs gibt unzählige Optionen die eigenen Daten zu visualisieren. Folgende Prinzipien helfen beim Erstellen einer informativen Grafik, die zur Kommunikation der Ergebnisse dient.\n\nDie Punkte 3-5 wurden aus dem Buch “The Visual Display of Quantitative Information” von Edward Tufte, 1986 entnommen.\n\n\n1. Eine Frage beantworten\nJede Grafik sollte mindestens eine teilweise aber auch mehrere Fragen beantworten.\n👉 Welche Frage möchte ich beantworten? Welche Form der Visualisierung beantwortet diese Frage am besten?\nHierbei kann es hilfreich sein den “Arbeitstitel” der Grafik als Frage zu formulieren.\n\n\n2. Zielgruppe berücksichtigen\nBeim Erstellen der Grafik sollte beachtet werden, an wen sich die Grafik richtet. Für eine Präsentation müssen die Achsenbeschriftungen vergrössert und die Grafik simpel gehalten werden. In einem wissenschaftlichen Artikel kann die Grafik komplexer gestaltet werden, da die Lesenden sich mehr Zeit zum Anschauen nehmen können. Zudem sollten hier die Vorgaben des Journals berücksichtigt werden. Auch wichtig ist das Verwenden von “farbenblind-freundlichen” Palletten, rot und grün ist z.B. eine schlechte Wahl.\n👉 Für welchen Zweck / für wen erstelle ich die Grafik? Wie ist das Vorwissen des Zielpublikums?\n\nFür einen Fachartikel lohnt es sich, zu Beginn die Vorgaben der Fachzeitschrift zu berücksichtigen.\n\n\n\n3. Die Daten zeigen\nDas tönt simpel, wird aber oft nicht berücksichtigt. Bei einer Grafik geht es in erster Linie um die Daten. Es sollte die simpelste Form gewählt werden, welche die Informationen vermittelt. Oft braucht es keine ausgefallenen Grafikideen oder neuartigen Formate. Hierbei ist es wichtig, die Art der Daten zu berücksichtigen: Wie viele Variablen sind es? Sind diese kontinuierlich (z.B. Reaktionszeiten) oder diskret (z.B. Experimentalbedingungen)? Wie viele Dimensionen haben meine Daten? Mit zwei Achsen lassen sich zwei Dimensionen darstellen, zusätzlich können mit Farben und Formen noch weitere Dimensionen abgebildet werden (z.B. Millisekunden, Bedingung 1 und Bedingung 2). Es können Rohwerte geplottet werden oder summary statistics (z.B. Mittelwerte, Standardabweichungen)\n👉 Welche Art Grafik eignet sich für meine Frage und meine Daten? Schauen Sie z.B. hier nach oder nutzen Sie das esquisse-Package.\n\nBeispiele für verschiedenen Plots in R sind z.B. histogram, boxplot, violin plot, scatter plot / correlogram, jitter plot, raincloud plot, percentiles / shift functions, area chart, heat map.\n\n\n\n4. Optimieren des data-ink ratios\nDas Daten-Tinte-Verhältnis sollte so optimal wie möglich sein. Das bedeutet, das idealerweise jeder Strich, jeder Punkt, jedes Textfeld Information beinhaltet. Alles was keine Information transportiert oder nur wiederholt kann weggelassen werden.\n👉 Was kann ich weglassen?\n\nIn R kann zum Schluss des Plots + theme_minimal() hinzugefügt werden, dies entfernt u.a. den grauen Hintergrund. Das Grau des Hintergrunds ist Farbe (ink), welche keine Information transportiert, das Weglassen lässt die Grafik ruhiger wirken.\n\n\n\n5. Feedback einholen und revidieren\nDas Erstellen einer guten Grafik ist iterativ, das heisst, sie wird immer wieder überarbeitet, bis sie die Information möglichst einfach, genau aber klar kommuniziert. Hierbei ist Feedback oft unerlässlich.\n👉 Was denken andere über Ihre Grafik?\n\n\n\n\n\n\nMatejka, Justin, and George Fitzmaurice. 2017. “Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics Through Simulated Annealing.” In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, 1290–94. Denver Colorado USA: ACM. https://doi.org/10.1145/3025453.3025912.\n\n\nRousselet, Guillaume A., Cyril R. Pernet, and Rand R. Wilcox. 2017. “Beyond Differences in Means: Robust Graphical Methods to Compare Two Groups in Neuroscience.” European Journal of Neuroscience 46 (2): 1738–48. https://doi.org/10.1111/ejn.13610.",
    "crumbs": [
      "Datenvisualisieren",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Grundlagen der Datenvisualisierung</span>"
    ]
  },
  {
    "objectID": "data_visualization_2.html#footnotes",
    "href": "data_visualization_2.html#footnotes",
    "title": "12  Grundlagen der Datenvisualisierung",
    "section": "",
    "text": "Für das Entzippen mit Windows machen Sie einen Rechtsklick auf den Ordner mit dem Reissverschluss und wählen Sie Entpacken nach und geben Sie den Ordner an, in dem Sie alle Ihre RProject-Ordner speichern. Für das Entzippen mit Mac speichern Sie den heruntergeladenen Ordner in den Ordner, in dem Sie alle Ihre RProject-Ordner speichern und Doppelklicken Sie danach auf den Ordner. Nur entzippte Ordner können einwandfrei verwendet werden!↩︎",
    "crumbs": [
      "Datenvisualisieren",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Grundlagen der Datenvisualisierung</span>"
    ]
  },
  {
    "objectID": "uebung_3.html",
    "href": "uebung_3.html",
    "title": "Übung 3",
    "section": "",
    "text": "Auftrag\nTeil A: Datapreprocessing-Pipeline  Erstellen Sie eine automatisierte Datenverarbeitungs-Pipeline, die die Daten des Random Dot Experiments einlesen und vorverarbeiten.\nTeil B: Datenvisualisieren Erstellen Sie einen Plot der Random Dot Daten. Verwenden Sie dazu ggplot(). Alle Plots und der entsprechende Code werden in der Galerie auf der Kurshomepage anonym veröffentlicht.\nWichtig:",
    "crumbs": [
      "Datenvisualisieren",
      "Übung 3"
    ]
  },
  {
    "objectID": "uebung_3.html#auftrag",
    "href": "uebung_3.html#auftrag",
    "title": "Übung 3",
    "section": "",
    "text": "Information zum Arbeiten in Kleingruppen:\n\nÜbungen dürfen alleine oder in Gruppen von max. 3 Personen erledigt werden. Alle Personen müssen die Übung auf Ilias hochladen, um die Übung zu bestehen.\nDie Files von Gruppenarbeiten müssen folgendermassen benannt werden, damit wir sehen, welche Übungsabgaben zusammengehören: Nennen Sie das File mit der Aufgabe und allen Initialen der Gruppe. Z.B. uebung_1_GW_EW.R. Geben Sie bei allen Files die Initialen in derselben Reihenfolge an.\n\nArbeitsanweisung unter dem Unterkapitel Vorgehen:\n\nLesen Sie die Anweisungen genau durch und geben Sie die Dateien in diesem Format ab (z.B. Benennung der Dateien).\nArbeiten Sie entlang der Arbeitsanweisung, um den Prozess zu vereinfachen.",
    "crumbs": [
      "Datenvisualisieren",
      "Übung 3"
    ]
  },
  {
    "objectID": "uebung_3.html#vorgehen",
    "href": "uebung_3.html#vorgehen",
    "title": "Übung 3",
    "section": "Vorgehen",
    "text": "Vorgehen\n\nDownload und Setup\n\nLaden Sie das R-Project uebung-3 herunter und entzippen Sie den Ordner. Im data-Ordner finden Sie die Einzeldateien vom Experiment.\nAchtung: Sie müssen nur die Daten des Random Dot Experiments einlesen, vorverarbeiten und visualisieren. Die Daten des Stroop Experiments wurden in der Veranstaltung bearbeitet.\n\n\n\nA. Datapreprocessing-Pipeline\n\n\n\n\n\n\nTipp\n\n\n\nAm besten erstellen Sie zuerst für einen Datensatz einen funktionierenden Vorverarbeitungsablauf. Dann erstellen Sie eine Funktion für diesen Ablauf. In einem letzten Schritt automatisieren Sie dann diesen Ablauf für alle Datensätze im Datenordner indem Sie eine Liste mit allen Filenamen erstellen. Sie können sich an dem Automatisierungsbeispiel mit dem Stroop Datensatz orientieren.\nDas Einlesen kann eine Weile dauern, es sind sehr viele Datensätze.\n\n\n\nErstellen Sie ein neues .R-File und speichern Sie dieses unter preprocessing_random_dot.R im Projekt-Ordner. Sie können auch ein RNotebook erstellen! Falls Sie in einer Gruppe arbeiten speichern Sie dieses mit den Initialen ab, z.B. preprocessing_random_dot_initialen_initialen_initialen.R.\na. Packages laden: Laden Sie das Package {tidyverse}.\nb. Daten einlesen (read.csv())\nc. Daten filtern, so dass nur Experimenttrials im Datensatz sind, keine Übungsaufgaben. (filter())\nd. Erstellen zwei neuer Variablen namens trial (diese Variable gibt die Trialnummer startend mit 1 an und initials (diese Variable gibt Ihre Initialen an) mit (mutate()). Falls Sie in einer Gruppe arbeiten, geben Sie mehrere Initialen in den Variablen initals1, initials2 und initials3 an.\ne. Datensatz vereinfachen: Der Datensatz soll in dieser Reihenfolge folgende Informationen/Variablennamen enthalten (select()):\n  - Versuchspersonenidentifikation (`id`)\n  - Trialnummer (`trial`)\n  - Bewegungsrichtung der Punkte (`direction`)\n  - Instruktionsbedingung (`condition`)\n  - Korrekte Antwort für diesen Trial (`corrAns`)\n  - Antwort der Versuchsperson (`resp`), \n  - war die Antwort der Versuchsperson korrekt? (`corr`)\n  - Antwortzeit der Versuchsperson (`rt`)\n  - Initialen der ausführenden Person (`initials`)\nf. Automatisieren\n\nErstellen Sie nun eine Funktion, die dies für alle Random Dot Datensätze ausführt und einen aggregierten Datensatz erstellt. Hier finden Sie ein Anwendungsbeispiel dazu.\n\ng. Erstellter Datensatz kontrollieren:\n\nLöschen Sie nun alle Variablen in der RStudio Umgebung (Environment) mit dem Besen-Icon oben rechts und führen Sie den Code nochmals aus. Wenn alles funktioniert, fahren Sie weiter.\n\nh. Datensatz speichern:\n\nDatensatz für Ilias: Speichern Sie den neuen Datensatz (der jetzt alle Datensätze vorverarbeitet und zusammengefügt enthält) als .csv-File namens dataset_random_dot_clean_initialen.csv in Ihren data-Ordner.\nDatensatz fürs Visualisieren: Speichern Sie den Datensatz als dataset_random_dot_clean.csv im data-Ordner ab.\n\ni. Gespeicherten Datensatz kontrollieren:\n\nIhr Datensatz sollte nun wie untenstehend aussehen. Benutzen Sie dazu in Ihrem Code den Sie abgeben zwingend die Funktion glimpse(). (Ohne glimpse() ist Ihre Abgabe ungültig.)\n\n\n\n\nRows: 28,680\nColumns: 8\n$ id        &lt;chr&gt; \"sub-001\", \"sub-001\", \"sub-001\", \"sub-001\", \"sub-001\", \"sub-…\n$ trial     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ direction &lt;chr&gt; \"right\", \"left\", \"right\", \"left\", \"right\", \"left\", \"right\", …\n$ condition &lt;chr&gt; \"speed\", \"speed\", \"speed\", \"speed\", \"speed\", \"speed\", \"speed…\n$ corrAns   &lt;chr&gt; \"right\", \"left\", \"right\", \"left\", \"right\", \"left\", \"right\", …\n$ resp      &lt;chr&gt; \"right\", \"left\", \"right\", \"left\", \"right\", \"left\", \"left\", \"…\n$ corr      &lt;int&gt; 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, …\n$ rt        &lt;dbl&gt; 49.83123, 49.61717, 49.31203, 49.57549, 50.58607, 49.55812, …\n\n\n\n\nB. Datenvisualisieren\n\nIm Ordner finden Sie eine Datei namens initialen_plot.R. Öffnen Sie die die Datei initialen_plot.R. Der Inhalt dieser Datei muss gleich aussehen, wie im Beispiel unten.\nÄndern Sie den Namen der Datei initialen_plot.R, indem Sie Ihre Initialen (oder mehrere: initialen_initialen_initialen_plot.R) einsetzen. Das File muss korrekt benannt werden für eine gültige Abgabe!\nDer Code auf von Zeile 1 bis Zeile 8 darf nicht verändert werden!\nFügen Sie den Code für Ihre Abbildung ab Zeile 9 ein.\nDer eingefügte Code muss die Abbildung erstellen (vgl. Zeile 9-12) und anzeigen (vgl. Zeile 12).\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\np = d |&gt;\n    ggplot(...) +\n    ...\np\n\n\nDer Plot muss Folgendes beinhalten:\n\nBeides, Rohdaten UND mind. 1 zusammenfassendes Mass(z.B. Mittelwert mit Standardabweichungen, Box-/Violinplot, etc.). TIPP: Mehrere Geoms können übereinander gelegt werden.\nMind. 2 unterschiedliche Farben (schwarz und weiss ausgenommen).\nBeschriftungen: Titel, Subtitel, Achsenbeschriftungen, (optional: Captions)\nDer Subtitel beinhaltet die Frage, welche der Plot beantwortet.\n\nEin Theme verwenden.\nOptional: Facets verwenden.\n\n\n\n\nHochladen der Dateien auf Ilias\nLaden Sie folgende Dateien unter Übung 3 auf Ilias hoch:\n\ndataset_random_dot_clean_initialen.csv-File\npreprocessing_random_dot_initialen_initialen_initialen.R\ninitialen_initialen_initialen_plot.R\n.jpg oder .png (oder anderes Bildformat) Ihres Plots\n\n\n\n\n\n\n\nWichtig\n\n\n\nIhr Plot und der dazugehörige Code wird in der Galerie Gruppe 1 und Galerie Gruppe 2 anonym veröffentlich. Deshalb ist es wichtig, dass die oben aufgelisteten Voraussetzungen erfüllt sind.",
    "crumbs": [
      "Datenvisualisieren",
      "Übung 3"
    ]
  },
  {
    "objectID": "uebung_3.html#abgabetermin",
    "href": "uebung_3.html#abgabetermin",
    "title": "Übung 3",
    "section": "Abgabetermin",
    "text": "Abgabetermin\nDer Abgabetermin für diese Übung ist der 18. April 2025 (Nachholtermin: 21. Mai 2025).",
    "crumbs": [
      "Datenvisualisieren",
      "Übung 3"
    ]
  },
  {
    "objectID": "plots_group1.html",
    "href": "plots_group1.html",
    "title": "Plot Gallery - Group 1",
    "section": "",
    "text": "PlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\n# Packages laden\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nd &lt;- d |&gt;\n    mutate(across(where(is.character), as.factor)) |&gt;\n    mutate(corr = as.factor(corr))\n\n# Ausschliessen von zu kurzer & langer Reaktionszeit [s]\nd_clean &lt;- d |&gt;\n    filter(!is.na(rt), rt &gt; 0.1 & rt &lt; 12)\n\n# Fehlende Werte\nnaniar::vis_miss(d_clean)\n\n# condition_con umbenennen\nd_clean &lt;- d_clean %&gt;%\n  rename(`Korrektheit der Antwort` = corr)\n    \n\n# mittlere Reaktionszeit berechnen\nmean_data &lt;- d_clean %&gt;%\n  group_by(`Korrektheit der Antwort`, condition) %&gt;%\n  summarise(mean_rt = mean(rt, na.rm = TRUE), .groups = \"drop\")\n\n# Grafik für Daten von Random-Dot-Experiment\nggplot(data = d_clean, aes(\n         x = condition, \n         y = rt, \n         group = factor(`Korrektheit der Antwort`), # Faktor für Gruppierung\n         fill = `Korrektheit der Antwort`)) + \n   \n  # Rohdaten als Punkte\n  geom_jitter(position = position_dodge(width = 0.8), \n              alpha = 0.5, \n              color = \"black\") +\n   \n  # Mittelwerte als Balken\n  geom_bar(stat = \"summary\", fun = mean, \n           position = position_dodge(width = 0.8), \n           width = 0.7) +\n \n  # Zahlen zu Mittelwert-Balken hinzufügen\n  stat_summary(fun = mean,\n               geom = \"text\",\n               aes(label = round(..y.., 1)),\n               position = position_dodge(width = 0.8),\n               vjust = +2,\n               color = \"black\",\n               size = 3.5) +\n  \n  # Farbgebung und Legenden-Beschriftung\n  scale_fill_manual(values = c(\"darkviolet\", \"pink\"),\n                  labels = c(`1` = \"korrekte Antwort\", `0` = \"falsche Antwort\")) +\n\n  # Titel und Achsen-Beschriftungen\n  labs(title = \"Random-Dot-Test\",\n       subtitle = \"Gibt es Reaktionszeit-Unterschiede bei korrekter Antwort abhängig von Condition?\",\n       x = \"Condition\", \n       y = \"Reaktionszeit [s]\",\n       fill = \"Korrektheit der Antwort\") + \n  \n  # Theme\n  theme_bw()\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\n#Variablen als Faktoren definieren\nd &lt;- d |&gt; \n  mutate(condition = factor(condition, levels = c(\"accuracy\", \"speed\")), \n        corr = factor(corr, levels = c(\"1\", \"0\"), labels = c(\"richtige Antwort\", \"falsche Antwort\")))\n\n#Beginn Plot \np = d |&gt;\n  ggplot(mapping = aes(x = condition, y = rt, fill = corr)) + #globales mapping\n  geom_jitter(alpha = 0.2) + #Rohdaten\n  geom_violin(alpha = 0.7) + #zusammenfassendes Mass (Verteilung mit Violin Plot)\n  # Mittelwert + Standardabweichung (zusammenfassendes Mass):\n  stat_summary(fun = mean, geom = \"point\", position = position_dodge(0.9), size = 2, shape = 4, color = \"red\") + \n  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = \"errorbar\", position = position_dodge(0.9), width = 0.2) +\n  # Beschriftung & Theme\n  labs(\n    title = \"Reaktionszeiten in Abhängigkeit von Instruktion und Korrektheit\",\n    subtitle = \"Verändert sich die Reaktionszeit abhängig von Instruktionsbedingung und Korrektheit der Antwort?\",\n    x = \"Instruktion\",\n    y = \"Reaktionszeit (s)\",\n    fill = \"Korrektheit der Antwort\"\n  ) +\n  scale_fill_manual(values = c(\"seagreen\", \"lightblue\")) +\n  theme_minimal()\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv') \n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nacc_rt_individual &lt;- d |&gt;\n    group_by(id, condition) |&gt;\n    summarise(\n        N = n(),\n        ncorrect = sum(corr),\n        accuracy = mean(corr),\n        median_rt = median(rt))\n\np &lt;- acc_rt_individual |&gt;\n    ggplot(aes(x = condition, y = accuracy, color = condition)) +\n    geom_jitter(width = 0.1, alpha = 0.4, size = 1.8, height = 0) +\n    geom_boxplot(width = 0.3, color = \"black\", linewidth = 0.4, fill = NA) +\n    scale_color_manual(values = c(genau = \"darkturquoise\",\n                                  schnell = \"deeppink\"))+\n    labs(\n        title = \"Genauigkeit der Vpn im Random Dot Experiment\",\n        subtitle = \"Unterscheidet sich die Genauigkeit der Vpn zwischen den Bedingungen?\",\n        x = \"Bedingung\",\n        y = \"Anteil richtiger Antworten\"\n\n    ) +\n    theme_minimal(base_size = 14) +\n    theme(legend.position = \"none\")\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\n#Fragestellung: Sind Personen in der speed condition wirklich schneller als\n#in der accuracy condition?\n\ndfilter &lt;- d |&gt;\n    filter(!is.na(rt)) |&gt; #fehlende Werte von rt entfernen\n    filter(rt &gt; 0.099 & rt &lt; 8) |&gt; #zu tiefe und hohe rt Werte entfernen\n    group_by(condition, id) |&gt;\n    summarise(mean_rt = mean(rt), #für jede Person die mittlere rt berechnen\n              accuracy = mean(corr)) |&gt; #wie viel % der Antworten waren richtig pro Person\n    filter(accuracy &gt; 0.5) #Werte unterhalb der Ratewahrscheinlichkeit entfernen\n\n\np = dfilter |&gt;\n    ggplot(aes(x = condition,\n               y = mean_rt,\n               color = condition,\n               fill = condition)) +\n    geom_violin(alpha = 0.5, width = 0.5, color = \"black\") +\n    geom_jitter(width = 0.1, alpha = 0.1, color = \"black\") +\n    scale_fill_manual(values = c(accuracy = \"violetred\",\n                                 speed = \"cornflowerblue\"),\n                      name = \"Instruktion\",\n                      labels = c(speed = \"So schnell wie möglich\",\n                                 accuracy = \"So genau wie möglich\")) +\n    scale_x_discrete(labels = c(speed = \"Schnelligkeit\",\n                                accuracy = \"Genauigkeit\")) +\n    labs(title = \"Reaktionszeiten nach Bedingung\",\n         y = \"Reaktionszeit [ms]\",\n         x = \"Instruktion\",\n         subtitle = \"Wie verändert sich die Reaktionsgeschwindigkeit \\nbei unterschiedlichen Instruktionen?\") +\n    theme_classic(base_size = 12)\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\nlibrary(patchwork)\n\ndata &lt;- d |&gt;\n  group_by(trial, condition) |&gt;\n  summarise(\n    N = n(),\n    ncorrect = sum(corr),\n    accuracy = mean(corr),\n    median_rt = median(rt))\n\n\np1 = data |&gt;\n    ggplot(data = data,\n           mapping = aes(x = condition, \n                         y = median_rt, \n                         color = condition)) +\n             geom_jitter(size = 3, alpha = 0.4, \n                         width = 0.2, height = 0) + \n             geom_violin(width = 0.2, alpha = 0, color = 'grey1') + \n             scale_color_manual(values = c(accuracy = \"darkorchid3\", \n                                           speed = \"yellow3\")) + \n  labs(x = 'Bedingung',\n       y = 'Reaktionszeit',\n       title = 'Speed-Accuracy-Trade-Off',\n       subtitle = 'Wie verändern sich die Reaktionszeit und Accuracy in Abhängigkeit der Bedingung?') +\n  theme_minimal(base_size = 10) + \n  theme(legend.position = 'none')\n\np2 = data |&gt; \n  ggplot(data = data, \n         mapping = aes(x = condition, \n                       y = accuracy,\n                       color = condition)) +\n  geom_jitter(size = 3, alpha = 0.4, \n              width = 0.2, height = 0) +\n  geom_violin(width = 0.2, alpha = 0, color = 'grey1') + \n  scale_color_manual(values = c(accuracy = 'darkorchid3',\n                                speed = 'yellow3')) + \n  labs(x = 'Bedingung', \n       y = 'Accuracy') +\n  theme_minimal() + \n  theme(legend.position = 'none')\n\n  \np1 + p2\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\nd &lt;- d |&gt;\n  mutate(condition = as.factor(condition))\n#glimpse(d)\n#hist(d$rt)\n\nd_plot &lt;- d |&gt;\n  select(id, condition, corr, rt) |&gt;\n  filter(rt &lt;= 12)\n#hist(d_plot$rt)\n\n\n#Reaktionszeiten pro Bedingung\nd_summary_rt &lt;- d_plot |&gt;\n  group_by(condition) |&gt;\n  summarise(mean_rt = mean(rt, na.rm = TRUE),\n            sd_rt = sd(rt, na.rm = TRUE)) |&gt;\n  ungroup()\n#glimpse(d_summary_rt)\n\n#Reaktionszeiten pro Person + Bedingung\nd_summary_id &lt;- d_plot |&gt;\n  group_by(id, condition) |&gt;\n  summarise(mean_rt_id = mean(rt, na.rm = TRUE)) |&gt;\n  ungroup()\n#glimpse(d_summary_id)\n\n\n#Plot für Reaktionszeiten pro Bedingung\np_rt = d |&gt;\n  ggplot(data = d_plot,\n         mapping = aes(x = condition,\n                       y = rt)) +\n  geom_jitter(data = d_summary_id,\n              mapping = aes(x = condition,\n                            y = mean_rt_id,\n                            group = id),\n              alpha = 1, width = 0.2, color = \"pink\") +\n  geom_boxplot(alpha = 0.25, width = 0.7) + #auch mal mit geom_violin versuchen\n  geom_pointrange(data = d_summary_rt,\n                  aes(x = condition,\n                      y = mean_rt,\n                      ymin = mean_rt - sd_rt,\n                      ymax = mean_rt + sd_rt),\n                  color = \"olivedrab\",\n                  size = 1) +\n  labs(title = \"Reaktionszeiten nach Bedingung\",\n       subtitle = \"Unterscheidet sich die Reaktionszeit nach Bedingung?\",\n       x = \"Bedingung\",\n       y = \"Reaktionszeit [s]\") +\n  theme_minimal()\n#p_rt\n\n\n\n#Accuracy pro Person + Bedingung\nd_accuracy_id &lt;- d_plot |&gt;\n  group_by(id, condition) |&gt;\n  summarise(mean_acc_id = mean(corr, na.rm = TRUE)) |&gt;\n  ungroup()\n#hist(d_accuracy_id$mean_acc_id)\n\n#VP, die &lt; 0.4\nd_accuracy_filter &lt;- d_accuracy_id |&gt;\n  filter(mean_acc_id &lt; 0.4)\nd_accuracy_noid &lt;- d_plot |&gt;\n  filter(!id %in% c(\"sub-014\", \"sub-076\", \"sub-128\"))\n\nd_accuracy_clean &lt;- d_accuracy_noid |&gt;\n  group_by(condition) |&gt;\n  summarise(mean_acc = mean(corr, na.rm = TRUE),\n            se_acc = sd(corr, na.rm = TRUE) / sqrt(n())) |&gt;\n  ungroup()\n#glimpse(d_accuracy_clean)\n\n\n#Plot für Accuracy -&gt; Filter nach accuracy fehlt noch\np_accuracy = d |&gt;\n  ggplot(data = d_accuracy_clean,\n         mapping = aes(x = condition,\n                       y = mean_acc,\n                       fill = condition)) +\n  geom_col(width = 0.5) +\n  geom_errorbar(mapping = aes(ymin = mean_acc - se_acc,\n                              ymax = mean_acc + se_acc,\n                              width = 0.4)) +\n  labs(title = \"Accuracy nach Bedingung\",\n       subtitle = \"Unterscheidet sich die Accuracy nach Bedingung?\",\n       x = \"Bedingung\",\n       y = \"Accuracy\",\n       fill = \"Bedingung\") +\n  theme_minimal()\n#p_accuracy\n\nlibrary(patchwork)\np_rt / p_accuracy\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd &lt;- read_csv(\"data/dataset_random_dot_clean.csv\") \n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\np &lt;- d |&gt; \n  ggplot(aes(x = condition, y = rt, fill = condition)) +\n  geom_violin(alpha = 0.3, color = NA, width = 0.6) +\n  geom_jitter(width = 0.15, alpha = 0.2, color = \"black\", size = 0.8) +\n  stat_summary(fun = mean, geom = \"point\", shape = 21, fill = \"white\", size = 3, stroke = 1) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.15, color = \"black\") +\n  labs(title = \"Antwortzeiten im Random Dot Experiment\",\n       subtitle = \"Unterscheidet sich die Reaktionszeit je nach Instruktion (Speed vs Accuracy)?\",\n       x = \"Instruktionsbedingung\",\n       y = \"Reaktionszeit [ms]\") +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"none\")\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv') \n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\nlibrary(naniar)\nnaniar::vis_miss(d)\n\n\n\np = d |&gt;\n  filter(rt &gt; 0.001, rt &lt; 12) |&gt;\n  ggplot(aes(x = corr, y = rt, fill = condition)) +\n  geom_boxplot(alpha = 0.4, outlier.shape = NA) +  # Boxplot\n  geom_jitter(width = 0.2, alpha = 0.05, color = \"black\") +  # Rohdaten\n  stat_summary(fun = mean, geom = \"point\", shape = 18, size = 3, color = \"red\") +\n  facet_wrap(~condition) +\n  scale_fill_manual(values = c(\"skyblue3\", \"seagreen\")) + # Farben\n  labs( # Beschriftungen\n    title = \"Reaktionszeitvergleich und Anzahl \n    korrekter Antworten nach Bedingungen\",\n    x = \"0 = Falsch, 1 = Richtig\",\n    y = \"Reaktionszeit (in Sekunden)\",\n    subtitle = \"Unterscheidet sich die Anzahl korrekter Antworten wenn man schnell\n    reagieren musste im Vergleich zu wenn man genau antworten musste?\") +\n  scale_y_continuous(breaks = seq(0,12, by = 2)) +\n  scale_x_continuous(breaks = seq(0,1, by = 1)) +\n  theme_minimal(base_size = 13) # Theme\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\np = d |&gt;\n  filter(rt &gt;= 0.2 & rt &lt;= 120) |&gt; \n  ggplot(aes(x = factor(condition, levels = c(\"accuracy\", \"speed\"), labels = c(\"Genauigkeit\", \"Geschwindigkeit\")),\n             y = rt, \n             color = factor(corr, levels = c(0, 1), labels = c(\"Falsch\", \"Richtig\"))))+\n  facet_wrap(~ corr, labeller = as_labeller(c(\"0\" = \"Falsch\", \"1\" = \"Richtig\")))+\n  geom_jitter(alpha = 0.4, width = 0.4)+\n  geom_boxplot(color = \"black\")+\n  labs(title = \"Random Dot Experiment: Reaktionszeiten nach Antwortkorrektheit und Experimentalbedingung\", \n       subtitle = \"Sind richtige Antworten mit längeren Reaktionszeiten verbunden, unabhängig von der Instruktion?\",\n       x = \"Experimentalbedingungen (Instruktion)\", \n       y = \"Reaktionszeit in Sekunden\",\n       color = \"Korrektheit der Antwort\",\n       caption = \"Reaktionszeiten unter 0.2 s und über 120 s wurden entfernt\")+\n  scale_color_manual(values = c(\"Falsch\" = \"red\", \"Richtig\" = \"darkgreen\"))+\n  theme_bw()\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('/data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\n# Durchschnittliche Korrektheit & Reaktionszeit pro Bedingung berechnen\nd_summary &lt;- d |&gt;\n    group_by(condition) |&gt;\n    summarise(\n        mean_corr = mean(corr, na.rm = TRUE),\n        mean_rt = mean(rt, na.rm = TRUE)  # Durchschnittliche Reaktionszeit\n    )\n\n# Plot mit Balkendiagramm und RT als Text auf den Balken\np &lt;- ggplot(d_summary, aes(x = condition, y = mean_corr, fill = condition)) +\n    geom_col() +  # Balkendiagramm für Korrektheit\n    geom_text(aes(label = paste0(\"Reaktionszeit Durchschnitt:\\n\",round(mean_rt, 2), \" ms\")), vjust = 2, color = \"black\", size = 4, alpha = 0.7) +  # RT als Text auf den Balken\n    scale_fill_manual(values = c(\"orange\",\"lightblue\")) +\n    labs(\n        title = \"Random Dot\",\n        subtitle = \"Einfluss der Instruktion auf die Güte der Reaktion\",\n        x = \"Instruktion\",\n        y = \"Güte der Reaktion\",\n        fill = \"Instruktion\"\n    ) +\n    theme_minimal() +\n    theme(legend.position = \"none\")  # Entferne die Legende, da die 'condition' bereits durch die Farben dargestellt wird\n\n# Plot anzeigen\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\n\n\nd10 &lt;- d |&gt; \n  filter(rt &lt;= 10)\n#d10\n\n\nggplot(data = d10,\n       mapping = aes(x = condition, \n                     y = rt, \n                     color = factor(corr))) +\n  geom_jitter(width = 0.4, alpha = 0.5) +\n  scale_color_manual(name = \"Antwort\",\n                     labels = c(\"Falsch\", \"Richtig\"),\n                     values = c(\"#cb4335\", \"#82e0aa\")) +\n  geom_boxplot(width = 0.3) +\n  labs(title = \"Auswertung Random Dot Experiment\",\n       subtitle = \"Gibt es einen Unterschied in den Reaktionszeiten zwischen den beiden Bedingungen\n       und unterscheiden sie sich abhängig von der Korrektheit der Antworten?\",\n       x = \"Bedingung\",\n       y = \"Reaktionszeiten\") +\n  scale_x_discrete(labels = c(\"accuracy\" = \"Genauigkeit\",\n                              \"speed\" = \"Geschwindigkeit\")) +\n  theme_minimal()\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n  \n\n# Frage: Gibt es in den beiden Bedingungen Lern- oder eher Ermüdungseffekte?\n\n# Werte ausschliessen\nd &lt;- d |&gt; \n  filter(!is.na(rt)) |&gt; # fehlende Werte \n  filter(rt &gt; 0.099 & rt &lt; 10) # zu schnelle/langsame Antworten\n\n\nrt_Werte &lt;- d |&gt; \n  group_by(id, trial, rt, condition)|&gt;\n  summarise(N = n(),\n            accuracy = mean(corr))\n\n# Personen unter chance level ausschliessen\nrt_Werte &lt;- rt_Werte |&gt; \n  filter(accuracy &gt;= 0.5) \n\n# visualisieren\np &lt;- rt_Werte |&gt; \n  ggplot(aes(x = trial, y = rt, color = condition)) +\n  geom_point(size = 0.5, alpha = 0.4) +\n  scale_color_manual(values = c(speed = \"peachpuff4\",\n                                accuracy = \"cadetblue\")) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"grey30\") +\n  facet_wrap(~ condition) +\n  labs(x = \"Trial\",\n       y = \"Reaktionsgeschwindigkeit (s)\",\n       title = \"Verlaufseffekte\",\n       subtitle = \"Gibt es Lern- bzw. Ermüdungseffekte in den beiden Bedingungen?\") +\n  theme_test() +\n  theme(legend.position = \"none\")\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv') \n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\nlibrary(ggplot2)\n\nd_filtered &lt;- d |&gt; \n  filter(rt &lt;= 12)\n\nggplot(d_filtered) +\n  aes(x = condition, y = rt, fill = condition) +\n  geom_boxplot() +\n  scale_fill_hue(direction = 1) +\n  labs(\n    x = \"Condition\",\n    y = \"Reaction time \",\n    title = \"Übung 3: Daten visualisieren \",\n    subtitle = \"Reaction time under speed and accuracy condition\",\n    fill = \"Condition\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.y = element_text(size = 3L)) +\n  ylim(0, 12)\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\nd &lt;- d |&gt;\n  filter(rt &lt; 5)\n\nacc_rt_vpn &lt;- d |&gt;\ngroup_by(id, condition) |&gt;\n  summarise(\n    N = n(),\n    ncorrect = sum(corr),\n    accuracy = mean(corr),\n    mean_rt = mean(rt)\n  )\nacc_rt_vpn\n\nplot &lt;- acc_rt_vpn |&gt;\n  ggplot(mapping = aes(x = condition,\n                       y = mean_rt,\n                       colour = condition,\n                       fill = condition)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c( \"accuracy\" = \"pink\",\n                                \"speed\" = \"lightblue\")) +\n  scale_colour_manual(values = c(\"accuracy\" = \"pink\",\n                                 \"speed\" = \"lightblue\")) + \n  geom_jitter(colour = \"cyan4\", width = 0.2, size = 1.8, alpha = 0.4) +\n  geom_hline(yintercept = 5) +\n  theme_light() +\n  labs(x = \"Bedingung\",\n       y = \"Mittelwert der Reaktionszeit\",\n       condition = \"Bedingung\",\n       title = \"Reaktionszeit: Accuracy vs. Speed\",\n       subtitle = \"Unterscheiden sich die Reaktionszeiten zwischen der Bedingung Accuracy und der Bedingung Speed?\")\n\nplot\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\np &lt;- d |&gt; \n  ggplot(aes(x = direction, y = rt, fill = direction)) +\n  \n  # Violinplot für Verteilung\n  geom_violin(trim = FALSE, alpha = 0.6, color = NA) +\n  \n  # Rohdatenpunkte – etwas kleiner & transparenter\n  geom_jitter(width = 0.15, size = 0.6, alpha = 0.15, color = \"gray40\") +\n  \n  # Mittelwert sichtbar machen: größer, mit dickem Rand\n  stat_summary(\n    fun = mean,\n    geom = \"point\",\n    shape = 21,\n    size = 4.5,\n    fill = \"white\",\n    stroke = 1.1,\n    color = \"black\"\n  ) +\n  \n  # Standardfehler (Error Bars)\n  stat_summary(\n    fun.data = mean_se,\n    geom = \"errorbar\",\n    width = 0.2,\n    color = \"black\"\n  ) +\n  \n  # Farben: skyblue & orchid\n  scale_fill_manual(values = c(\"skyblue\", \"orchid\")) +\n  \n  # Titel, Subtitle, Achsenbeschriftungen\n  labs(\n    title = \"Reaktionszeiten im Random-Dot-Experiment\",\n    subtitle = \"Unterscheiden sich Reaktionszeiten zwischen Bewegungsrichtungen?\",\n    x = \"Bewegungsrichtung\",\n    y = \"Reaktionszeit (ms)\",\n    fill = \"Richtung\"\n  ) +\n  \n  # Klarer, moderner Stil\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 15),\n    plot.subtitle = element_text(size = 11),\n    axis.title = element_text(size = 12),\n    legend.position = \"top\"\n  )\n\n# Plot anzeigen\nprint(p)\n\n# Plot erstellen\np &lt;- d |&gt; \n  ggplot(aes(x = direction, y = rt, fill = direction)) +\n  \n  # Violinplot für Verteilung\n  geom_violin(trim = FALSE, alpha = 0.6, color = NA) +\n  \n  # Rohdatenpunkte (leicht transparent, nicht zu groß)\n  geom_jitter(width = 0.15, size = 0.6, alpha = 0.15, color = \"gray40\") +\n  \n  # Mittelwert sichtbar machen (weiß, mit schwarzem Rand)\n  stat_summary(\n    fun = mean,\n    geom = \"point\",\n    shape = 21,\n    size = 4.5,\n    fill = \"white\",\n    stroke = 1.1,\n    color = \"black\"\n  ) +\n  \n  # Standardfehler als Errorbars\n  stat_summary(\n    fun.data = mean_se,\n    geom = \"errorbar\",\n    width = 0.2,\n    color = \"black\"\n  ) +\n  \n  # Farben setzen: skyblue & orchid\n  scale_fill_manual(values = c(\"skyblue\", \"orchid\")) +\n  \n  # Titel, Subtitle, Achsenbeschriftungen\n  labs(\n    title = \"Reaktionszeiten im Random-Dot-Experiment\",\n    subtitle = \"Unterscheiden sich Reaktionszeiten zwischen Bewegungsrichtungen?\",\n    x = \"Bewegungsrichtung\",\n    y = \"Reaktionszeit (ms)\",\n    fill = \"Richtung\"\n  ) +\n  \n  # Theme mit weißem Hintergrund und feinen Gitterlinien\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 15),\n    plot.subtitle = element_text(size = 11),\n    axis.title = element_text(size = 12),\n    legend.position = \"top\",\n    \n    # Weißer Hintergrund\n    panel.background = element_rect(fill = \"white\", color = NA),\n    plot.background = element_rect(fill = \"white\", color = NA),\n    \n    # Dezente Gitterlinien\n    panel.grid.major = element_line(color = \"gray85\"),\n    panel.grid.minor = element_blank()\n  )\n\n# Plot anzeigen\nprint(p)\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\n\n## Graphik um Verteilung der Daten zu schauen und event. Ausreisser raus\n\nggplot(d, aes(x = id, y = rt)) +\n  geom_boxplot(outlier.color = \"red\", outlier.shape = 8, alpha = 0.6) +\n  labs(\n    title = \"Ausreissercheck: Reaktionszeit pro Versuchsperson\",\n    x = \"ID\",\n    y = \"Reaktionszeit (ms)\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(axis.text.x = element_blank())\n\n## Ich filtere die 2 Ausreisser raus indem ich &gt;1000ms Reaktionszeit rausnehme (danach n=238)\nd_clean = d |&gt; \n  filter(rt &lt;= 1000)\n\n## Mache erst eine Summary um übersicht der Daten zu bekommen\nsummary_table &lt;- d_clean |&gt;\n  group_by(condition) |&gt;\n  summarise(\n    trials = n(),\n    mean_rt = mean(rt, na.rm = TRUE),\n    sd_rt = sd(rt, na.rm = TRUE),\n    accuracy_rate = mean(corr, na.rm = TRUE),  # Anteil korrekter Antworten\n    .groups = \"drop\"\n  )\n\n#print(summary_table)\n\n## PLOT (Rohdaten + MW/SD )\n\n# Ich filtere zuerst nur korrekte Antworten für meinen Plot\nd_corr &lt;- d_clean %&gt;% filter(corr == 1)\n\nggplot() +\n geom_jitter( \n    data = d_corr,\n    aes(x = condition, y = rt, color = condition),# Rohdatenpunkte\n    alpha = 0.2,\n    width = 0.2,\n    size = 1,\n    show.legend = FALSE\n  ) +\n  geom_point(\n    data = summary_table,\n    aes(x = condition, y = mean_rt),  # Mittelwertpunkte\n    shape = 18,\n    size = 3,\n    color = \"black\"\n  ) +\n  geom_errorbar(\n    data = summary_table,\n    aes(x = condition, ymin = mean_rt - sd_rt, ymax = mean_rt + sd_rt),# SD-Balken\n    width = 0.2,\n    color = \"black\"\n  ) +\n  scale_color_manual(values = c(\"speed\" = \"turquoise\", \"accuracy\" = \"orange\")) +# Farben\n  # Achsen, Titel etc.\n  labs(\n    title = \"Reaktionszeiten im Random Dot Experiment\",\n    subtitle = \"Unterscheiden sich die Reaktionszeiten zwischen Speed- und Accuracy-Bedingung?\",\n    x = \"Instruktionsbedingung\",\n    y = \"Reaktionszeit (ms)\",\n    caption = \"n= 238 | Nur korrekte Antworten | Ausreisser (&gt;1000 ms) entfernt | Balken = MW + SD\"\n  ) +\n  # Theme\n  theme_minimal()  +\ntheme( plot.caption = element_text(hjust = 0)  # Links ausrichten\n  )\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv') \n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\np = d |&gt;\n    ggplot(aes(x = condition, y = rt, color = condition)) + #Rohdaten nach Bed. gruppiert\n    geom_jitter(alpha = 0.4, width = 0.5, size = 1) +\n    geom_violin(alpha = 0.4, width = 0.2, color = \"black\") + #Zusammenfassendes Mass\n    scale_color_manual(values = c(accuracy = \"deeppink\",\n                                  speed = \"darkcyan\")) +\n    labs(\n        x = \"Instruction Condition\",\n        y = \"Reaction Time (ms)\",\n        title = \"Impact of Instruction Type on Reaction Speed\",\n        subtitle = \"Does Emphasis on Accuracy vs. Speed Affect Reaction Times?\") +\n    theme_linedraw(base_size = 12) +\n    theme(legend.position = \"bottom\")\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\nlibrary(ggplot2)\nlibrary(RColorBrewer)\nd$corr_label &lt;- ifelse(d$corr == 1, \"correct\", \"incorrect\")\nfill_colors &lt;- c(\"correct\" = \"#B8E186\", \"incorrect\" = \"#DE77AE\")  \npoint_colors &lt;- c(\"correct\" = \"#4DAC26\", \"incorrect\" = \"#D01C8B\") \n\np = d |&gt;\n    filter(rt &lt;= 12) |&gt;\n    ggplot(aes(x = condition, y = rt, fill = corr_label)) +\n    geom_violin(\n        position = position_dodge(0.9),\n        trim = FALSE,\n        alpha = 0.6\n    ) +\n    geom_jitter(\n        aes(color = corr_label),\n        size = 0.6,\n        alpha = 0.15,\n        position = position_jitterdodge(jitter.width = 0.1, dodge.width = 0.9)\n    ) +\n    stat_summary(\n        fun = mean,\n        geom = \"crossbar\",\n        width = 0.4,\n        fatten = 2,\n        color = \"black\",\n        position = position_dodge(0.9)\n    ) +\n    scale_fill_manual(values = fill_colors, name = \"Antwort\") +\n    scale_color_manual(values = point_colors, guide = \"none\") +\n    labs(\n        title = \"Reaktionszeiten nach Instruktion und Antwortgenauigkeit\",\n        subtitle = \"Beeinflusst die Instruktion (Speed vs Accuracy), wie schnell und korrekt geantwortet wird?\",\n        x = \"Instruktion\",\n        y = \"Reaktionszeit (s)\"\n    ) +\n    theme_minimal() +\n    theme(\n        plot.title = element_text(size = 14, face = \"bold\"),\n        plot.subtitle = element_text(size = 12, face = \"italic\"),\n        axis.text.x = element_text(size = 12),\n        legend.position = \"top\"\n    )\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\np = d |&gt;\n    ggplot(aes(x = condition, y = rt, fill = condition)) + \n             geom_violin(trim = FALSE, alpha = 0.6) +\n             geom_boxplot(width = 0.2, color = \"black\", alpha = 0.5) +\n             stat_summary(fun = \"mean\", geom = \"point\", shape = 18, size = 3, color = \"red\") +\n             labs(\n               title = \"Verteilung der Reaktionszeiten nach Bedingung\",\n               subtitle = \"Welche Bedingung führt zu schnelleren Reaktionszeiten?\",\n               x = \"Bedingung\",\n               y = \"Reaktionszeit (Sekunden)\",\n               caption = \"Datenquelle: Random Dot Experiment\"\n             ) +\n             scale_fill_manual(values = c(\"blue\", \"green\")) +\n             theme_minimal() +\n             theme(\n               plot.title = element_text(hjust = 0.5),\n               plot.subtitle = element_text(hjust = 0.5),\n               plot.caption = element_text(size = 8, face = \"italic\")\n             )\n          p\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\n\np = d |&gt;\n  ggplot(aes(x = condition, y = rt, color = condition)) +\n  geom_point(size = 2, alpha = 0.5, position = position_jitter(width = 0.2)) +\n  geom_boxplot(aes(fill = condition), alpha = 0.3, width = 0.4) +\n  scale_color_manual(values = c(\"blue\", \"red\")) +\n  scale_fill_manual(values = c(\"blue\", \"red\")) +\n  labs(\n    title = \"Antwortzeiten nach Instruktionsbedingung\",\n    subtitle = \"Wie unterscheiden sich die Antwortzeiten zwischen den Instruktionsbedingungen?\",\n    x = \"Instruktionsbedingung\",\n    y = \"Antwortzeit (s)\"\n  ) +\n  theme_minimal()\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\nagg_data &lt;- d |&gt;\n  filter(rt &lt; 10) |&gt;\n  group_by(id, condition) |&gt;\n  summarise(mean_rt = mean(rt, na.rm = TRUE), .groups = \"drop\")\n\np &lt;- agg_data |&gt;\n  ggplot(aes(x = condition, y = mean_rt, color = condition)) +\n  geom_jitter(size = 3, alpha = 0.4, \n              width = 0.2, height = 0) +\n  geom_boxplot(width = 0.5, alpha = 0, color = \"black\") +\n  scale_color_manual(values = c(speed = \"skyblue\",\n                                accuracy = \"tomato\")) +  \n  stat_summary(fun = mean, geom = \"point\", size = 4, shape = 18) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.2) +  \n  labs(\n    title = \"Reaktionszeiten nach Bedingung\",\n    subtitle = \"Gibt es einen Unterschied zwischen den Bedingungen?\",\n    x = \"Bedingung\",\n    y = \"Mittlere Reaktionszeit (s)\",\n    color = \"Bedingung\"\n  ) +\n  theme_minimal()\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code: Code von Leo Rieder und Vivienne Schäublin\n\nd &lt;- read.csv(\"data/dataset_random_dot_clean.csv\") %&gt;%\n  mutate(condition = as.factor(condition)) # Variable condition zu Faktor konvertieren (wieso? ist das nicht schon der Fall?)\n\n#glimpse(d)\n\n# Ein Plot erstellen für die Fragestellung: Ist die Reaktionszeit in der Speed-Bedingung kürzer?\nggplot(data = d,\n       mapping = aes(x = condition,\n                     y = rt)) +\n  geom_jitter(width = 0.4, color= \"#009ACD\", alpha = 0.3) + \n  geom_boxplot(width = 0.2, color= \"#FF34B3\", alpha = 0.5) + # Boxplot-Farbe und transparenz, Boxplot als 2. Aufgelistet, dass es über den blauen Rohdatenpunkten erscheint und transparent, dass Rohdaten sichtbar bleiben.\n  labs(title = \"Vergleich der Reaktionszeiten zwischen den beiden Bedingungen\", \n       subtitle = \"Ist die Reaktionszeit in der Speed-Bedingung kürzer?\",\n       x = \"Bedingung\",\n       y = \"Reaktionszeit (ms)\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5), # Den Titel und Subtitel mittig ausrichten\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n# Zahlen zum Boxplot: hier können die Werte vom Mean und der SD ausgegeben werden\nunique(d$condition)\n# Mean und AD für die Bedingung speed berechnen\nd_speed &lt;- d %&gt;% \n  filter(condition == \"speed\")\n\nd_speed_summary &lt;- d_speed %&gt;% \n  summarise(mean_rt = mean(rt),\n            sd_value = sd(rt))\n#glimpse(d_speed_summary)\n\n# Mean und SD für die Bedingung accuracy berechenen\nd_accuracy &lt;- d %&gt;% \n  filter(condition == \"accuracy\")\n\nd_accuracy_summary &lt;- d_accuracy %&gt;% \n  summarise(mean_rt = mean(rt),\n                  sd_value = sd(rt))\n#glimpse(d_accuracy_summary)\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\np = d |&gt;\n    group_by(id, condition) |&gt;\n    summarise(N = n(), ncorrect = sum(corr), accuracy = mean(corr), median_rt = median(rt)) |&gt;\n    ggplot(mapping = aes(x = condition, y = accuracy, colour = condition)) +\n    geom_boxplot(width = 0.3) +\n    geom_jitter(width = 0.1) +\n    scale_colour_manual(labels = c(accuracy = \"genau\", speed = \"schnell\"), \n                        values = c(accuracy = \"mediumseagreen\", speed = \"coral\")) +\n    scale_x_discrete(labels = c(accuracy = \"genau\", speed = \"schnell\")) +\n    labs(title = \"Exaktheit der Versuchspersonen im Random Dot Experiment\",\n         subtitle = \"Beeinflusst die Art der Bedingung die Exaktheit der Versuchspersonen?\",\n         x = \"Bedingung\",\n         y = \"Anteil korrekter Antworten\") +\n    theme_classic()\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\nd_corr &lt;- d %&gt;%\n  filter(corr == 1) %&gt;%\n  group_by(direction) %&gt;%\n  mutate(id = row_number())  # Jeder Punkt bekommt eine y-Position\n\n# Zusammenfassung\nsummary_d &lt;- d_corr %&gt;%\n  group_by(direction) %&gt;%\n  summarise(count = n())\n\n# Plot\np &lt;- ggplot() +\n  # Rohdaten\n  geom_point(data = d_corr, aes(x = direction, y = id), \n             position = position_jitter(width = 0.2), \n             shape=21, \n             fill=\"lightgrey\", \n             color=\"black\", \n             alpha = 0.1, ) +\n  # Liniendiagramm mit Punkten\n  geom_line(data = summary_d, aes(x = direction, y = count, group = 1), color = \"lightblue\", size = 1.5) +\n  geom_point(data = summary_d, aes(x = direction, y = count), color = \"lightpink\", size = 4) +\n  #Beschriftung\n  labs(\n    title = \"Einfluss der Bewegungsrichtung auf richtige Antworten\",\n    subtitle = \"Welche Seite produziert mehr richtige Antworten?\",\n    x = \"Bewegungsrichtung\",\n    y = \"Anzahl richtiger Antworten\"\n  ) +\n  theme_minimal()\n        \np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n# Beginnen Sie hier mit Ihrem Code:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n#str(d)\n\n# Daten filtern: Reaktionszeiten &lt;= 12 Sekunden\nd_filtered &lt;- d %&gt;%\n  filter(rt &lt;= 12)\n\nd_filtered$condition &lt;- factor(d_filtered$condition, levels = c(\"accuracy\", \"speed\"))\n\n\n\np &lt;- ggplot(d_filtered, aes(x = condition, y = rt, color = condition)) +\n  geom_violin(trim = FALSE, fill = NA, size = 1.2) +\n  geom_jitter(width = 0.2, alpha = 0.6) +\n  stat_summary(fun = mean, geom = \"point\", shape = 18, size = 4, color = \"darkred\") +\n  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1),\n               geom = \"errorbar\", width = 0.2, color = \"darkred\") +\n  scale_color_manual(values = c(\"accuracy\" = \"#1f78b4\", \"speed\" = \"#33a02c\")) +\n  labs(\n    title = \"Reaktionszeiten nach Instruktionstyp\",\n    subtitle = \"Wie verändert sich das Entscheidungsverhalten von Menschen, je nachdem wie sie instruiert wurden?\",\n    x = \"Instruktionsbedingung\",\n    y = \"Reaktionszeit (Sekunden)\",\n    color = \"Bedingung\"\n  ) +\n  theme_minimal(base_size = 14)\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse) \nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\np = d |&gt;\n    ggplot(aes(x = condition, y = rt, color = condition)) + #Rohdaten nach Bed. gruppiert\n    geom_jitter(alpha = 0.4, width = 0.5, size = 1) +\n    geom_violin(alpha = 0.4, width = 0.2, color = \"black\") + #Zusammenfassendes Mass\n    scale_color_manual(values = c(accuracy = \"#FFB6C1\",\n                                  speed = \"#ADD8E6\")) +\n    labs(\n        x = \"Instruction Condition\",\n        y = \"Reaction Time (ms)\",\n        title = \"Impact of Instruction Type on Reaction Speed\",\n        subtitle = \"Does Emphasis on Accuracy vs. Speed Affect Reaction Times?\") +\n    theme_linedraw(base_size = 12) +\n    theme(legend.position = \"bottom\")\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\n# Mittelwert Reaktionszeit berechnen\nmean_rt &lt;- mean(d$rt, na.rm = TRUE)\n#mean_rt\n\n# Reaktionszeit bereinigen – sehr schnelle oder sehr langsame ausschließen\nd &lt;- d |&gt; \n  filter(rt &gt; 0.1, rt &lt; 8)\n\n# Plot erstellen\np &lt;- d |&gt;\n  ggplot(aes(x = condition, y = rt, fill = condition)) +\n  geom_jitter(width = 0.25, alpha = 0.3, size = 1.5, color = \"gray30\") +\n  geom_violin(alpha = 0.4, trim = FALSE) +\n  stat_summary(fun = mean, geom = \"point\", size = 2, color = \"black\") +\n  labs(\n    title = \"Instruktion beeinflusst Reaktionsgeschwindigkeit\",\n    subtitle = \"Sind Reaktionen bei Speed schneller als bei Accuracy?\",\n    x = \"Instruktionsbedingung\",\n    y = \"Reaktionszeit (Sekunden)\"\n  ) +\n  scale_fill_manual(values = c(\"tomato\", \"skyblue3\")) +\n  theme_light(base_size = 13)\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\n#Datensatz einlesen und Variablen konvertieren\nd_AW &lt;- d |&gt; filter(rt &lt; 12) |&gt;\n  mutate(\n    condition = as.factor(condition), # Variable condition zu Faktor konvertieren\n    corr = as.factor(corr)) # Variable corr zu Faktor konvertieren\n\n\n\n#Plot erstellen\n#Labels umbenennen\nfacet_labels &lt;- c(\"accuracy\" = \"Genauigkeit\", \"speed\" = \"Geschwindigkeit\")\ncondition_labels &lt;- c(\"0\" = \"falsch\", \"1\" = \"richtig\")\n\n#Plot Design\np = d_AW |&gt;\n  ggplot(aes(x = corr, y = rt)) +\n  geom_jitter(alpha = 0.2, color = \"gray\") +\n  geom_boxplot(aes(fill = corr)) +\n  labs(x = \"Bedingungen\",\n       y = \"Reaktionszeit in [s]\",\n       title = \"Vergleich Bedingungen\",\n       subtitle = \"Wie fällt die Reaktionszeit bei falschen\\nbzw. richtigen Antworten aus?\",\n       caption = \"falsch bzw. richtig = gegebene Antwort // braun: MW +/- 1SD\") +\n  stat_summary(fun.data = mean_sdl, #+/- 1 Standardabweichung darstellen\n               fun.args = list(mult = 1), \n               geom = \"errorbar\", \n               color = \"chocolate4\",\n               width = 0.2) +\n  stat_summary(fun = mean,\n               geom = \"point\", #Standardabweichung darstellen\n               color = \"chocolate4\", \n               shape = 18) +\n  scale_y_continuous(breaks = seq(0, 12, by = 2)) + #y-Achse Intervall ändern\n  scale_x_discrete(labels = condition_labels) +  #x-Achsen-Beschriftung ändern\n  scale_fill_manual(values = c('0' = 'peachpuff1', '1' = 'paleturquoise1')) + #0 = falsch, 1 = richtig\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"none\",\n        plot.title = element_text(hjust = 0.5), #zentrieren von Titel\n        plot.subtitle = element_text(hjust = 0.5), #zentrieren von Subtitel\n        plot.caption = element_text(hjust = 0.5)) + #zentrieren von Caption\n  facet_wrap(~condition, labeller = labeller(condition = facet_labels))\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\np = d |&gt;\n  ggplot(aes(x = corr, y =rt, fill= condition)) +\n  geom_violin(alpha = 0.7, linetype = 7, show.legend = FALSE) + # Zusammenfassendes Mass Violinplot \n  geom_jitter(width = 0.2, alpha = 0.05, color = \"black\", show.legend = FALSE) + # Rohdaten \n  stat_summary(fun = mean, geom = \"point\", shape = 3, size = 4, color = \"red\", show.legend = FALSE) +\n  facet_wrap(~condition) +\n  scale_fill_manual(values = c(\"skyblue\", \"red2\"))  + # Mind. 2 unterschiedliche Farben (schwarz und weiss ausgenommen)\n  theme_minimal(base_size = 14) + # Theme verwenden\n  scale_x_continuous(breaks = seq(0,1, by = 1)) + # Beschriftungen: Achsenbeschriftungen\n  scale_y_continuous(breaks = seq(0,12, by = 1)) +\n  theme(legend.position = \"right\") +\n  labs( # Beschriftungen: Titel, Subtitel\n    title = \"Reaktionszeit nach Bedingung und Korrektheit\",\n    subtitle = \"Wie unterscheiden sich die Verteilungen der Reaktionszeiten zwischen den Bedingungen?\", \n    x = \"0 = Falsche Antworten, 1 = Richtige Antworten\",\n    y = \"Reaktionszeit in Sekunden\",\n    caption = \"\",\n    tag = \"\"\n  )\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\nd_filtered &lt;- d %&gt;%\n    filter((rt &lt; 12) & (rt &gt; 0.1))\n\n#glimpse(d_filtered)\n\nd_filtered_summary &lt;- d_filtered %&gt;%\n    summarise(mean_corr = mean(corr),\n              sd_corr = sd(corr),\n              mean_rt = mean(rt),\n              sd_rt = mean(rt))\n\n#glimpse(d_filtered_summary)\n\np = d_filtered |&gt;\n    ggplot(data = d_filtered,\n           mapping = aes(x = corr,\n                         y = rt)) +\n        geom_boxplot(width = 0.3) +\n        geom_jitter(width = 0.1) +\n        geom_hline(data = d_filtered_summary,\n                   mapping = aes(yintercept = mean_rt),\n                     colour = 'red') +\n        geom_hline(data = d_filtered_summary,\n           mapping = aes(yintercept = mean_rt + sd_rt),\n           linetype = 'dashed',\n           colour = 'red') +\n        geom_hline(data = d_filtered_summary,\n            mapping = aes(yintercept = mean_rt - sd_rt),\n            linetype = 'dashed',\n            colour = 'red') +\n        geom_vline(data = d_filtered_summary,\n            mapping = aes(xintercept = mean_corr),\n            colour = 'blue') +\n        geom_vline(data = d_filtered_summary,\n            mapping = aes(xintercept = mean_corr + sd_corr),\n            linetype = 'dashed',\n            colour = 'blue') +\n        geom_vline(data = d_filtered_summary,\n            mapping = aes(xintercept = mean_corr - sd_corr),\n            linetype = 'dashed',\n            colour = 'blue') +\n        labs(title = \"Random-Dot Experiment Boxplot\",\n             subtitle = \"Abhängigkeit der Genauigkeit von der Entscheidungszeit\",\n             caption = \"Rot: Mittelwert der Entscheidungszeit, Blau: Mittelwert der Genauigkeit, Gestrichelt: Jeweilige Standardabweichung\",\n             y = \"Entscheidungszeit [s]\",\n             x = \"Genauigkeit [0,1]\") +\n        theme_minimal()\np\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv(\"data/dataset_random_dot_clean.csv\") \n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\np = d |&gt;\n  ggplot(aes(x = corr, y = rt, fill = condition)) +\n  geom_boxplot(alpha = 0.4, outlier.shape = NA) +  # Hier wird Boxplot erstellt. Mit Alpha = 0.4 werden die Punkte ahlbtransparent dargestellt. \n                                                  # Outlier.shapte = NA =&gt; Ausreisser werden nicht extra angezeigt.\n  geom_jitter(width = 0.2, alpha = 0.05, color = \"royalblue\") +  # Rohdaten werden hier dargestellt mit Geomjitter\n  stat_summary(fun = mean, geom = \"point\", shape = 18, size = 3, color = \"darkturquoise\") +  # Mittelwert wird als Extra-Punkt eingefügt\n  facet_wrap(~condition) + # erstellt separate Plots für jede Bedingung nebeneinander\n  scale_fill_manual(values = c(\"lightgreen\", \"blueviolet\")) +  # Zwei Farben für Bedingungen\n  labs(\n    title = \"Random-Dot-Test\",\n    subtitle = \"Hängen Reaktionszeit und Genauigkeit von der Instruktionsbedingung ab?\",\n    x = \"0 = Falsch, 1 = Richtig\",\n    y = \"Reaktionszeit (in Sekunden)\",\n    caption = \"\"\n  ) +\n  scale_y_continuous(breaks = seq(0, 12, by = 2)) +\n  scale_x_continuous(breaks = seq(0, 1, by = 1)) +\n  theme_minimal(base_size = 13)\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\nRows: 15,315\nColumns: 8\n$ id        &lt;chr&gt; \"sub-007\", \"sub-007\", \"sub-007\", \"sub-007\", \"sub-007\", \"sub-…\n$ trial     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ direction &lt;chr&gt; \"right\", \"left\", \"left\", \"right\", \"left\", \"right\", \"left\", \"…\n$ condition &lt;chr&gt; \"speed\", \"speed\", \"speed\", \"speed\", \"speed\", \"speed\", \"speed…\n$ corrAns   &lt;chr&gt; \"right\", \"left\", \"left\", \"right\", \"left\", \"right\", \"left\", \"…\n$ resp      &lt;chr&gt; \"right\", \"right\", \"left\", \"right\", \"right\", \"left\", \"left\", …\n$ corr      &lt;dbl&gt; 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, …\n$ rt        &lt;dbl&gt; 0.6537022, 0.5821858, 1.3868371, 0.9550371, 0.4990127, 0.692…\n\n\nRows: 1\nColumns: 2\n$ mean_rt_corr &lt;dbl&gt; 1.943695\n$ sd_rt_corr   &lt;dbl&gt; 1.892218\n\n\n\n\n\n\n\n\n\n\n\n\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\nd_filtered &lt;- d %&gt;%\n    filter((rt &lt; 12) & (rt &gt; 0.1))\n\nglimpse(d_filtered)\n\nd_filtered_corr &lt;- d_filtered %&gt;%\n    filter(corr == 1)\n\nd_filtered_incorr &lt;- d_filtered %&gt;%\n    filter(corr == 0)\n\nd_filtered_summary_corr &lt;- d_filtered_corr %&gt;%\n    summarise(mean_rt_corr = mean(rt),\n              sd_rt_corr = sd(rt))\n\nglimpse(d_filtered_summary_corr)\n\nd_filtered_summary_incorr &lt;- d_filtered_incorr %&gt;%\n    summarise(mean_rt_incorr = mean(rt),\n              sd_rt_incorr = sd(rt))\n\np = d_filtered |&gt;\n    ggplot(data = d_filtered,\n           mapping = aes(x = corr,\n                         y = rt)) +\n        geom_boxplot(width = 0.3) +\n        geom_jitter(width = 0.1) +\n        geom_hline(data = d_filtered_summary_corr,\n                   mapping = aes(yintercept = mean_rt_corr),\n                     colour = 'green') +\n        geom_hline(data = d_filtered_summary_corr,\n           mapping = aes(yintercept = mean_rt_corr + sd_rt_corr),\n           linetype = 'dashed',\n           colour = 'green') +\n        geom_hline(data = d_filtered_summary_corr,\n            mapping = aes(yintercept = mean_rt_corr - sd_rt_corr),\n            linetype = 'dashed',\n            colour = 'green') +\n        geom_hline(data = d_filtered_summary_incorr,\n            mapping = aes(yintercept = mean_rt_incorr),\n            colour = 'red') +\n        geom_hline(data = d_filtered_summary_incorr,\n            mapping = aes(yintercept = mean_rt_incorr + sd_rt_incorr),\n            linetype = 'dashed',\n            colour = 'red') +\n        geom_hline(data = d_filtered_summary_incorr,\n            mapping = aes(yintercept = mean_rt_incorr - sd_rt_incorr),\n            linetype = 'dashed',\n            colour = 'red') +\n        labs(title = \"Abhängigkeit der Korrektheit von der Entscheidungszeit\",\n             subtitle = \"Wie unterscheiden sich die Reaktionszeiten der Korrekten Antworten, von den Inkorrekten?\",\n             caption = \"Grün: Mittelwert der Entscheidungszeit für korrekte Antworten\\nRot: Mittelwert der Entscheidungszeit für inkorrekte Antworten\\nGestrichelt: Jeweilige Standardabweichung\",\n             y = \"Entscheidungszeit [s]\",\n             x = \"Korrektheit [0,1]\") +\n        theme_minimal()\np + scale_x_continuous(breaks=seq(0,1,by=1))\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\np = d |&gt;\n  ggplot(aes(x = corr, y =rt, fill= condition)) +\n  geom_violin(alpha = 0.7, linetype = 7, show.legend = FALSE) + # Zusammenfassendes Mass Violinplot \n  geom_jitter(width = 0.2, alpha = 0.05, color = \"black\", show.legend = FALSE) + # Rohdaten \n  stat_summary(fun = mean, geom = \"point\", shape = 3, size = 4, color = \"red\", show.legend = FALSE) +\n  facet_wrap(~condition) +\n  scale_fill_manual(values = c(\"skyblue\", \"red2\"))  + # Mind. 2 unterschiedliche Farben (schwarz und weiss ausgenommen)\n  theme_minimal(base_size = 14) + # Theme verwenden\n  scale_x_continuous(breaks = seq(0,1, by = 1)) + # Beschriftungen: Achsenbeschriftungen\n  scale_y_continuous(breaks = seq(0,12, by = 1)) +\n  theme(legend.position = \"right\") +\n  labs( # Beschriftungen: Titel, Subtitel\n    title = \"Reaktionszeit nach Bedingung und Korrektheit\",\n    subtitle = \"Wie unterscheiden sich die Verteilungen der Reaktionszeiten zwischen den Bedingungen?\", \n    x = \"0 = Falsche Antworten, 1 = Richtige Antworten\",\n    y = \"Reaktionszeit in Sekunden\",\n    caption = \"\",\n    tag = \"\"\n  )\np",
    "crumbs": [
      "Datenvisualisieren",
      "Plot Gallery - Group 1"
    ]
  },
  {
    "objectID": "plots_group2.html",
    "href": "plots_group2.html",
    "title": "Plot Gallery - Group 2",
    "section": "",
    "text": "PlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n#install.packages(\"ggimage\")\nlibrary(ggimage)\nlibrary(magick)\nlibrary(png)\nlibrary(grid)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(patchwork)\nlibrary(tibble)\n\nd &lt;- d |&gt;\n  mutate(\n    is_accuracy = condition == \"accuracy\",\n    is_speed = condition == \"speed\")\n\nd &lt;- d |&gt;\n  filter(rt &lt; 10)\n\nd_summary &lt;- d |&gt;\n  group_by(condition, corr) |&gt;\n  summarise(avg_rt = mean(rt), .groups = \"drop\") |&gt;\n  mutate(image = ifelse(corr == 1, \"1.png\", \"0.png\"))\n\npp = d |&gt;\n  ggplot(aes(x = condition, y = rt)) +\n  \n  \n# Rohwert Datenpunkte\n  geom_jitter(aes(color = as.factor(corr)),\n              position = position_jitterdodge(jitter.width = 0.7, dodge.width = 1),\n              show.legend = FALSE,\n              alpha = 0.2,\n              size = 1.5) +\n  \n# Violinen mit Mean & SD\n  geom_violin(aes(fill = as.factor(corr)),\n              position = position_dodge(width = 1),\n              draw_quantiles = c(0.25, 0.5, 0.75),\n              alpha = 0.5,\n              width = 0.5,\n              color = alpha(\"#434255\", 0.5))+\n  \n  geom_image(data = d_summary,\n             aes(x = condition, y = avg_rt, image = image),\n             size = 0.05,\n             position = position_dodge(width = 1)) +\n  \n  # Beschriftung Skalen\n  ## Violinen\n  scale_fill_manual(values = c(\"0\" = \"#277B74\", \"1\" = \"#B8696D\"),\n                    labels = c(\"Falsch\", \"Richtig\"),\n                    name = \"\") +\n  \n  ## #f98052orange, #8088D3blau\n  \n  ## Jitter\n  scale_color_manual(values = c(\"0\" = \"#277B74\", \"1\" = \"#B8696D\")) +\n\n  \n  # Captions & Themes\n  labs(title = \"Random Dot Task\",\n       subtitle = \"Wie unterscheiden sich die Conditions in ihren Reaktionszeiten?\",\n       x = \"Condition\",\n       y = \"Reaktionszeit [s]\") +\n  \n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        plot.title = element_text(hjust = 0.5,\n                                  face = \"bold\",\n                                  color = \"#277B74\",\n                                  size = 14),\n        plot.subtitle = element_text(hjust = 0.5,\n                                     size = 11))\n\n# Dummy-Daten für die Bildlegende\nlegend_images &lt;- tibble(x = 1:2,\n                        y = 1,\n                        label = c(\"\", \"\"),\n                        image = c(\"1.png\", \"0.png\"))\n\n# Plot für die Bildlegende\nimage_legend &lt;- ggplot(legend_images, aes(x = x, y = y)) +\n  \n  geom_image(aes(image = image), \n             size = 0.5) +\n  geom_text(aes(label = label), \n            vjust = 3, \n            size = 4) +\n  xlim(0.0000000001, 3.02) + \n  ylim(0.5, 1.3) +\n  ggtitle(\"Durchschnitt RT\") +\n  theme_void() +\n  theme(plot.title = element_text(hjust = 0.5, size = 10))\n\n\nTHE_plot &lt;- pp / image_legend + plot_layout(heights = c(4, 1))\nTHE_plot\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\n##Loading packages \nlibrary(ggimage)\n# install.packages(\"remotes\")\n# remotes::install_github(\"MatthewBJane/ThemePark\")\nlibrary(ThemePark)\nlibrary(magick)\nlibrary(png)\nlibrary(grid)\n\n#Preparing data for plotting\nmean_data &lt;- d |&gt; filter(rt &gt; 0.099 & rt &lt; 12) |&gt;\n    group_by(id, condition) |&gt;\n    summarise(\n        N = n(),\n        ncorrect = sum(corr),\n        accuracy = mean(corr),\n        median_rt = median(rt)) |&gt;\n          ungroup()\n\nmean_data &lt;- mean_data |&gt;\n  group_by(condition) |&gt;\n  mutate(overall_accuracy = mean(accuracy), \n  overall_rt = mean(median_rt))|&gt;\n  ungroup()\n\nplot_data &lt;- mean_data |&gt; \n  mutate(image = \"clownfish.png\",\n         image2 = \"shark.png\")\n\nimg = png::readPNG(\"darla.png\") %&gt;%\n  rasterGrob(interpolate = TRUE) \nimg_1 &lt;- readPNG(\"coral.png\")\n\n#Creating plot\nnemo_plot &lt;- ggplot(data = plot_data, \n       mapping = aes(x = accuracy, y = median_rt))+\n  annotation_custom(rasterGrob(img_1, \n                               width = unit(1,\"npc\"),\n                               height = unit(1,\"npc\")), \n                    -Inf, Inf, -Inf, Inf) + \n    geom_violin(draw_quantiles = c(0.25, 0.5, 0.75), color = \"black\", fill = \"yellow\") +\n           geom_image(aes(image=image), size = 0.06) +\n       facet_wrap(~condition) +\n  geom_image(aes(image = image2, x = overall_accuracy, y = overall_rt), size = 0.22) +\n  labs(title = \"speed-accuracy-trade-off\",  \n         subtitle = \"Wie hängen die Reaktionszeit und die Accuracy zusammen?\", \n        x = \"Accuracy\", \n        y = \"Reaktionszeit\") +\n  coord_cartesian(clip = \"off\") +\n  annotation_custom(img, x = 1.2, y = 13, ymax = 15.8, xmax = 2) +\n      theme_nemo()\nnemo_plot\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n\nRows: 28680 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): id, direction, condition, corrAns, resp\ndbl (3): trial, corr, rt\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\nd_filtered &lt;- d %&gt;% filter(rt &lt;= 120)\np = d |&gt;\n  ggplot(data = d_filtered,\n         mapping = aes(x = condition, y = rt/10)) +\n  # Enhanced boxplots with more harmonious colors\n  geom_boxplot(aes(fill = condition), \n               alpha = 0.7,              \n               outlier.shape = NA, \n               width = 0.5,\n               lwd = 0.8,                \n               fatten = 1.2) +           \n  geom_jitter(aes(color = condition), \n              width = 0.15, \n              alpha = 0.5,               \n              size = 0.7) +              \n  stat_summary(fun = mean, \n               geom = \"point\", \n               shape = 23, \n               size = 4, \n               color = \"black\", \n               fill = \"#FFD700\") +  # More refined gold color for the diamond\n  # Harmonious color palette: complementary blues and purples\n  scale_color_manual(values = c(\"speed\" = \"#3B7A9E\", \"accuracy\" = \"#7A559E\")) +\n  scale_fill_manual(values = c(\"speed\" = \"#5F9AB8\", \"accuracy\" = \"#9C7CB8\")) +\n  labs(title = \"Verteilung der Reaktionszeiten nach Bedingungen\",\n       subtitle = \"Unterscheiden sich die Reaktionszeiten zwischen den beiden Bedingungen?\",\n       x = \"Instruktionsbedingung\",\n       y = \"Reaktionszeit (s)\") +\n  theme_bw() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11, color = \"grey30\"),\n    axis.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    panel.grid.major.x = element_blank(),\n    panel.border = element_rect(color = \"grey70\", size = 1)\n  )\np\n\n\n\n\n\n\n\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\n# Reaktionszeiten filtern (realistische Werte zwischen 0.1 und 8 Sekunden)\nd &lt;- d |&gt; \n  filter(rt &gt; 0.1 & rt &lt; 8)\n\n# Plot erstellen\np = d |&gt;\n  ggplot(aes(x = condition, y = rt, color = condition)) +\n  geom_jitter(width = 0.2, alpha = 0.4, size = 2) +\n  geom_boxplot(width = 0.4, alpha = 0, color = \"black\", outlier.shape = NA) +\n  stat_summary(fun = mean, geom = \"point\", shape = 21, size = 3, fill = \"gold\", color = \"black\") +\n  scale_color_manual(values = c(\"darkred\", \"darkgreen\")) +\n  labs(\n    title = \"Reaktionszeiten unter Speed- vs. Accuracy-Instruktion\",\n    subtitle = \"Beeinflusst die Instruktion die mittlere Reaktionszeit?\",\n    x = \"Instruktion\",\n    y = \"Reaktionszeit (Sekunden)\",\n    color = \"Instruktion\"\n  ) +\n  theme_minimal(base_size = 14)\n\n# Plot anzeigen\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\nd &lt;- read.csv(\"data/dataset_random_dot_clean.csv\") %&gt;%\n    mutate(resp = as.factor(resp))\n\n\n\n\np &lt;- d %&gt;%\n    ggplot(aes(x = resp, y = rt, fill = resp)) +\n    geom_violin(trim = FALSE, alpha = 0.5, color = NA) +  # Verteilung\n    geom_jitter(aes(color = resp), width = 0.15, alpha = 0.3, size = 1.5) +  # Rohdatenpunkte\n    stat_summary(fun = mean, geom = \"point\", shape = 21, size = 3.5, fill = \"white\", color = \"black\", stroke = 1) +  # Mittelwert\n    stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.1, color = \"black\") +  # Standardfehler\n    labs(\n        title = \"Reaktionszeiten in Abhängigkeit der Antwort\",\n        subtitle = \"Beeinflusst die gegebene Antwort (linke vs. rechte Taste) die Reaktionszeit?\",\n        x = \"Antwort (Taste)\",\n        y = \"Reaktionszeit (ms)\",\n        color = \"Antwort\",\n        fill = \"Antwort\"\n    ) +\n    scale_fill_manual(values = c(\"#9E5E9E\", \"#39BEBE\")) +  # Neue Füllfarben\n    scale_color_manual(values = c(\"#C28CCC\", \"#7FDAD7\")) +  # Neue Punktfarben\n    theme_minimal(base_size = 12) +\n    coord_cartesian(ylim = c(0, 0300))  # Ausreißer optisch begrenzen\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Paketen laden\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Mittelwerte pro VPN berechnen\nd1 &lt;- d %&gt;%\n  group_by(id) %&gt;%\n  summarise(mean_corr = mean(corr),\n            mean_rt = mean(rt)) %&gt;%\n  filter(mean_rt &lt; 12)\n\n#richtiger Datensatz mit Mittelwerten\nd2 &lt;- d %&gt;%\n  group_by(id) %&gt;%\n  mutate(mean_corr = mean(corr, na.rm = TRUE),\n         mean_rt = mean(rt, na.rm = TRUE)) %&gt;%\n  filter(mean_rt &lt; 12)\n\n#Plots\n\np &lt;- ggplot(d2, aes(x = condition, y = rt, color = condition)) +                      #Struktur der grafische Darstellung\n  geom_jitter(width = 0.2, alpha = 0.6, size = 1.5) +                                 #Rohdaten\n  geom_boxplot(aes(fill = condition), alpha = 0.3, color = \"black\") +                 #zusammenfassendes Mass\n  scale_color_manual(values = c(\"violetred\", \"olivedrab3\")) +                         #Farben für Rohdaten\n  scale_fill_manual(values = c(\"violetred\", \"olivedrab3\")) +                          #Farben für zusammenfassendes Mass  \n  labs( title = \"Reaktionszeit pro experimenteller Bedingung\",                        #Beschriftungen\n        subtitle = \"Wie verändert sich die Reaktionszeit je nach Versuchsbedingung?\",\n        x = \"Bedingung\",\n        y = \"Reaktionszeit (s)\",\n        caption = \"Random Dot Experiment\") +\n  theme_light() +                                                                     #Theme\n  facet_wrap(~ direction)                                                             #Optional: Facets\n\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv(\"data/dataset_random_dot_clean.csv\")\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\nd_clean &lt;- d |&gt; \n  filter(rt &lt; 3)\n\nd$rt = d$rt / 1000\n\np = d_clean |&gt;\n  ggplot(aes(x = condition, y = rt, color = condition)) +  \n  geom_jitter(alpha = 0.5, width = 0.2) +  \n  geom_violin(alpha = 0.3, aes(fill = condition), show.legend = FALSE) +  \n  geom_boxplot(width = 0.2, outlier.shape = NA, color = \"black\") +  \n  labs(\n    title = \"Reaktionszeiten in verschiedenen Bedingungen\",\n    subtitle = \"Gibt es Unterschiede in den Reaktionszeiten zwischen den Bedingungen?\",\n    x = \"Bedingung\",\n    y = \"Reaktionszeit (s)\",\n    caption = \"Datenquelle: dataset_random_dot_clean_Initialen.csv\"\n  ) +\n  theme_minimal() +  \n  scale_color_manual(values = c(\"blue\", \"red\", \"green\")) +  \n  scale_fill_manual(values = c(\"blue\", \"red\", \"green\")) +\n  scale_y_continuous(breaks = seq(0, 3, by = 0.5), limits = c(0, 3))\np\nprint(p)\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n\np &lt;- d |&gt;\n  ggplot(aes(x = condition, y = rt, color = condition)) +\n  geom_jitter(alpha = 0.3, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", shape = 18, size = 4, color = \"black\") +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.15, color = \"black\") +\n  labs(\n    title = \"Reaktionszeiten in Abhängigkeit der Instruktion\",\n    subtitle = \"Beeinflusst die Aufgabeninstruktion (Schnelligkeit vs. Genauigkeit) die Reaktionszeit?\",\n    x = \"Instruktion\",\n    y = \"Reaktionszeit (s)\",\n    color = \"Instruktion\"\n  ) +\n  theme_minimal() +\n  ylim(0.1, 12)  \n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n[1] 51.2217\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\n#mittelwert reaktionszeit\nmean_rt &lt;- mean(d$rt, na.rm = TRUE)\nmean_rt\n\n#ausreisser filtern\n# zu schnelle und zu langsame Antworten ausschliessen\nd &lt;- d |&gt;\n  filter(rt &gt; 0.099 & rt &lt; 10)\n\n\n#code für GGPLOT\np = d |&gt;\n  ggplot(aes(x = condition, y = rt, color = condition)) +\n  geom_jitter(width = 0.2, alpha = 0.4, size = 2) +\n  geom_boxplot(width= 0.4, alpha= 0, color=\"black\") +\n  scale_color_manual(values = c(\"darkorange\", \"steelblue\")) +\n  labs(\n    title = \"Reaktionszeiten unter Speed- vs. Accuracy-Instruktion\",\n    subtitle = \"Beeinflusst die Instruktion die Reaktionszeit?\",\n    x = \"Instruktion\",\n    y = \"Reaktionszeit (Sekunden)\",\n    color = \"Instruktion\") +\n  theme_minimal(base_size = 14)\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\nlibrary(\"ggpubr\")\n\np = d |&gt;\n  filter(rt &gt; 0.099 & rt &lt; 12) |&gt;\n    ggplot(d,\n           mapping = aes(x = condition, y = rt, color = condition, fill = condition)) +\n  geom_jitter(width = 0.1, alpha = 0.2, size = 1.2) +\n  geom_boxplot (alpha = 0.4, width = 0.5, outlier.shape = NA) +\n  labs(\n    title = \"Reaktionszeiten in Speed- vs. Accuracy-Bedingung\",\n    subtitle = \"Führt die Instruktion 'Speed' zu kürzeren Reaktionszeiten als 'Accuracy'?\",\n    x = \"Instruktionsbedingung\",\n    y = \"Reaktionszeit (Sekunden)\",\n    caption = \"Daten aus dem Random-Dot-Paradigma. Punkte = einzelne Trials.\"\n  ) +\n  scale_color_manual(values = c(\"speed\" = \"#E63946\", \"accuracy\" = \"#1D3557\")) +\n  scale_fill_manual(values = c(\"speed\" = \"#E63946\", \"accuracy\" = \"#1D3557\")) +\n  theme_pubclean()\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n##Bei Unklarheiten wurde für diese Abgabe zur Unterstützung Chatgpt verwendet.\nmeans &lt;- d |&gt;\n  group_by(condition, trial) |&gt;\n  summarise(mean_acc = mean(corr),sd_acc = sd(corr),\n            .groups = \"drop\"\n  )\n\n# Schritt 2: Plot bauen (Rohdaten + Mittelwertlinie)\np &lt;- ggplot(d, aes(x = trial, y = corr, color = condition)) +\n  geom_jitter(width = 0.05, height = 0.05, alpha = 0.05, size = 0.05) +  # Rohdaten\n  \n  geom_ribbon(data = means, aes(x = trial, ymin = mean_acc - sd_acc, ymax = mean_acc + sd_acc, fill = condition),\n              alpha = 0.2, color = NA, inherit.aes = FALSE) +\n  \n  geom_line(data = means, aes(x = trial, y = mean_acc, color = condition), linewidth = 0.05) +  # Mittelwert\n  geom_point(data = means, aes(x = trial, y = mean_acc, color = condition), size = 0.5) +\n  \n  scale_color_manual(values = c(\"darkorange\", \"royalblue\")) +\n  scale_fill_manual(values = c(\"darkorange\", \"royalblue\")) +\n  \n  labs(\n    title = \"Genauigkeitsverlauf im Dot-Experiment\",\n    subtitle = \"Wie verändert sich die Genauigkeit \n    über die Durchgänge hinweg?\",\n    x = \"Durchgang (Trial)\",\n    y = \"Antwort korrekt (0 = falsch, 1 = richtig)\",\n    caption = \"Datenquelle: dataset_random_dot_clean.csv\"\n  ) +\n  facet_wrap(~ condition) +\n  theme_minimal(base_size = 12)\n\n# Plot anzeigen\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\ndfilt &lt;- d |&gt;\n  filter(rt &gt; 0.099 & rt &lt; 10)\n\np = dfilt |&gt;\n  ggplot(aes(x= condition, y= rt, color = condition)) +\n  geom_jitter(size = 3, alpha = 0.4, \n              width = 0.2, height = 0) +\n  geom_violin(alpha = 0, width = 1, color = \"black\") +\n  scale_color_manual(values = c(accuracy = \"skyblue3\",\n                                speed = \"tomato3\")) +\n  labs(x = \"Bedingung\",\n       y = \"Reaktionszeit [s]\",\n       title = \"Reaktionszeiten (RZn)\",\n       subtitle = \"Unterscheiden sich die RZn der beiden Bedingungen?\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\np = d |&gt;\n  ggplot(mapping = aes(x = condition,\n                       y = rt )) + \n         geom_jitter(width = 0.3, alpha = 0.6, color = \"green\") + \n  geom_violin(width = 0.3, alpha = 0.5, color = \"blue\") +\n  labs(title = \"Eine schöne Zusammenfassung\",\n       subtitle  = \"Wie verhält sich die Reaktionszeit zur Bedingung\",\n       x = \"Bedingung\",\n       y = \"Reaktionszeit\") +\n  theme_dark()\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\nmean_values &lt;- d |&gt; \n  filter(!is.na(rt), rt &gt; 0.1, rt &lt; 12) |&gt; \n  group_by(condition) |&gt; \n  summarise(mean_rt = mean(rt))\nmean_values$condition &lt;- factor(mean_values$condition, levels = c(\"accuracy\", \"speed\"))\n\n\np = d |&gt; \n  filter(!is.na(rt), rt &gt; 0.1, rt &lt; 12) |&gt; \n  ggplot(aes(y = rt, x = condition, fill = condition)) + \n  geom_violin(alpha = 0.5) + \n  geom_boxplot(alpha = 0.2, width = 0.4, outlier.shape = NA,) +\n  geom_point(data = mean_values, aes(x = condition, y = mean_rt), \n             size = 3) +  \n  geom_line(data = mean_values, aes(x = as.numeric(condition), y = mean_rt, group = 1), \n            size = 1, linetype = \"dashed\") +\n  labs(title = \"Reaktionszeiten im Random-Dot-Experiment\",\n       subtitle = \"Unterscheiden sich die durchschnittlichen Reaktionszeiten zwischen den Bedingungen?\",\n       x = \"Bedingung\",\n       y = \"Reaktionszeit (s)\",\n       color = \"Bedingung\") + \n  scale_fill_manual(values = c(\"blue\", \"red\")) +  \n  scale_color_manual(values = c(\"blue\", \"red\")) + \n  theme_minimal() +\n  coord_cartesian(ylim = c(0, 12)) + \n  theme(legend.position = \"none\")\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd &lt;- read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\n#Frage: is the accuracy of direction sig. lower in the speed condition?\n\nd &lt;- d %&gt;%\n  filter(rt &gt;= 100, rt &lt;= 12000)\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Beispiel: Gruppiere nach Bedingung\ndf_summary &lt;- d %&gt;%\n  group_by(condition) %&gt;%\n  summarise(\n    mean_speed = mean(rt),\n    sd_speed = sd(rt)\n  )\n\nggplot(d, aes(x = condition, y = rt, color = condition)) +\n  # Rohdaten\n  geom_jitter(width = 0.2, alpha = 0.3) +\n  \n  # Mittelwert + SD\n  stat_summary(fun = mean, geom = \"point\", size = 3, color = \"red\") +\n  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), \n               geom = \"errorbar\", width = 0.2, color = \"red\") +\n  \n  # Farben & Theme\n  scale_color_brewer(palette = \"Set2\") +\n  theme_minimal() +\n  \n  # Beschriftungen\n  labs(\n    title = \"Reaktionszeiten pro Bedingung\",\n    subtitle = \"Unterscheiden sich die Reaktionszeiten je nach Bedingung?\",\n    x = \"Bedingung\",\n    y = \"Reaktionszeit (ms)\",\n    color = \"Bedingung\",\n    caption = \"Rohdaten + Mittelwerte mit Standardabweichung\"\n  )\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 252 × 5\n# Groups:   id [126]\n   id      condition trial ncorrect mean_rt\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1 sub-007 accuracy     60       41   1.23 \n 2 sub-007 speed        60       40   0.942\n 3 sub-010 accuracy     60       60   1.01 \n 4 sub-010 speed        60       60   0.636\n 5 sub-011 accuracy     60       55   1.28 \n 6 sub-011 speed        60       54   0.535\n 7 sub-012 accuracy     60       58   0.611\n 8 sub-012 speed        60       59   0.650\n 9 sub-014 accuracy     60       43   2.60 \n10 sub-014 speed        59       22   3.01 \n# ℹ 242 more rows\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\np &lt;- d |&gt;\n    filter(rt &gt; 0.099 & rt &lt; 8)\n\np &lt;- p|&gt;\n    group_by(id, condition) |&gt;\n    summarise(\n        trial = n(),\n        ncorrect = sum(corr),\n        mean_rt = mean(rt)\n)\n\np &lt;- ggplot(aes(x = condition, y = mean_rt, color = condition))+\n    geom_jitter(size = 3, alpha = 0.4,\n                width = 0.2, height = 0) +\n    geom_boxplot(width = 0.3, alpha = 0.05, color = \"black\") +\n    scale_color_manual(values = c(congruent = \"skyblue3\",\n                                  incongruent = \"tomato3\")) +\n    labs(x = \"id\",\n         y = \"Mittlere Reaktionszeit\",\n         title = \"Reaktionszeiten\",\n         subtitle = \"War die Reaktionszeit schneller bei falschen antworten\") +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\")\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\nd_filter &lt;- d %&gt;%\n  filter(rt &lt;= 120)\n\nsp = d |&gt;\n  ggplot(data = d_filter, mapping = aes(x = condition, y =rt)) +\n  geom_violin(alpha = 0.9, fill = \"skyblue\", trim = TRUE) + \n  geom_jitter(width = 0.2, alpha = 0.2, size = 0.2) +\n  stat_summary(fun = \"mean\", geom = \"point\", shape = 23, size = 3, color = \"blue\", fill = \"violet\") +\n  labs(title = \"Übung 3\", \n       subtitle = \"Verteilung der Reaktionszeit nach Bedingung\",\n       x = \"Bedingung\",\n       y = \"Reaktionszeit\") +\n  theme_minimal()\n\nsp\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\n#install.packages(\"ggpubr\")\n#library(ggpubr)\n\nsummary_d &lt;- d |&gt;\n  group_by(condition,id) |&gt;\n  summarise(mean_acc= mean(corr))\n\n\n#t.test(mean_acc ~ condition, data = summary_d, paired = TRUE)\n\n\n\np= summary_d |&gt;\n  ggplot(mapping = aes(x = condition,y = mean_acc, fill= condition)) +\n  geom_boxplot(alpha = 0.5, outlier.shape = NA, width = 0.4) +\n  geom_jitter(width = 0.01, size = 0.4, alpha = 0.3) +\n  stat_summary(fun = mean, geom = \"crossbar\", color = \"darkred\", size = 0.5)+\n  labs(\n    title = \"Random-Dot Paradigma\",\n    subtitle = \"Gab es signifikant mehr richtige Antworten in der \\nAccuracy-Bedingung als in der Speed-Bedingung? \",\n    x = \"Bedingung\",\n    y = \"Korrekte Antworten\"\n  ) +\n  scale_fill_manual(values = c(\"speed\" = \"steelblue\", \"accuracy\" = \"tomato\"))+\n  theme_minimal() +\n  stat_compare_means(\n    comparisons = list(c(\"speed\", \"accuracy\")),\n    method = \"t.test\",\n    paired = TRUE,\n    label = \"p.signif\",  \n    label.y = 0.7       \n  )\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\nd &lt;- d |&gt;\n    filter(rt &gt; 0.1 & rt &lt; 12)\n\np1  = d |&gt; \n  group_by(condition) |&gt; \n  summarise(accuracy = mean(corr, na.rm = TRUE)) |&gt; \n  ggplot(aes(x = condition, y = accuracy, fill = condition)) +\n  geom_col() +\n  labs(title = \"Trefferquote pro Bedingung\",\n       subtitle = \"Wie hoch ist die durchschnittliche Trefferquote pro Bedingung?\",\n       x = \"Bedingung\", y = \"Trefferquote\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"#FF5733\", \"#3498DB\"))  # Orange & Blau\n\n\n## 2. Reaktionszeit pro Bedingung\np2 = d |&gt; \n  ggplot(aes(x = condition, y = rt, fill = condition)) +\n  geom_boxplot(alpha = 0.5) +\n  geom_jitter(width = 0.2, aes(color = condition), alpha = 0.5) +\n  labs(title = \"Reaktionszeit pro Bedingung\",\n       subtitle = \"Wie unterscheiden sich die Reaktionszeiten pro Bedingung?\",\n       x = \"Bedingung\", y = \"Reaktionszeit (s)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"#FF5733\", \"#3498DB\")) +\n  scale_color_manual(values = c(\"#FF5733\", \"#3498DB\"))\n\n\n#Reaktionszeit über Trials\np3 = d |&gt; \n  ggplot(aes(x = trial, y = rt, group = id, color = condition)) +\n  geom_line(alpha = 0.5) +\n  geom_point(alpha = 0.5) +\n  labs(title = \"Reaktionszeit über Trials\",\n       subtitle = \"Verändert sich die Reaktionszeit im Verlauf des Experiments?\",\n       x = \"Trial\", y = \"Reaktionszeit (s)\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"#FF5733\", \"#3498DB\"))\n\n\n# Plots kombinieren mit patchwork\nlibrary(patchwork)\n\np = p1 / p2 / p3\n\n# Endgültigen Plot anzeigen\nprint(p)\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\n# Metbrewer hat colour palettes, die von berühmten Kunstwerken inspiriert sind,\n# Und ich finde sie sehr hübsch :)\n#install.packages(\"MetBrewer\")\nlibrary(MetBrewer)\n\n# violin plot, Ausreisser in der Reaktionszeit werden herausgefiltert\np = d |&gt;\n    filter(rt&lt;12) |&gt;\n    ggplot(aes(x = direction, y = rt, fill = direction)) +\n    geom_violin(width = 1) +\n    geom_boxplot(width=0.1, alpha=0.4) +\n    geom_jitter(color=\"royalblue4\", size=0.4, alpha=0.2) +\n    labs(title = \"Reaktionszeiten nach Richtung\",\n         subtitle = \"Sind Menschen schneller in eine bestimmte Richtung?\",\n         x = \"Richtung\",\n         y = \"Reaktionszeiten (s)\") +\n    theme_bw() +\n    # Klimt ist eine der Farbpaletten, und direction gibt an, welche Farben gewählt werden\n    scale_fill_manual(values=met.brewer(\"Klimt\", direction = -1))\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\nd_filter &lt;- d %&gt;%\n  filter(rt &lt;= 120)\n\nsp = d |&gt;\n  ggplot(data = d_filter, mapping = aes(x = condition, y =rt)) +\n  geom_violin(alpha = 0.9, fill = \"skyblue\", trim = TRUE) + \n  geom_jitter(width = 0.2, alpha = 0.2, size = 0.2) +\n  stat_summary(fun = \"mean\", geom = \"point\", shape = 23, size = 3, color = \"blue\", fill = \"violet\") +\n  labs(title = \"Übung 3\", \n       subtitle = \"Verteilung der Reaktionszeit nach Bedingung\",\n       x = \"Bedingung\",\n       y = \"Reaktionszeit\") +\n  theme_minimal()\n\nsp\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Ein Plot erstellen für die Fragestellung: Ist die Reaktionszeit in der Speed-Bedingung kürzer?\n\nd &lt;- d %&gt;% filter(rt &lt;=12)|&gt; # nur Reaktionszeiten unter 12ms \nggplot(data = d,\n       mapping = aes(x = condition,\n                     y = rt)) +\n  geom_jitter(width = 0.4, color= \"#009ACD\", alpha = 0.3) + \n  geom_boxplot(width = 0.2, color= \"#FF34B3\", alpha = 0.5) + # Boxplot-Farbe und transparenz, Boxplot als 2. Aufgelistet, dass es über den blauen Rohdatenpunkten erscheint und transparent, dass Rohdaten sichtbar bleiben.\n  labs(title = \"Vergleich der Reaktionszeiten zwischen den beiden Bedingungen\", \n       subtitle = \"Ist die Reaktionszeit in der Speed-Bedingung kürzer?\",\n       x = \"Bedingung\",\n       y = \"Reaktionszeit (ms)\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5), # Den Titel und Subtitel mittig ausrichten\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n# Zahlen zum Boxplot: hier können die Werte vom Mean und der SD ausgegeben werden\nunique(d$condition)\n# Mean und AD für die Bedingung speed berechnen\nd_speed &lt;- d %&gt;% \n  filter(condition == \"speed\")\n\nd_speed_summary &lt;- d_speed %&gt;% \n  summarise(mean_rt = mean(rt),\n            sd_value = sd(rt))\nglimpse(d_speed_summary)\n\n# Mean und SD für die Bedingung accuracy berechenen\nd_accuracy &lt;- d %&gt;% \n  filter(condition == \"accuracy\")\n\nd_accuracy_summary &lt;- d_accuracy %&gt;% \n  summarise(mean_rt = mean(rt),\n                  sd_value = sd(rt))\nglimpse(d_accuracy_summary)\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv')\n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\np = d |&gt;\n    ggplot(aes(x = condition, y = rt, color = condition)) + #Rohdaten nach Bed. gruppiert\n    geom_jitter(alpha = 0.4, width = 0.5, size = 1) +\n    geom_violin(alpha = 0.4, width = 0.2, color = \"black\") + #Zusammenfassendes Mass\n    scale_color_manual(values = c(accuracy = \"deeppink\",\n                                  speed = \"darkcyan\")) +\n    labs(\n        x = \"Instruction Condition\",\n        y = \"Reaction Time (ms)\",\n        title = \"Impact of Instruction Type on Reaction Speed\",\n        subtitle = \"Does Emphasis on Accuracy vs. Speed Affect Reaction Times?\") +\n    theme_linedraw(base_size = 12) +\n    theme(legend.position = \"bottom\")\n\np\n\n\n\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code innerhalb der folgenden 2 Linien darf nicht verändert werden\n# ---------------------------------------------------------------------\nlibrary(tidyverse)\nd = read_csv('data/dataset_random_dot_clean.csv') \n# ---------------------------------------------------------------------\n\n# Beginnen Sie hier mit Ihrem Code:\n\npreplot &lt;- d |&gt;\n  group_by(condition) %&gt;%\n  summarise(\n    Reaktionszeit = mean(rt, na.rm = TRUE),\n    sd_rt = sd(rt, na.rm = TRUE)\n  )\n\np = ggplot(d, aes(x = condition, y = rt, fill = condition)) +\n  geom_jitter(width = 0.2, alpha = 0.2, color = \"gray70\") + # Rohdaten\n  geom_boxplot(alpha = 0.4, outlier.shape = NA, color = \"gray10\") +  # Boxplot ohne Ausreißer  \n  geom_point(data = preplot, aes(x = condition, y = Reaktionszeit), \n             shape = 18, size = 4, color = \"darkblue\", inherit.aes = FALSE) +  # Mittelwert,\n  scale_fill_manual(values = c(\"speed\" = \"#E69F00\", \"accuracy\" = \"#56B4E9\")) +\n  labs(\n    title = \"Reaktionszeiten im Random dot Experiment\",\n    subtitle = \"Unterscheidet sich die Reaktionszeit je nach Instruktion (Speed vs. Accuracy)?\",\n    x = \"Instruktionsbedingung\",\n    y = \"Reaktionszeit (in ms)\",\n    caption = \"Die Quadrate bezeichnen den Mittelwert in der jeweiligen Bedingung\"\n    )  +\n  ylim(0,70) +\n  theme_minimal()\np",
    "crumbs": [
      "Datenvisualisieren",
      "Plot Gallery - Group 2"
    ]
  },
  {
    "objectID": "intro_analysis.html",
    "href": "intro_analysis.html",
    "title": "13  Datengenerierende Prozesse",
    "section": "",
    "text": "13.1 Herausforderungen in der Analyse von neurowissenschaftlichen Daten\nNeurowissenschaftliche Datensätze bringen oft folgende Herausforderungen in der Datenanalyse mit sich:\nZiel ist es, trotz diesen Umständen, möglichst viel Information aus den vorhandenen Daten zu gewinnen. Hierbei spielt die Analysemethode eine wichtige Rolle.",
    "crumbs": [
      "Datenanalyse",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Datengenerierende Prozesse</span>"
    ]
  },
  {
    "objectID": "intro_analysis.html#herausforderungen-in-der-analyse-von-neurowissenschaftlichen-daten",
    "href": "intro_analysis.html#herausforderungen-in-der-analyse-von-neurowissenschaftlichen-daten",
    "title": "13  Datengenerierende Prozesse",
    "section": "",
    "text": "Kleine Stichprobengrössen (z.B. aufgrund teurer Datenerhebung oder Patientengruppen die schwieriger zu rekrutieren sind).\nHeterogenität / Rauschen (z.B. weil der zu untersuchende Prozess schwierig zu isolieren ist, weil Personen sich sehr unterschiedlich verhalten)\nTeure Datenerhebung und damit hoher Druck Resultate zu generieren sowie oft keine Möglichkeit das Experiment zu wiederholen (wichtig daher die gute Planung der Analyse sowie Vermeidung von inkonklusive Resultaten)\nVorgehen bei nicht-signifikanten/nicht-konklusiven Ergebnissen (Research waste, publication bias/file drawer effect)\n\n\nIm Artikel Power failure: why small sample size undermines the reliability of neuroscience von Button et al. 2013 finden Sie einen Artikel über die Problematik von kleinen Stichprobengrössen in Neuroscience\n\n\n\nAbsence of evidence oder Evidence of absence?\nBei Nullhypothesen-Signifikanztests (NHST) wird eine binäre Entscheidung getroffen: Der Hypothesentest kann entweder ein signifikantes oder ein nicht signifikantes Ergebnis haben. Kann kein Effekt gefunden werden besteht die Notwendigkeit zu unterscheiden zwischen den zwei Möglichkeiten: - Absence of evidence: Es ist unklar ob es einen Effekt gibt oder nicht. Die Ergebnisse des Verfahrens sind inkonklusiv. - Evidence of absence: Es ist klar, dass es keinen Effekt gibt. Die Daten zeigen dies deutlich.\nZum Unterscheiden dieser zwei Fälle eignen sich die typischen NHSTs oft weniger, gerade wenn die Power nicht sehr hoch war. Bayesianische Statistik (z.B. bei begrenzten Datensätzen) sowie frequentistische Äquivalenztests (zwei entgegengesetzte NHSTs zum Testen von Nullunterschieden) sind Ansätze, um zwischen absence of evidence und evidence of absence zu unterscheiden.\nWir werden uns in den folgenden Veranstaltungen deshalb damit auseinandersetzen,\n\nwelche Annahmen hinter statistischen Verfahren stecken.\nwelche Fragen mit Bayesianischer Statistik beantwortet werden können.\nwie Nullunterschiede statistisch getestet werden können.",
    "crumbs": [
      "Datenanalyse",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Datengenerierende Prozesse</span>"
    ]
  },
  {
    "objectID": "intro_analysis.html#vorbereitung",
    "href": "intro_analysis.html#vorbereitung",
    "title": "13  Datengenerierende Prozesse",
    "section": "13.2 Vorbereitung",
    "text": "13.2 Vorbereitung\n\n\n\n\n\n\nHands-on: Reaktivierung Statistikwissen\n\n\n\n1. Besprechen Sie in kleinen Gruppen folgende Fragen:\n\nWas ist eine Null-, was eine Alternativhypothese?\nWas bedeutet die Distanz zwischen den beiden Mittelwerten?\nWas ist statistische Power?\nWelche Rolle spielt die Stichprobengrösse?\nWas ist ein p-Wert?\nWas sind Typ I und Typ II Fehler?\nWelche Fragen können Sie mit einem Nullhypothesen- Signifikanztest (NHST) beantworten?\n\n2. Können Sie die Begrifflichkeiten in dieser Grafik einordnen?\n\n\nÜberlegen Sie sich, was Null- und Alternativhypothese in unseren beiden Kursexperimenten (Stroop und Random Dot) sein können.\n\n[10 Minuten]\n\n\n\nSie können zur Beantwortung dieser Fragen z.B. die Interaktive Visualisierung “Understanding Statistical Power and Significance Testing” nutzen.\n\n\n\n\n\n\n\nProjekt und Daten herunterladen\n\n\n\nHier finden Sie die Daten zum herunterladen.\nLesen Sie anschliessend die Daten ein:\n\n## Daten einlesen\nlibrary(tidyverse)\nd_stroop &lt;- read_csv(\"data/dataset_stroop_clean.csv\") |&gt;\n    mutate(across(where(is.character), as.factor)) |&gt; # zu Faktoren machen\n    filter(rt &lt; 4 & rt &gt;= 0.1) |&gt; # nur Antworten zwischen 100 und 4000ms einbeziehen\n    filter(corr == 1) |&gt; # nur korrekte Antworten einbeziehen\n    na.omit() # Messungen mit missings weglassen",
    "crumbs": [
      "Datenanalyse",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Datengenerierende Prozesse</span>"
    ]
  },
  {
    "objectID": "intro_analysis.html#datengenerierende-prozesse",
    "href": "intro_analysis.html#datengenerierende-prozesse",
    "title": "13  Datengenerierende Prozesse",
    "section": "13.3 Datengenerierende Prozesse",
    "text": "13.3 Datengenerierende Prozesse\nNach dem Data Cleaning und Preprocessing geht es darum, welche Informationen die Daten über den zu untersuchenden Prozess beinhalten. Anhand der Daten sollen also Rückschlüsse auf den datengenerierenden Prozess, der zu diesen Daten geführt hat gezogen werden.\nBei jeder Datenanalyse müssen zahlreiche Annahmen getroffen werden. Um diese explizit zu machen und auch die Datenanalyse zu planen, hilft oft eine grafische Darstellung. Directed Acyclic Graphs (DAGs) sind eine Variante hierfür.\n\n13.3.1 Directed Acyclic Graphs (DAGs)\nEin DAG (directed acyclic graph) eignet sich für die Darstellung komplexer Zusammenhänge in Daten und Prozessen. Mit einem DAG kann veranschaulicht werden, welche Variablen einander beeinflussen. Die Kreise (nodes) werden für einzelne Elemente verwendet und die Pfeile (arrows oder edges) beschreiben die Beziehung zwischen den Elementen. Die Darstellung beschreibt einen Prozess also mit gerichteten (directed) und nicht zyklischen (acyclic) Beziehungen.\nWir können beispielsweise annehmen, dass die Farbe-Wort-Kongruenz im Stroop Task beeinflusst, wie schnell die Aufgabe gelöst werden kann.\nEin DAG kann mit folgenden Schritten erstellt werden:\n\n1. Beobachtete Variable bestimmen\nDie beobachtete Variable nennen wir hier \\(y\\). Der Kreis ist grau eingefärbt, weil die Werte in dieser Variable gemessen wurden bzw. bekannt sind.\nIn unserem Beispiel haben wir die Reaktionszeit gemessen. Im Datensatz enthält die Variable rt die Information, wie schnell eine Person in jedem Trial geantwortet hat.\n\n\n2. Verteilung bestimmen\nEs muss festgelegt werden, welche Verteilung die Daten \\(y\\) am besten beschreibt. Eine Verteilung ist immer nur eine Annäherung. Die gemessenen Daten entsprechen dieser Annahme eigentlich nie perfekt. Es geht darum eine Verteilung zu finden die gut genug zu den Daten passt. Jede Verteilung hat Parameter, die geschätzt werden können. Es gibt Verteilungen, welche durch einen Parameter definiert werden, andere brauchen mehrere Parameter.\nEine sehr häufig verwendete Verteilung in statistischen Analysen ist die Normalverteilung. Die Annahme einer Normalverteilung ermöglicht es uns, mit nur 2 Parametern die Daten in der Variable zu beschreiben: Dem Mittelwert (\\(\\mu\\)) und der Standardabweichung (\\(\\sigma\\)). Natürlich ist das nur eine Annäherung, aber meistens eine genügend Gute!\n\nHier im Distribution Zoo werden Verteilungen, zugrundeliegende Daten sowie Code und Formeln zusammengefasst.\n\n\nUm die Verteilung unserer Datenpunkte zu bestimmen bzw. zu überprüfen können die Daten in R geplottet werden, z.B. mit geom_histogram(). Das Argument binwidth = bestimmt, wie breit ein Balken wird (hier 50 ms).\n\nd_stroop |&gt;\n    ggplot(aes(x = rt)) +\n    geom_histogram(colour=\"black\", fill = \"white\", \n                   binwidth = 0.05, \n                   alpha = 0.5) +\n    theme_minimal()\n\n\n\n\n\n\n\n\nDiese Verteilung könnte beispielsweise mit einer Normalverteilung beschrieben werden. Der Mittelwert und die Standardabweichung können wir mit R berechnen:\n\n# clean dataset first\nmu = mean(d_stroop$rt)\nmu\n\n[1] 0.7465929\n\nsigma = sd(d_stroop$rt)\nsigma\n\n[1] 0.3455337\n\n\nUm zu schauen, wie gut diese Normalverteilung mit den Parametern \\(\\mu\\) = 0.7465929 und \\(\\sigma\\) = 0.3455337 unsere Daten beschreibt, können wir die Daten und simulierte Daten mit der angenommenenen Verteilung übereinander plotten:\n\nd_stroop |&gt;\n    ggplot(aes(x = rt)) +\n    geom_histogram(colour=\"black\", fill = \"white\", \n                   binwidth = 0.05, \n                   alpha = 0.5) +\n    geom_histogram(aes(x = rnorm(1:length(rt), mu, sigma)),\n                   binwidth = 0.05,\n                   alpha = 0.2) +\n    theme_minimal()\n\n\n\n\n\n\n\n\nWir können auch density-Plots dafür nutzen:\n\nd_stroop |&gt;\n    ggplot(aes(x = rt)) +\n    geom_density(colour=\"black\", fill = \"white\") +\n    geom_density(aes(x = rnorm(1:length(rt), mu, sigma)),\n                 fill=\"grey\",\n                 alpha = 0.2) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on: Verteilungen\n\n\n\n\nWelche Daten stammen aus unseren Daten, welche entsprechen der Normalverteilung \\(N(0.747, 0.346)\\) ?\nWie gut passt die Annahme der Normalverteilung für unsere Reaktionszeitdaten? Wo passt sie gut? Wo nicht?\nFinden Sie auf Distribution Zoo eine passendere Verteilung?\nPrüfen Sie Ihre Verteilung, indem Sie unten an den obigen Plot diese Verteilung mit gewählten Parametern folgenden Code einfügen.\n\nWählen Sie dazu eine Verteilung und passende Parameter auf Distribution Zoo aus.\nSchauen Sie unter dem Reiter Code mit welcher Funktion die Daten in R generiert werden können. Wählen Sie Language: R und Property: random sample of size n aus.\nKopieren Sie die Funktion und ersetzen Sie rnorm(1:length(rt), mu, sigma) in unserem R-Code für das Histogram oder den Density-Plot mit Ihrer neuen Funktion. Das n müssen Sie wieder 1:length(rt) nennen.\n\n\n[10 Minuten]\n\n\nBei Reaktionszeiten ist die Verteilung gar nicht so einfach anzupassen: Hier finden Sie “besser” geeignete Verteilungen, sowie die Möglichkeit für einen vorgegebenen Datensatz oder Ihre eigenen Daten Parameterwerte anzupassen.\n\n\n3. Weitere Einflussfaktoren\nIn einem DAG können auch weitere Informationen, zum Beispiel Bedingungen sowie Messwiederholungen, hinzugefügt werden.\n\\(\\mu\\) kann sich zum Beispiel in Abhängigkeit der Bedingung (condition) verändern, also je nachdem ob die angezeigte Farbe kongruent war oder nicht.\nWenn wir nun den Einfluss der Bedingung untersuchen möchten, könnten wir uns fragen, wie stark diese eine Veränderung im Wert \\(\\mu\\) bewirkt. Genau dies tun wir z.B. bei Mittelwertsvergleichen wie z.B. bei t-Tests.\n\n\n\n\n\n\nHands-on: DAG zeichnen\n\n\n\nWie würde ein DAG für die accuracy (Korrektheit) der Stroop-Daten aussehen?\nGehen Sie wie folgt vor:\n\nWas ist bekannt/wurde gemessen?\nWelche Verteilung beschreibt die Daten gut?\nWelche Parameter müssen geschätzt werden?\n\n[5 Minuten]",
    "crumbs": [
      "Datenanalyse",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Datengenerierende Prozesse</span>"
    ]
  },
  {
    "objectID": "intro_analysis.html#datensimulation",
    "href": "intro_analysis.html#datensimulation",
    "title": "13  Datengenerierende Prozesse",
    "section": "13.4 Datensimulation",
    "text": "13.4 Datensimulation\nSich Gedanken zum datengenerierenden Prozess zu machen (wie beispielsweise mit einem aufgezeichneten Modell) hilft nicht nur in der Planung der Datenanalyse, sondern ermöglicht u.a. auch das Simulieren von Daten.\nMögliche Schritte in der Datensimulation \n\nShiny-App für Datensimulation\n\nDatensimulation ist nützlich für:\n\nDie Vorbereitung von Präregistrationen und Registered Reports\nTesten/Debugging von Analysekripten (weil die ground truth bekannt ist)\nPower für komplexe Modelle schätzen\nErstellen von reproduzierbaren Beispielsdatensätzen (für Demos, Lehre, oder wenn echte Datensätze nicht veröffentlicht werden können)\nPrior distribution checks in der Bayesianischen Statistik\nVerstehen von Modellen und Statistik\n\n\nWeitere Infos zu Datensimulation\n\nUm Hypothesen zu testen, müssen selbstverständlich nicht simulierte Daten erhoben werden! 2",
    "crumbs": [
      "Datenanalyse",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Datengenerierende Prozesse</span>"
    ]
  },
  {
    "objectID": "intro_analysis.html#footnotes",
    "href": "intro_analysis.html#footnotes",
    "title": "13  Datengenerierende Prozesse",
    "section": "",
    "text": "Uns kann beispielsweise die Aufmerksamkeitsleistung interessieren, welche wir mit einem Testverfahren für Aufmerksamkeit zu messen versuchen. Eine Neurowissenschaftlerin, welche sich für den Prozess von Aufmerksamkeit interessiert, würde versuchen die Aufmerksamkeitsleistung von vielen Leuten unter verschiedenen Bedingungen zu messen um zu untersuchen, durch was Aufmerksamkeit beeinflusst wird. Ein klinischer Neuropsychologe hingegen hätte vielleicht das Ziel festzustellen, ob die Aufmerksamkeitsleistung einer Person von der Norm abweicht, beispielsweise weil sie durch einen Unfall eine Kopfverletzung erlitten hat. Beide messen Daten und beide ziehen aus den gemessenen Daten Rückschlüsse auf eine unterliegende Eigenschaft eines Prozesses oder einer Person.↩︎\nhttps://www.science.org/content/article/dutch-university-sacks-social-psychologist-over-faked-data↩︎",
    "crumbs": [
      "Datenanalyse",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Datengenerierende Prozesse</span>"
    ]
  },
  {
    "objectID": "parameterestimates.html",
    "href": "parameterestimates.html",
    "title": "14  Parameterschätzung: Einführung in die Bayesianische Statistik",
    "section": "",
    "text": "14.1 Frequentistische und Bayesianische Parameterschätzung\nIn der Frequentistischen Statistik wird angenommen, dass ein Parameter einen wahren (aber unbekannten) Wert hat. Die frequentistische Parameterschätzung ergibt eine Punktschätzung: Der geschätzte Parameter hat damit genau einen Wert und keine Wahrscheinlichkeitsverteilung. Daher dürfen keine Aussagen über eine Wahrscheinlichkeitsverteilung des Parameters bzw. die Wahrscheinlichkeit eines Parameterswerts gemacht werden. Nur Ereignisse die wiederholt werden können eine Wahrscheinlichkeit (eine Häufigkeitsverteilung) haben.\nIn der Bayesianischen Statistik hingegen wird für jeden möglichen Parameterwert geschätzt, wie wahrscheinlich dieser einzelne Wert ist in Anbetracht der Vorannahmen (Priors) und der Daten. Das bedeutet, wir erhalten für jeden dieser Werte gleichzeitig auch eine Wahrscheinlichkeit mit der dieser zutrifft. Die Zusammenfassung aller geschätzten Werte und deren Wahrscheinlichkeiten wird in der Posterior-Verteilung zusammengefasst. Die Posterior Wahrscheinlichkeit beschreibt unser degree of belief, also unser aktuelles Wissen darüber, wie wahrscheinlich dieser Parameterwert wirklich hinter den Daten steckt.\nWir schauen uns die unterschiedlichen Ansätze der Parameterschätzung im Folgenden an einem Beispiel an. Wir haben bei einer Person z.B. beobachtet, dass sie in 15 von 20 Trials korrekt geantwortet hat.\ncorrect &lt;- 15 # Anzahl korrekter Antworten\ntrials &lt;- 20 # Anzahl Trials insgesamt",
    "crumbs": [
      "Datenanalyse",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Parameterschätzung: Einführung in die Bayesianische Statistik</span>"
    ]
  },
  {
    "objectID": "parameterestimates.html#frequentistische-und-bayesianische-parameterschätzung",
    "href": "parameterestimates.html#frequentistische-und-bayesianische-parameterschätzung",
    "title": "14  Parameterschätzung: Einführung in die Bayesianische Statistik",
    "section": "",
    "text": "Hands-on: Frequentistisch oder Bayesianisch?\n\n\n\nOrdnen Sie die untenstehenden Aussagen dem frequentistischen bzw. dem baysianischen Ansatz zu:\n\n“Der Mittelwert liegt mit 95%-iger Wahrscheinlichkeit zwischen 0.75 und 0.85 Sekunden.”\n“Wenn das Experiment 100 Mal wiederholt wird, ist der wahre Mittelwert in 95% der Konfidenzintervalle enthalten.”\n\n\n\n\n\n\n14.1.1 Maximum-Likelihood Schätzung\n\\(\\theta\\) ist der Parameterwert unter dem die beobachteten Daten am wahrscheinlichsten entstanden sind. Die beste Punktschätzung des Parameters \\(\\theta\\), die wir machen können, wenn wir nur die Daten betrachten, und kein weiteres Vorwissen berücksichtigen, ist die Maximum-Likelihood Schätzung.\nMöchten wir also z.B. schätzen mit welcher Wahrscheinlichkeit die Person beim nächsten Trial eine richtige Antwort gibt, können wir dies aus den bisherigen Trials berechnen:\n\\[\\theta = correct / all \\]\nWenn die Person in insgesamt 20 Trials 15 Mal richtig geantwortet hat, wäre die Schätzung\n\\(\\theta = 15 / 20 = 0.75\\)\n\ntheta &lt;- correct / trials\ntheta\n\n[1] 0.75\n\n\nWir erhalten eine Punktschätzung (einen Wert), die uns angibt mit welcher Wahrscheinlichkeit die Person beim nächsten Trial richtig antworten wird, nämlich 0.75 bzw. sie wird in 3/4 der Fälle richtig antworten.\nWenn man ganz viele Male diese Spiele wiederholen würde, dann würde man diese Messung am wahrscheinlichsten reproduzieren können, wenn man für \\(\\theta\\) den Wert 0.75 einsetzt.\nDer grosse Nachteil einer Punktschätzung ist es, dass wir keine Wahrscheinlichkeitsverteilung erhalten. Es gäbe auch noch viele andere Parameterwerte, die dieses Ergebnis von 15 korrekten Antworten in 20 Trials hervorbringen könnten, z.B. \\(\\theta = 0.73\\) oder \\(\\theta = 0.78\\). Diese werden bei der Punktschätzung nicht beachtet.\nUm das zu veranschaulichen plotten wir die Wahrscheinlichkeit von 15 korrekten Antworten in 20 Trials für alle Werte welche \\(\\theta\\) annehmen könnte. Diese Werte liegen zwischen 0 und 1, da wir von einer Wahrscheinlichkeit sprechen.\n\nlibrary(tidyverse)\n\n# seed setzen für reproduzierbare ergebnisse\nset.seed(42) \n\n# daten generieren\nd &lt;- tibble(x = seq(from = 0, to = 1, by = .01)) |&gt;\n    mutate(density = dbinom(15, 20, x))\n\nd |&gt;\n    ggplot(aes(x = x, ymin = 0, ymax = density)) +\n    geom_ribbon(alpha = 0.5, fill = \"steelblue\") +\n    geom_vline(xintercept = theta, linetype = 2, linewidth = 1.2) +\n    scale_y_continuous(NULL, breaks = NULL) +\n    coord_cartesian(xlim = c(0, 1)) +\n    labs(x = expression(paste(\"Geschätzter Wert von \", theta))) +\n    theme_minimal()\n\n\n\n\n\n\n\n\nDie Punktschätzung von \\(\\theta\\) wird mit der schwarzen gestrichelten Linie dargestellt. Die hellblaue Fläche zeigt, wie wahrscheinlich die einzelnen Werte jeweils sind (hier abgebildet sehen Sie relative Wahrscheinlichkeiten).\n\n\n\n\n\n\nHands-on: Punktschätzung\n\n\n\nDiskutieren Sie in kleinen Gruppen, wie sinnvoll es ist sich hier auf einen Wert festzulegen:\n\nWie genau denken Sie bildet die Punktschätzung die Realität ab?\nWie viel wahrscheinlicher ist das berechnete \\(\\theta = 0.75\\) im Vergleich zu \\(\\theta = 0.70\\)?\nWas kann das Schätzen der Wahrscheinlichkeit für alle Parameterwerte für einen Mehrwert bringen?\nWo kann eine Punktschätzung einen Mehrwert haben?\n\n\n\n\n\n14.1.2 Posterior-Schätzung in der Bayesianischen Statistik\nIn der Bayesianischen Statistik wird die Wahrscheinkeitslehre angewandt, um die Wahrscheinlichkeit von Parameterwerten zu berechnen. Es wird für jeden möglichen Parameterwert die Wahrscheinlichkeit geschätzt mit der dieser Parameterwert die Daten generiert hat. Im Gegensatz zu der Frequentistischen Statistik wird hier also nicht eine Punktschätzung vorgenommen (ein “wahrer Wert” geschätzt), sondern es wird ein Verteilung geschätzt.\n\nDie Posterior-Verteilung beschreibt, wie wahrscheinlich verschiedene Werte eines unbekannten Parameters sind – basierend auf Vorwissen (Prior) und den beobachteten Daten.\n\nDass der Posterior über alle möglichen Parameterwerte integriert wird, ist eine grösse Stärke der Bayesianischen Statistik. So wird der ganze Möglichkeitsraum beschrieben. Es wird nicht nur der wahrscheinlichste Parameterwert berücksichtigt wie bei der Punktschätzung, sondern durch das Einbeziehen der ganzen Parameterverteilung können auch Nebenoptima und “fast” genauso wahrscheinliche Werte einbezogen werden.\nUm die Posterior-Verteilung, also die Wahrscheinlichkeit aller Parameterwerte, zu berechnen wird in der Bayesianischen Statistik das Bayes Theorem verwendet.\n\n\n\n\n\n\nBayes Theorem\n\n\n\nDas Bayes Theorem gibt die Formel für eine bedingte Wahrscheinlichkeit \\(P(A|B)\\) an.\n\\[ P(A|B) = \\frac{P(B|A)⋅P(A)}{P(B)} \\]\nDas kann gelesen werden als: “Die Wahrscheinlichkeit eines Ereignisses \\(A\\) unter der Bedingung, dass ein Ereignis \\(B\\) wahr ist, ist gleich der a priori Wahrscheinlichkeit, dass \\(A\\) wahr ist, multipliziert mit der Wahrscheinlichkeit, dass \\(B\\) eintritt, wenn \\(A\\) wahr ist. Dividiert wird das Ganze durch die Wahrscheinlichkeit, dass \\(B\\) eintritt, egal ob \\(A\\) wahr oder falsch ist.”\n\n\nDas bedeutet, um eine Bayesianische Parameterschätzung zu machen, müssen wir Vorwissen integrieren. Dies tun wir in Form einer Prior-Verteilung. Ein simple Variante ist, den Prior ist so zu wählen, dass er allen möglichen Werten dieselbe Wahrscheinlichkeit zuschreibt (wie in der Grafik unten). Diese Verteilung wird uniform genannt. Ein uniformer Prior ist aber selten empfehlenswert, da er zu breit und uniformativ ist.\nParameterschätzung\n\n\n\n\n\n\n\nHands-on: Bayesianische Parameterschätzung in JASP\n\n\n\nAktivieren Sie in JASP das Modul Learn Bayes. Wählen Sie unter Learn Bayes &gt; Binomial Estimation mit der Einstellung Enter Sequence.\n1. Modell: Stellen Sie sich vor, sie untersuchen eine Person, welche behauptet, extrasensorische Fähigkeiten zu besitzen. Diese Person behauptet, dass sie bevor eine Münze aufgeworfen wurde vorhersagen kann, auf welcher Seite die Münze landet: Kopf oder Zahl).\n\nWie werden die Daten verteilt sein? Welchen Parameter schätzen wir? Wie sieht das DAG aus?\nWie würden Sie die Behauptung der Person überprüfen?\n\n2. Daten erheben: Sie werfen die Münze 20 mal und die Person macht x korrekte Vorhersagen.\n\nGlauben Sie, dass die Person über extra-sensorische Fähigkeiten verfügt? Sind Sie skeptisch?\nUnter den Dropdown Menus Model, Prior and Posterior Distributions und Plots gibt es verschiedene Checkboxes. Versuchen Sie herauszufinden, was diese bewirken.\n\n3. Vorwissen / Prior definieren.\n\nWie können Sie Ihr Vorwissen in die Analyse einbeziehen? Wie verbinden Sie Ihr Vorwissen mit den beobachteten Daten?\nWelcher Prior bedeutet was (vgl. Bild unten)?\nPassen Sie Ihren Prior für \\(\\theta\\) in JASP an.\n\nBeta Verteilungen",
    "crumbs": [
      "Datenanalyse",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Parameterschätzung: Einführung in die Bayesianische Statistik</span>"
    ]
  },
  {
    "objectID": "parameterestimates.html#zusammenfassen-von-posteriors",
    "href": "parameterestimates.html#zusammenfassen-von-posteriors",
    "title": "14  Parameterschätzung: Einführung in die Bayesianische Statistik",
    "section": "14.2 Zusammenfassen von Posteriors",
    "text": "14.2 Zusammenfassen von Posteriors\nDer Vorteil einer Posterior-Verteilung im Vergleich zu einer Punktschätzung ist es, dass wir damit Aussagen zu der Wahrscheinlichkeit eines Parameterwertes machen können. Da Posterior-Verteilungen oft komplex sind, fassen wir sie mit Hilfe von Kennzahlen zusammen.\nTypische Zusammenfassungen sind Mittelwert, Median, Modus und Intervalle (Credible Intervals). Hier einige Beispiele, zu möglichen Aussagen:\n\nA. \\(\\theta\\) liegt am wahrscheinlichsten bei \\(X\\). (Mittelwert, Median, Modus)\nB. Die Wahrscheinlichkeit, dass \\(\\theta\\) mindestens \\(X\\) ist, liegt bei \\(P_x\\).\nC. Die Wahrscheinlichkeit, dass \\(\\theta\\) kleiner als \\(X\\) ist, liegt bei \\(P_x\\).\nD. Die Wahrscheinlichkeit, dass \\(\\theta\\) zwischen \\(X_{tiefer}\\) und \\(X_{höher}\\) liegt ist \\(P_x\\).\nE. \\(\\theta\\) liegt mit einer Wahrscheinlichkeit von 95% zwischen \\(X_{tiefer}\\) und \\(X_{höher}\\).\nF. \\(\\theta\\) liegt mit einer Wahrscheinlichkeit von 20% ausserhalb des Bereichs zwischen \\(X_{tiefer}\\) und \\(X_{höher}\\).\n\n\n\nGute Möglichkeiten zum Zusammenfassen von Posterior-Verteilungen bieten die Plot-Funktionen R-Packages {brms}, {tidybayes} und {bayesplot}.\n\n\n\n\n\n\n\nHands-on: Credible interval vs. confidence interval\n\n\n\nCredible interval ist ein bayesianisches Konzept, das sich vom confidence interval (Konfidenzintervall) in der frequentistischen Statistik unterscheidet. Ein credible interval ist ein Intervall, das eine bestimmte Wahrscheinlichkeit enthält, dass der wahre Parameter innerhalb dieses Intervalls liegt. Dies sollte nicht mit dem Konfidenzintervall verwechselt werden. Können Sie sich daran erinnern, wie ein Konfidenzintervall definiert ist? Was ist der Unterschied zwischen einem Konfidenzintervall und einem credible interval?",
    "crumbs": [
      "Datenanalyse",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Parameterschätzung: Einführung in die Bayesianische Statistik</span>"
    ]
  },
  {
    "objectID": "parameterestimates.html#entscheidung",
    "href": "parameterestimates.html#entscheidung",
    "title": "14  Parameterschätzung: Einführung in die Bayesianische Statistik",
    "section": "14.3 Entscheidung",
    "text": "14.3 Entscheidung\nAnhand der Posterior-Verteilung eines Parameters kann eine Entscheidung getroffen werden. Wichtig ist hierbei, dass diese Kriterien vor dem Betrachten der Posterior-Verteilungen festgesetzt werden (analog zu den Hypothesen und Signifikanzniveaus bei frequentistischen Verfahren). Beispiele für Entscheidungskriterien sind:\n\nIm besten Fall werden (unabhängig ob frequentistisch oder bayesianisch analysiert wird) die Hypothesen, Analysen, Entscheidungskriterien, etc. vor der Datenanalyse festgelegt, z.B. durch das Erstellen einer Präregistration!\n\n\nWenn 95% der geschätzen Parameterwerte über einem bestimmten Cut-off liegen, ist der Unterschied bedeutsam.\nIst der geschätzte Parameterwert mehr als 2 Einheiten von 0 verschieden, so wird auf einen Unterschied geschlossen.\n\naber auch:\n\nUnterscheiden sich die Posterior-Verteilungen zweier Parameterschätzungen nicht zu mehr als 10% wird darauf geschlossen, dass kein Unterschied besteht.\nWeicht der geschätzte Parameterwert weniger als 2 Einheiten von 0 ab, gibt es keinen Unterschied.\n\nHier zeigt sich der Vorteil der Bayesianischen Parameterschätzung: Wir können durch das Zusammenfassen der Posterior-Verteilung direkt evidence of absence testen.\n\n\n\n\n\n\nHands-on: Entscheidungen aufgrund von Posterior-Verteilungen\n\n\n\nWie könnten Entscheidungen aussehen bezüglich unserer Stroop oder Random-Dot Daten?\nErstellen Sie je ein Entscheidungskriterium, für das Erkennen eines Effekts und für die Abwesenheit eines Effekts.\n\n\n\n14.3.1 Wrap-up\nZusammenfassend kann gesagt werden:\n\nIn der frequentistischen Statistik wird angenommen, dass der Parameter einen wahren Wert hat, den wir aber nicht kennen. Wir erhalten als Resultat eine Punktschätzung für den Parameter und können keine Aussage über die Wahrscheinlichkeit dieses einen geschätzten Parameterwerts machen. Der 95%-CI (confidence interval) sagt aus, dass bei Wiederholung des Experiments der “wahre” Parameterwert in 95% der Konfidenzintervalle enthalten sein wird.\nIn der bayesianischen Statistik wird angenommen, dass der Parameter eine Wahrscheinlichkeitsverteilung hat, die wir schätzen können. Es muss zusätzlich eine Priorverteilung festgelegt werden. Wir erhalten eine Posterior Verteilung für die Parameterwerte und können eine Aussage über Wahrscheinlichkeit eines Parameterwerts oder eines Modelles machen. Der 95%-CrI (credible interval) enthält zu 95% den “wahren” Parameterwert.\n\n\n“Wahr” bedeutet hier, den Parameterwert der (angenommen ein passendes Modell wurde verwendet) zu diesen Daten geführt hat.\n\n\n\n\n\n\n\nNote\n\n\n\nIn der Bayesianischen Statistik erhalten wir nach der Anwendung des Satzes von Bayes die sogenannte Posterior-Verteilung. Sie beschreibt, wie wahrscheinlich verschiedene Werte eines unbekannten Parameters sind – basierend auf unserem Vorwissen (Prior) und den beobachteten Daten.\n\n\n\n\n14.3.2 Weiterführende Informationen\n\nKruschke, J.K., Liddell, T.M. The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective. Psychonomic Bulletin & Review 25, 178–206 (2018). https://doi.org/10.3758/s13423-016-1221-4",
    "crumbs": [
      "Datenanalyse",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Parameterschätzung: Einführung in die Bayesianische Statistik</span>"
    ]
  },
  {
    "objectID": "hypothesistests.html",
    "href": "hypothesistests.html",
    "title": "15  Hypothesentests: Bayesianischer \\(t\\)-Test und Äquivalenztests (TOSTs)",
    "section": "",
    "text": "15.1 Bayesianische Hypothesentests",
    "crumbs": [
      "Datenanalyse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Hypothesentests: Bayesianischer $t$-Test und Äquivalenztests (TOSTs)</span>"
    ]
  },
  {
    "objectID": "hypothesistests.html#bayesianische-hypothesentests",
    "href": "hypothesistests.html#bayesianische-hypothesentests",
    "title": "15  Hypothesentests: Bayesianischer \\(t\\)-Test und Äquivalenztests (TOSTs)",
    "section": "",
    "text": "15.1.1 Modellvergleich\nBeim Modellvergleich interessiert welches Modell die Daten besser erklärt. Die Bayes’sche Regel kann verwendet werden, um die Wahrscheinlichkeit zweier Modelle \\(\\mathcal{M1}\\) und \\(\\mathcal{M2}\\) zu berechnen (gemittelt über alle möglichen Parameterwerte innerhalb des Modells):\n\\[\np(\\mathcal{M}_1 | y) = \\frac{P(y | \\mathcal{M}_1) p(\\mathcal{M}_1)}{p(y)}\n\\]\nund\n\\[\np(\\mathcal{M}_2 | y) = \\frac{P(y | \\mathcal{M}_2) p(\\mathcal{M}_2)}{p(y)}\n\\]\nHierfür kann das Verhältnis der beiden Wahrscheinlichkeiten (Posterior Odds) berechnet werden: \\(p(\\mathcal{M}_1 | y) / p(\\mathcal{M}_2 | y)\\), was gekürzt folgende Formel ergibt:\n\\[\n\\underbrace{\\frac{p(\\mathcal{M}_1 | y)} {p(\\mathcal{M}_2 | y)}}_\\text{Posterior odds} = \\underbrace{\\frac{P(y | \\mathcal{M}_1)}{P(y | \\mathcal{M}_2)}}_\\text{Ratio of marginal likelihoods} \\cdot \\underbrace{ \\frac{p(\\mathcal{M}_1)}{p(\\mathcal{M}_2)}}_\\text{Prior odds}\n\\]\nAuf der linken Seite steht das Verhältnis der a-posteriori Wahrscheinlichkeiten der beiden Modelle, auf der rechten Seite das Verhältnis der Marginal Likelihoods der beiden Modelle, multipliziert mit den a-priori Wahrscheinlichkeiten jedes Modells.\nDie Marginal Likelihoods (auch bekannt als Modell-Evidenz) zeigen, wie gut jedes Modell die Daten erklärt. Diese geben darüber Auskunft, wie wahrscheinlich die Daten sind, wenn wir alle möglichen Parameterwerte berücksichtigen. Die Marginal Likelihoods sind also die Wahrscheinlichkeit der Daten, gemittelt über alle möglichen Parameterwerte.\n\n\n15.1.2 Bayes Factors\nDie Posterior Odds sagen uns, welches Modell wir a-priori und a-posteriori für wahrscheinlicher halten. Da unsere a-priori Überzeugungen aber subjektiv sein können, sind wir eigentlich nur an dem Verhältnis der marginalen Likelihoods interessiert. Wir können annehmen, dass a-priori die beiden Modelle gleichwahrscheinlich sind; das heisst, wir setzen die Prior Odds auf 1 setzen. So erhalten wir den Bayes Factor:\n\\[\n\\frac{P(y | \\mathcal{M}_1)}{P(y | \\mathcal{M}_2)}\n\\]\nWenn \\(P(y | \\mathcal{M}_1)\\) grösser ist als \\(P(y | \\mathcal{M}_2)\\), dann ist der Bayes Factor grösser als 1. Falls \\(P(y | \\mathcal{M}_1)\\) kleiner ist als \\(P(y | \\mathcal{M}_2)\\), dann ist der Bayes Factor kleiner als 1. Der Bayes Factor gibt also direkt an, welches Modell die Daten besser erklärt.\nWenn wir zwei Modelle \\(\\mathcal{M}_1\\) und \\(\\mathcal{M}_2\\) vergleichen, wird der Bayes Factor oftmals so geschrieben:\n\\[ BF_{12} = \\frac{P(y | \\mathcal{M}_1)}{P(y | \\mathcal{M}_2)}\\]\n\\(BF_{12}\\) ist also der Bayes Factor für \\(\\mathcal{M}_1\\) und gibt an um wieviel \\(\\mathcal{M}_1\\) die Daten besser “erklärt”.\nAls Beispiel, wenn wir ein \\(BF_{12} = 5\\) erhalten, bedeutet dies, dass die Daten 5 Mal wahrscheinlicher unter Modell 1 als unter Modell 2 aufgetreten sind. Umgekehrt, wenn \\(BF_{12} = 0.2\\), dann sind die Daten 5 Mal wahrscheinlicher unter Modell 2 aufgetreten.\nWenn wir \\(BF_{12} = 0.2\\) erhalten, ist es einfacher, Zähler und Nenner zu vertauschen:\n\\[ BF_{21} = \\frac{P(y | \\mathcal{M}_2)}{P(y | \\mathcal{M}_1)}\\]\nDie folgenden Interpretationen von Bayes Factors werden manchmal verwendet, obwohl es nicht wirklich notwendig ist, diese zu klassifizieren. Bayes Factors sind ein kontinuierliches Mass für Evidenz.\nZusammenfassend kann gesagt werden:\n\nDer Bayes Factor ist ein Verhältnis zweier konkurrierender statistischer Modelle, die durch ihre Evidenz dargestellt werden, und wird verwendet, um die Unterstützung für ein Modell gegenüber dem anderen zu quantifizieren. Die fraglichen Modelle können einen gemeinsamen Satz von Parametern haben, z. B. eine Nullhypothese und eine Alternative, dies ist jedoch nicht erforderlich. Zum Beispiel könnte es sich auch um ein nichtlineares Modell im Vergleich zu seiner linearen Näherung handeln. Wikipedia\n\nBayes Factors sind eine alternative Methode, um Evidenz zu quantifizieren. Alternativ zu p-Werten bieten Bayes Factors Evidenz für oder gegen eine Hypothese.\n\n\\(p\\)-Werte sind schwierig zu erklären, auch für erfahrene Forschende, wie dieses Video zeigt.\n\n\n\n15.1.3 Bayesianischer \\(t\\)-Test\nWir führen oft Modellvergleiche zwischen einer Nullhypothese \\(\\mathcal{H}_0\\) und einer alternativen Hypothese \\(\\mathcal{H}_1\\) durch (Die Begriffe “Modell” und “Hypothese” werden synonym verwendet). Eine Nullhypothese bedeutet, dass wir den Wert des Parameters auf einen bestimmten Wert festlegen, z.B. \\(\\theta = 0.5\\). Die alternative Hypothese bedeutet, dass wir den Wert des Parameters nicht festlegen, sondern eine a-priori Verteilung annehmen. Im Gegensatz zu NHST muss die Alternativhypothese spezifiziert werden. Mit anderen Worten, die Parameter müssen eine a-priori Verteilung erhalten.\nIn JASP werden Bayes Factors (BF) so berichtet:\n\\[ BF_{10} = \\frac{P(y | \\mathcal{H}_1)}{P(y | \\mathcal{H}_0)}\\]\nDies ist ein BF für eine ungerichtete Alternative \\(\\mathcal{H}_1\\) gegen die Nullhypothese \\(\\mathcal{H}_0\\). Wenn wir einen gerichteten Test durchführen, dann wird der BF entweder so (\\(&gt;0\\)):\n\\[ BF_{+0} = \\frac{P(y | \\mathcal{H}_+)}{P(y | \\mathcal{H}_0)}\\]\noder so (\\(&lt;0\\)) berichtet. \\[ BF_{-0} = \\frac{P(y | \\mathcal{H}_-)}{P(y | \\mathcal{H}_0)}\\]\nWenn wir nun einen BF für die Nullhypothese wollen, können wir einfach den Kehrwert von \\(BF_{10}\\) nehmen:\n\\[ BF_{01} = \\frac{1}{BF_{10}}\\]\n\n\n\n\n\n\nHands-on: Bayesiansicher \\(t\\)-Test in JASP\n\n\n\n\nLaden Sie hier den Stroop Datensatz herunter. Der Datensatz wurde für diese Zwecke ins wide-Format angepasst.\nLaden Sie den Datensatz in JASP und schauen Sie ihn an.\nWie könnte die Forschungsfrage lauten? Was ist die Null- und Alternativhypothese?\nFühren Sie einen Bayesian Paired Sample $t$-Test aus.\nExplorieren Sie die Resultate und welche Optionen Sie für weitere Einstellungen haben.\n\nWo kann der Prior angepasst werden?\nWo können Hypothesen spezifiziert werden?\nWo können Sie den Posterior anschauen?\nWelche Visualisierungsmöglichkeiten haben Sie?\n\nWelche Bayes Factors finden Sie für die Nullhypothese? Und für die Alternativhypothese?\n\n\n\n\n\n\n\n\n\nWeiterführende Informationen zu Bayesianischem Hypothesentesten\n\n\n\n\nVertiefende Informationen, inkl. Herleitung, zu Bayesianischen Hypothesentest finden Sie hier.\nHier finden Sie eine interaktive Visualisierung.\nbrms ist ein R-Package, welches sich für Bayesianische Multilevel-Modelle eignet, da JASP relativ rasch an die Grenzen stösst für komplexere Modelle.",
    "crumbs": [
      "Datenanalyse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Hypothesentests: Bayesianischer $t$-Test und Äquivalenztests (TOSTs)</span>"
    ]
  },
  {
    "objectID": "hypothesistests.html#äquivalenztests",
    "href": "hypothesistests.html#äquivalenztests",
    "title": "15  Hypothesentests: Bayesianischer \\(t\\)-Test und Äquivalenztests (TOSTs)",
    "section": "15.2 Äquivalenztests",
    "text": "15.2 Äquivalenztests\nEin nicht signifikanter Nullhypothesensignifikanztest (NHST) bedeutet nicht zwingend, dass kein Unterschied zwischen Gruppen/Bedingungen besteht, nur dass keiner gefunden werden konnte. Bei einer Power von 80% ist beträgt die Wahrscheinlichkeit die Nullhypothese fälschlicherweise nicht abzulehnen 20%!\nStatt aussschliesslich einen NHST durchzuführen und gegen die Nullhypothese zu testen, können Äquivalenztests, z.B. das “Two One-Sided Tests” (TOST) Verfahren durchgeführt werden. Dabei werden zwei einseitige \\(t\\)-Tests ausgeführt, um zu testen, ob der Mittelwertsunterschied innerhalb eines gewissen Rahmens liegt.\nDieser Rahmen muss von den Forschenden festgelegt werden und bezeichnet die kleinstmöglichste interessierende Effekt: Smallest Effect Size of Interest (SESOI). Mit dem TOST-Verfahren wird festgestellt, ob der gefundene Unterschied überraschend gering ist, wenn ein Effekt mindestens so gross wie der definierte SESOI tatsächlich existiert. (Lakens, Scheel & Isager, 2018).\n\nHier finden Sie eine interaktive Visualisierung des Äquivalenztests.\n\n\n15.2.1 Festlegen des SESOIs\nDer SESOI wird durch eine untere (lower bound \\(\\Delta\\)L) und obere Grenze (upper bound \\(\\Delta\\)U). Es gibt keine vorgegebenen Regeln für das Festlegen von SESOIs. Sie müssen von den Forschenden (vor der Datenanalyse) passend für Fragestellung und Feld festgelegt werden. Dafür gibt es folgende Ansätze:\n\nMehr dazu finden Sie hier Lakens et al. 2018\n\n\nSubjektive Ansätze:\n\nBenchmarks (z.B. Standardisierte Effektgrössen wie \\(d\\) = 0.5)\nvorherige Studien\nbegründet in vorhandenen Ressourcen für die Studie\n\nObjektive Ansätze:\n\ntheoretische Begründungen (z.B. just-noticeable difference, JND)\nbasierend auf quantifizierbaren theoretischen Vorhersagen (z.B. durch computational models)\n\n\n\n\n15.2.2 TOST-Verfahren\nNach dem Festlegen des SESOIs werden zwei einseitige \\(t\\)-Tests durchgeführt:\n\nTest, ob Unterschied signifikant höher als untere Grenze (\\(\\Delta\\)L)\nTest, ob Unterschied signifikant tiefere als obere Grenze (\\(\\Delta\\)U)\n\nWenn beide signifikant ausfallen sind die Gruppen/Bedingungen äquivalent. Es muss nicht für zweifaches Testen korrigiert werden, weil beide Tests signifikant sein müssen, um Äquivalenz anzunehmen.\nAnstelle von \\(p\\)-Werten können auch Konfidenzintervalle verwendet werden: Wenn das 90% Konfidenzintervall innerhalb des SESOIs liegt wird Äquivalenz angenommen. Das Konfidenzintervall muss festgelegt werden, es könnte auch z.B. 95%, 87% oder 99% sein (oder eine andere Zahl). Es muss daher zwingend vor dem Test gewählt werden.\n\n\n\n\n\n\nHands-on: Äquivalenztests in JASP\n\n\n\n\nDefinieren Sie einen SESOI für den Unterschied der Reaktionszeiten zwischen der kongruenten und inkongruenten Bedingung im Stroop Task. Was ist \\(\\Delta\\)L, was ist \\(\\Delta\\)U?\nAktivieren Sie in JASP das Modul Equivalence T-Tests.\nSie können wieder dieselben Daten wie vorhin verwenden.\nWählen Sie die passenden Variablen zum Vergleich aus.\nGeben Sie bei Equivalence Region Ihren SESOI ein (in Rohwerten oder standardisiert).\nExplorieren Sie die Resultate und welche Optionen Sie für weitere Einstellungen haben.\n\n\n\n\n\n\n\n\n\nWeiterführende Informationen zu Äquivalenztests\n\n\n\n\nVertiefende Informationen finden Sie in diesem Paper.\nHier finden Sie eine interaktive Visualisierung.\nEquivalence-Testing in JASP\nIn Jamovi können Äquivalenztest mit dem Modul TOSTER durchgeführt werden (weitere Informationen)",
    "crumbs": [
      "Datenanalyse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Hypothesentests: Bayesianischer $t$-Test und Äquivalenztests (TOSTs)</span>"
    ]
  },
  {
    "objectID": "uebung_4.html",
    "href": "uebung_4.html",
    "title": "Übung 4",
    "section": "",
    "text": "Auftrag\nDatenanalyse\nData Report in Form eines Posters erstellen\nWichtig:",
    "crumbs": [
      "Datenanalyse",
      "Übung 4"
    ]
  },
  {
    "objectID": "uebung_4.html#auftrag",
    "href": "uebung_4.html#auftrag",
    "title": "Übung 4",
    "section": "",
    "text": "optional: Datenvorverarbeiten in R (z.B. Filtern für bestimmte Gruppen/Fälle, nur wenn Bedarf)\nAnalysieren der Daten des Random-Dot Experiments\n\n\n\nPoster erstellen\n\n\n\nInformation zum Arbeiten in Kleingruppen:\n\nÜbungen dürfen alleine oder in Gruppen von max. 3 Personen erledigt werden. Alle Personen müssen die Übung auf Ilias hochladen, um die Übung zu bestehen.\nDie Files von Gruppenarbeiten müssen folgendermassen benannt werden, damit wir sehen, welche Übungsabgaben zusammengehören: Nennen Sie das File mit der Aufgabe und allen Initialen der Gruppe. Z.B. analyse_GW_EW.jasp. Geben Sie bei allen Files die Initialen in derselben Reihenfolge an.\n\nArbeitsanweisung unter dem Unterkapitel Vorgehen:\n\nLesen Sie die Anweisungen genau durch und geben Sie die Dateien in diesem Format ab (z.B. Benennung der Dateien).\nArbeiten Sie entlang der Arbeitsanweisung, um den Prozess zu vereinfachen.\n\nEs wird eine kleine Poster-Session geben, bei der die Poster angeschaut werden können.\n\nVerkleinern Sie die Schriftgrösse nicht zu sehr. Die Poster werden auf A3-Format von uns ausgedruckt. Bei zu kleiner Schrift wird das schwierig.\nDas Poster darf in Deutsch oder Englisch geschrieben sein.",
    "crumbs": [
      "Datenanalyse",
      "Übung 4"
    ]
  },
  {
    "objectID": "uebung_4.html#vorgehen",
    "href": "uebung_4.html#vorgehen",
    "title": "Übung 4",
    "section": "Vorgehen",
    "text": "Vorgehen\n\nDownload und Setup\n\n\n\n\n\n\nDatensatz Update 07.04.2025\n\n\n\nDer Datensatz wurde nochmals vorverarbeitet und hochgeladen.\nInformationen zu den Aussschlusskriterien und Ausschlüssen:\n\nAlle Trials mit Reaktionszeiten unter 100ms und über 12 Sekunden wurden ausgeschlossen.\nAlle Personen mit weniger als 50 gültigen Trials (von 60) pro Bedingung wurden ausgeschlossen. (= 3 Personen)\nAlle Personen mit einer Accuracy von weniger als 55% wurden ausgeschlossen. (= 39 Personen)\n\nFalls Sie den Datensatz vor dem 07.04.2025 heruntergeladen haben, können Sie ihn hier nochmals herunterladen (oder via zip-Ordner).\n\nwide Format\nlong Format\n\n\n\n\nLaden Sie die Daten und die Postervorlage hier uebung-4 herunter und entzippen Sie den Ordner. Der Ordner enthält 2 Datensätze mit vorverarbeiteten Daten des Random-Dot Experiments:\n\ndata_random_dot_wide.csv: Daten im wide-Format, geeignet für JASP-Analysen\ndata_random_dot_long.csv: Daten im long-Format, geeignet für R- oder JASP-Visualisierung (unter Descriptives) (optional)\n\n\nEr enthält eine id-Variable und Variablen(paare):\n\nVersuchspersonenidentifikation (id)\nrt_speed und rt_accuracy: Mittelwerte der Reaktionszeiten für die Instruktion speed(so schnell wie möglich antworten) und accuracy (so richtig wie möglich antworten) pro Versuchsperson\ncorr_speed und corr_accuracy: Mittelwerte der korrekten Antworten für die Instruktion speed(so schnell wie möglich antworten) und accuracy (so richtig wie möglich antworten) pro Versuchsperson\nrt_speed_left und rt_speed_right: Mittelwerte der Reaktionszeiten für die Instruktion speed(so schnell wie möglich antworten), bei Bewegung der Punkte nach links (left) oder gegen rechts (right) pro Versuchsperson\nrt_accuracy_left und rt_accuracy_right: Mittelwerte der Reaktionszeiten für die Instruktion accuracy(so richtig wie möglich antworten) bei Bewegung der Punkte nach links (left) oder gegen rechts (right) pro Versuchsperson\ncorr_speed_left und corr_speed_right: Mittelwerte der korrekten Antworten für die Instruktion speed(so schnell wie möglich antworten), bei Bewegung der Punkte nach links (left) oder gegen rechts (right) pro Versuchsperson\ncorr_accuracy_left und corr_accuracy_right: Mittelwerte der korrekten Antworten für die Instruktion accuracy(so richtig wie möglich antworten), bei Bewegung der Punkte nach links (left) oder gegen rechts (right) pro Versuchsperson\n\n\n\n1. Teil: Poster (Präregistration)\nWählen Sie eine Fragestellung und ein Variablenpaar aus.\n\na. Einführung/Introduction: Beschreiben Sie\n\nIhre Fragestellung/Forschungsfrage\nRelevanz\nHypothesen (Nullhypothese, Alternativhypothese)\nfügen Sie mind. 1 Referenz ein\n\nb. Methode/Methods: Beschreiben Sie\n\ndas Sample (N)\ndas Experimentalparadigma / den Task\nden Ablauf (evtl. eine Flowchart/ein Stimulusbild etc. einfügen)\nUV(n)\nAV(n)\ndas Analyseverfahren\n\n\nWählen Sie dafür eines dieser Analyseverfahren in JASP aus:\n- Bayesianischer t-Test für abhängige Stichproben (Paired Samples T-Test)\n- Bayesiansicher t-Test für unabhängige Stichproben (Independent Samples T-Test)\n- Äquivalenztest für abhängige Stichproben (Equivalence Paired Samples T-Test)\n- Äquivalenztest für unabhängige Stichproben (Equivalence Independent Samples T-Test)\nGeben Sie auch an:\n- für Äquivalenztests: Equivalence Region (2 boundaries, raw/cohens-d?)\n- für Bayesianische Tests: gewählter Prior (Standard ist ok)\n\n\n2. Teil: Datenanalyse in JASP\n\na. Lesen Sie für die Analysen das wide-Datenfile in JASP ein\nb. Schauen Sie sich die Deskriptivstatistik an\n\nTipp: Mit den Daten im long Format können Sie in Jasp Descriptives &gt; Raincloud Plots oder in R den esquisser() verwenden.\n\nc. Wählen Sie den geplanten Test.\n\nBayesianischer t-Test für abhängige Stichproben (Paired Samples T-Test)\nBayesiansicher t-Test für unabhängige Stichproben (Independent Samples T-Test)\nÄquivalenztest für abhängige Stichproben (Equivalence Paired Samples T-Test)\nÄquivalenztest für unabhängige Stichproben (Equivalence Independent Samples T-Test)\n\nd. Wählen Sie das geplante Variablenpaar und führen Sie den Test aus.\n\n\n\n3. Teil: Poster (Ergebnisse)\n\na. Resultate/Results: Berichten Sie die Resultate\n\nfür Äquivalenztests: Statistic, t, df, CI90%\nfür Bayesianische Tests: BF10, BF01, Median, CI95%\nFügen Sie mind. 1 Plot ein (aus JASP, R, Esquisser)\nFügen Sie evtl. eine Tabelle ein\nBeschreiben Sie Ihre Resultate in mind. 2 Punkten\n\nb. Diskussion/Discussion:\n\nWelche Schlussfolgerungen können Sie aus Ihrer Analyse ziehen?\nWelche Schlussfolgerungen können Sie aus Ihrer Analyse nicht ziehen? / Welche Limitation(en) hat Ihre Analyse?\n\nWelche Implikation haben Ihre Resultate?\n\n\n\n\nHochladen der Dateien auf Ilias\nLaden Sie folgende Dateien unter Übung 4 auf Ilias hoch:\n\n.jasp-File\nPoster (.pdf)",
    "crumbs": [
      "Datenanalyse",
      "Übung 4"
    ]
  },
  {
    "objectID": "uebung_4.html#abgabetermin",
    "href": "uebung_4.html#abgabetermin",
    "title": "Übung 4",
    "section": "Abgabetermin",
    "text": "Abgabetermin\nDer Abgabetermin für diese Übung ist der 08. Mai 2025.",
    "crumbs": [
      "Datenanalyse",
      "Übung 4"
    ]
  },
  {
    "objectID": "opensci.html",
    "href": "opensci.html",
    "title": "16  Getting Started: Open Science and Good Practices",
    "section": "",
    "text": "16.1 Was ist das Problem ?",
    "crumbs": [
      "Open Science",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Getting Started: Open Science and Good Practices</span>"
    ]
  },
  {
    "objectID": "opensci.html#was-ist-das-problem",
    "href": "opensci.html#was-ist-das-problem",
    "title": "16  Getting Started: Open Science and Good Practices",
    "section": "",
    "text": "Reproduzierbarkeitskrise in der Psychologie (und anderen Fachbereichen)\nEin Forschungsteam versuchte, 100 psychologische Studien zu replizieren – nur etwa 39 % zeigten denselben Effekt (https://osf.io/ezcuj/)\nHauptgründe: P-Hacking und mangelnde Transparenz\n\n\n\n\nsource: www.nature.com",
    "crumbs": [
      "Open Science",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Getting Started: Open Science and Good Practices</span>"
    ]
  },
  {
    "objectID": "opensci.html#open-science",
    "href": "opensci.html#open-science",
    "title": "16  Getting Started: Open Science and Good Practices",
    "section": "16.2 Open Science",
    "text": "16.2 Open Science\n\n16.2.1 FAIR Principles\nFAIR ist ein Satz von Leitprinzipien, die Daten nützlicher machen sollen – nicht nur für andere, sondern auch für das eigene zukünftige Ich. FAIR steht für Findable (auffindbar), Accessible (zugänglich), Interoperable (interoperabel) und Reusable (wiederverwendbar).\n\n\n\n\n\n\n\n\nPrinzip\nWas es bedeutet\nBeispiel\n\n\n\n\nF – Findable (Auffindbar)\nDaten sollen sowohl für Menschen als auch für Computer leicht auffindbar sein. Sie brauchen eine eindeutige Kennung und klare Metadaten (Informationen über die Daten).\nLaden Sie Ihren Datensatz in ein öffentliches Repositorium (z. B. OSF, Zenodo) mit DOI und einem aussagekräftigen Titel hoch.\n\n\nA – Accessible (Zugänglich)\nSobald Daten gefunden wurden, sollen sie über standardisierte Wege abrufbar sein – mit klaren Zugriffsbedingungen.\nAuch wenn der Zugriff eingeschränkt ist, sollten die Metadaten öffentlich sein und der Zugang erklärt werden.\n\n\nI – Interoperable (Interoperabel)\nDaten sollen in standardisierten Formaten und Begriffen strukturiert sein, damit sie mit anderen Daten und Tools zusammenarbeiten können.\nVerwenden Sie CSV statt proprietärer Formate und sprechende Variablennamen (z. B. „alter“ statt „a_g3“).\n\n\nR – Reusable (Wiederverwendbar)\nDaten sollen ausreichend dokumentiert sein, damit andere sie korrekt interpretieren und wiederverwenden können.\nFügen Sie eine README-Datei mit Beschreibung der Variablen hinzu und verwenden Sie eine offene Lizenz wie CC-BY.\n\n\n\nsource: https://www.go-fair.org/fair-principles/\n\n\n\nsource: FOSTER Open Science Training Handbook (https://github.com/Open-Science-Training-Handbook)\n\n\n\n\n16.2.2 Reproduzierbarkeit in der Praxis: rMarkdown\n\n\n\n\n\n\nHands-on: rMarkdown selber erstellen\n\n\n\nFür diesen Teil benötigen Sie ein RProject und die Daten, die Sie erhoben haben:\n\nErstellen Sie ein R Markdown file in R.\nSpeichern Sie dieses file in den selben Ordner als Ihr R Projekt.\nGeben Sie einen Titel und Datum ein.\nKopieren Sie Ihren Code aus Übung 3 und unterteilen Sie ihn in verschiedene code chunks.\nKommentieren Sie jeden chunk und erstellen Sie eine Outline.\nKlicken Sie auf Knit to HTML.\nÄndern Sie die Chunk Optionen “include” und “echo”, und klicken Sie wieder auf Knit to HTML. Was bewirkt das? (https://rmarkdown.rstudio.com/lesson-3.html)\n\n\n\nMehr Infos hier: https://rmarkdown.rstudio.com/index.html\n\n\n\nsource: https://imgflip.com/i/1v1jxs",
    "crumbs": [
      "Open Science",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Getting Started: Open Science and Good Practices</span>"
    ]
  },
  {
    "objectID": "opensci.html#versionskontrolle-zusammenarbeit-mit-git",
    "href": "opensci.html#versionskontrolle-zusammenarbeit-mit-git",
    "title": "16  Getting Started: Open Science and Good Practices",
    "section": "16.3 Versionskontrolle & Zusammenarbeit mit Git",
    "text": "16.3 Versionskontrolle & Zusammenarbeit mit Git\nHatten Sie schon einmal eine Datei mit einem Namen wie: final_final_v4_REAL?\n\nGit verfolgt jede Änderung, die Sie an Dateien vornehmen – wie eine Zeitmaschine für Ihr Projekt.\nEs ist eine Cloud-Plattform, um Projekte zu speichern, zu teilen und gemeinsam daran zu arbeiten.\n\n\n\n\n\n\n\nHands-on: Git vs Google Drive/Docs\n\n\n\n\nGehen Sie zur Dokumentation von GitHub oder GitLab\nDiskutieren Sie: Was sind Unterschiede zwischen Git und Google Drive oder anderen “klassischen” Cloud-Plattformen?",
    "crumbs": [
      "Open Science",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Getting Started: Open Science and Good Practices</span>"
    ]
  },
  {
    "objectID": "opensci.html#preregistration",
    "href": "opensci.html#preregistration",
    "title": "16  Getting Started: Open Science and Good Practices",
    "section": "16.4 Preregistration",
    "text": "16.4 Preregistration\n\n16.4.1 Was ist Preregistration ?\n\nEin Dokument, das Ihr Forschungsdesign und Ihre Analyse im Voraus festlegt – also bevor Sie das Projekt tatsächlich beginnen (z. B. bevor Sie mit der Datenerhebung starten)\nEs hilft dabei, Ihre Wissenschaft transparenter und vertrauenswürdiger zu machen\nEs schützt vor P-Hacking und kognitiven Verzerrungen\n\nFür weitere Informationen: https://help.osf.io/article/330-welcome-to-registrations\n\n\n\n\n\n\nHands-on: Preregistration selber machen\n\n\n\n\nGehen Sie auf https://aspredicted.org/\nKreuzen Sie Just trying it out an\nKlicken Sie auf Create a new pre-registration\nGeben Sie Ihre Unibe-Email ein und Continue\nWarten Sie, bis Sie einen Link bekommen und klicken Sie diesen Link\nKlicken Sie noch einmal auf I am just trying things out\nErstellen Sie eine Preregistrierung für Ihr Stroop oder Random-Dot Experiment.\n\n\n\n\nExtra Ressourcen:\n\nGood practices for scientific computing\nBuilding reproducible analytical pipelines with R",
    "crumbs": [
      "Open Science",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Getting Started: Open Science and Good Practices</span>"
    ]
  },
  {
    "objectID": "goodpractices_data.html",
    "href": "goodpractices_data.html",
    "title": "17  Good Practices in der Datenverarbeitung",
    "section": "",
    "text": "17.1 Herausforderungen in der Arbeit mit neurowissenschaftlichen Daten\nIn der neurowissenschaftlichen Forschung werden zunehmend sehr grosse und komplexe Datensätze generiert. Daten aus unterschiedlichen Datenerhebungsverfahren sollen miteinander verknüpft (aggregiert) werden, um neue Erkenntnisse zu gewinnen. Eine sehr häufige Kombination sind beispielsweise Verhaltens- und Bildgebungsdaten, wie es in vielen fMRI-Studien der Fall ist. Das erfordert Kenntnisse der unterschiedlichen Formate und Eigenschaften dieser Daten sowie Programmierkenntnisse um diese Daten möglichst automatisiert vorzuverarbeiten, zu verknüpfen, visualisieren und analysieren.",
    "crumbs": [
      "Open Science",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Good Practices in der Datenverarbeitung</span>"
    ]
  },
  {
    "objectID": "goodpractices_data.html#herausforderungen-in-der-arbeit-mit-neurowissenschaftlichen-daten",
    "href": "goodpractices_data.html#herausforderungen-in-der-arbeit-mit-neurowissenschaftlichen-daten",
    "title": "17  Good Practices in der Datenverarbeitung",
    "section": "",
    "text": "Definition Datenmanagement\n\n\n\n\n\n\n\nHands-on: Herausforderungen von neurowissenschaftlichen Daten\n\n\n\nLesen Sie den untenstehenden Abschnitt aus Pierré et al. (2024). Besprechen Sie in Gruppen, welche spezifischen Herausforderungen Datenmanagement, -vorverarbeitung und -analyse in den Neurowissenschaften bestehen.\n\n\n\nIncreasing complexity of neuroscience data\nOver the past 20 years, neuroscience research has been radically changed by two major trends in data production and analysis.\nFirst, neuroscience research now routinely generates large datasets of high complexity. Examples include recordings of activity across large populations of neurons, often with high resolution behavioral tracking (Steinmetz et al., 2019; Stringer et al., 2019; Mathis et al., 2018; Siegle et al., 2021; Koch et al., 2022), analyses of neural connectivity at high spatial resolution and across large brain areas (Scheffer et al., 2020; Loomba et al., 2022), and detailed molecular profiling of neural cells (Yao et al., 2023; Langlieb et al., 2023; Braun et al., 2022; Callaway et al., 2021). Such large, multi-modal data sets are essential for solving major questions about brain function (Brose, 2016; Jorgenson et al., 2015; Koch and Jones, 2016).\nSecond, the collection and analysis of such datasets requires interdisciplinary teams, incorporating expertise in systems neuroscience, engineering, molecular biology, data science, and theory. These two trends are reflected in the increasing numbers of authors on scientific publications (Wareham, 2016), and the creation of mechanisms to support team science by the NIH and similar research funding bodies (Cooke and Hilton, 2015; Volkow, 2022; Brose, 2016).\nThere is also an increasing scope of research questions that can be addressed by aggregating “open data” from multiple studies across independent labs. Funding agencies and publishers have begun to aggressively promote data sharing and open data, with the goals of improving reproducibility and increasing data reuse (Dallmeier-Tiessen et al., 2014; Tenopir et al., 2015; Pasquetto et al., 2017). However, open data may be unusable if scattered in a wide variety of naming conventions and file formats lacking machine-readable metadata.\nBig data and team science necessitate new strategies for how to best organize data, with a key technical challenge being the development of standardized file formats for storing, sharing, and querying datasets. Prominent examples include the Brain Imaging Data Structure (BIDS) for neuroimaging, and Neurodata Without Borders (NWB) for neurophysiology data (Teeters et al., 2015; Gorgolewski et al., 2016; Rübel et al., 2022; Holdgraf et al., 2019). The Open Neurophysiology Environment (ONE), best known from adoption by The International Brain Laboratory (The International Brain Laboratory et al., 2020, 2023), has a similar application domain to NWB, but a highly different technical design. (…)\nThese initiatives provide technical tools for storing and accessing data in known formats, but more importantly provide conceptual frameworks with which to standardize data organization and description in an (ideally) universal, interoperable, and machine-readable way. Pierré et al. (2024) (Preprint)\n\n\n17.1.1 Datenquellen, Datenformate und Interdisziplinariät\nNeurowissenschaftliche Daten von Verhaltensdaten zu Bildgebungsdaten stammen aus unterschiedlichsten Quellen und haben alle spezifische Eigenschaften, die in der Datenverarbeitung berücksichtigt werden müssen. Das bedeutet, dass sehr unterschiedliche Formate miteinander verknüpft werden müssen für die Analyse. Bei der Generierung von Daten sind oft auch Personen aus unterschiedlichen Fachrichtungen beteiligt, welche andere Hintergründe bezüglich Datenmanagement haben (z.B. bei fMRI Experimenten Fachpersonen aus den Bereichen Radiologie, Physik, Neurowissenschaften, Psychologie, Medizin, etc.). Auch bei der Analyse sind oft verschiedene Personen beteiligt, die alle darauf angewiesen sind, den Datensatz zu verstehen.\n\n\n17.1.2 Grosse und komplexe Datensätze\nNeurowissenschaftliche Datensätze sind oft sehr gross (pro Versuchsperson oft mehrere Gigabytes) und für die Speicherung und das Safety Back-up wird also viel Speicherplatz benötigt (mehrere Terrabytes). Komplexe Datensätze bedeutet, dass eine gute Dokumentation nötig ist, welche Variable welche Bedeutung hat und wo gegebenenfalls Änderungen gemacht wur#den.\n\n\n17.1.3 Umsetzung Open Science\nFalls dabei keine Persönlichkeitsrechte verletzt werden, sollten Daten möglichst zugänglich gemacht werden, um einerseits transparente Resultate zu ermöglichen und andererseits kann so ein Datensatz auch von anderen Forschenden für weitere Fragestellung verwendet werden. Dies ist aber nur möglich, wenn der Datensatz für Aussenstehende verständlich abgespeichert wurde.\n\n\n17.1.4 Zahlreiche Verarbeitungsschritte\nVon der Datenerhebung bis zur Datenanalyse werden die Daten oft ausführlich vorverarbeitet. Dazu gehören u.a. folgende Schritte (wobei jeder eigene Fehlerquellen beinhalten kann):\n\nImportieren der Rohdaten\nVerändern des Formats (z.B. .dcm -&gt; .nii in fMRI)\nIdentifikation von Missings\nAusschluss von ungültigen Messungen\nAnonymisierung\nWeglassen nicht benötigter Datenpunkte\nFehlende Werte ergänzen\nDuplizierungen identifizieren und löschen\nMetadaten hinzufügen\nDatensätze zusammenfügen\nSpalten teilen\nVariablen umbenennen\nVariablen normalisieren/standardisieren\nVariablen verändern (z.B. ms zu sec)\nVariablentypen anpassen\nVariablen recodieren\nNeue Variablen erstellen\nNeuer Datensatz abspeichern\n\n\n\n\n\n\n\nTipp: Datencheck\n\n\n\nBevor mit einem Datensatz gearbeitet wird, empfiehlt es sich den Datensatz anzuschauen und folgendes zu identifizieren:\n\nIn welchem Dateiformat ist der Datensatz gespeichert? z.B. in .csv, .xlsx oder anderen?`\nIn welchem Datenformat ist der Datensatz geordnet? (long oder wide oder mixed?)\nGibt es ein data dictionary mit Erklärungen zu den Variablen?\n\n\n\n\n\n17.1.5 Verschiedene Standards\nEs gibt zahlreiche Bemühungen Datenmanagement in den Neurowissenschaften zu vereinheitlichen. Viele verfügbare Standards und Tools machen die Datenverarbeitung aber nicht nur einfacher. Im nächsten Kapitel finden Sie deshalb eine Übersicht von Ansätzen, die uns sinnvoll und allgemein anwendbar erscheinen und sich in unseren Labors bewährt haben.\n xkcd Comic",
    "crumbs": [
      "Open Science",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Good Practices in der Datenverarbeitung</span>"
    ]
  },
  {
    "objectID": "goodpractices_data.html#good-practices-in-der-datenverarbeitung",
    "href": "goodpractices_data.html#good-practices-in-der-datenverarbeitung",
    "title": "17  Good Practices in der Datenverarbeitung",
    "section": "17.2 Good Practices in der Datenverarbeitung",
    "text": "17.2 Good Practices in der Datenverarbeitung\nIm Folgenden wird auf einige wichtige Good Practices in der Datenverarbeitung eingegangen. Wichtig sind hier die Reproduzierbarkeit der Datenverarbeitungs- und Datenanalysepipelines sowie die Sicherstellung der Datenqualität.\n\n17.2.1 Reproduzierbarkeit\nDie Replikationskrise hat in der Psychologie, aber auch in den kognitiven Neurowissenschaften ein Umdenken ausgelöst. Mit dem Ziel nachhaltigere Forschungsergebnisse zu erreichen sind verschiedene Begriffe wie Reproduzierbarkeit und Replizierbarkeit zu wichtigen Schlagworten geworden. Die Begrifflichkeiten werden verwirrenderweise aber oft unterschiedlich definiert und verwendet (Plesser (2018)).\n\n\n\n\n\n\n\n\n\n\nReplizierbarkeit (replicability) bedeutet, dass ein Experiment von einer anderen Forschungsgruppe mit einer neuen Stichprobe durchgeführt werden kann, und ähnliche oder dieselben Resultate hervorbringt, wie die Originalstudie. Wird eine Studie mehrmals repliziert, steigt die Wahrscheinlichkeit, dass kein Zufallsbefund vorliegt.\n\nReplicability refers to the ability of a researcher to duplicate the results of a prior study if the same procedures are followed but new data are collected. Cacioppo et al. (2015)\n\nReproduzierbarkeit (reproducibility) hängt eng mit der Replizierbarkeit zusammen. Der Begriff wird teilweise sehr allgemein verwendet, und bedeutet so dass Forschungsergebnisse wiederholt gefunden werden. Reproduzierbarkeit im engeren Sinn hingegen bezieht sich darauf, ob die durchgeführte Analyse wiederholt werden kann. Die Reproduzierbarkeit ist somit hoch, wenn Forschende die Daten und Datenanalyseskripts bereitstellen und andere Forschende damit dieselben Analysen durchführen können und zu gleichen Resultaten kommen.\n\nReproducibility refers to the ability of a researcher to duplicate the results of a prior study using the same materials as were used by the original investigator. That is, a second researcher might use the same raw data to build the same analysis files and implement the same statistical analysis in an attempt to yield the same results…. Reproducibility is a minimum necessary condition for a finding to be believable and informative. Cacioppo et al. (2015)\n\nUm die Begriffe zusammenzufassen schlugen Goodman, Fanelli, and Ionnidis (2016) vor von\n\nReproduzierbarkeit der Methoden (Daten und Prozesse können exakt wiederholt werden)\nReproduzierbarkeit der Resultate (andere Studien kommen auf dieselben Resultate) und\nReproduzierbarkeit der wissenschaftlichen Schlussfolgerung (bei Repetition der Analyse oder der Experimente werden dieselben Schlüsse gezogen)\n\nzu sprechen. Die Reproduzierbarkeit von Resultaten und Schlussfolgerungen ist hier nicht klar abgrenzbar vom Begriff der Replizierbarkeit. Grundsätzlich besteht das Ziel, dass in der Forschung möglichst viel Evidenz für eine Schlussfolgerung gesammelt werden kann. Dies gelingt, wenn die Prozesse transparent, fehlerfrei und wiederholbar sind.\n\n\n17.2.2 Hindernisse bei der Reproduzierbarkeit\nIn diesem Kurs beschäftigen wir uns vor allem mit der Reproduzierbarkeit der Methoden als Grundlage für solide Erkenntnisgewinne. Reproduzierbarkeit kann laut Nosek et al. (2022) vor allem aus zwei Gründen nicht gegeben sein: Weil die Daten/Skripte nicht zur Verfügung stehen, oder weil diese Fehler enthalten:\n\nIn principle, all reported evidence should be reproducible. If someone applies the same analysis to the same data, the same result should occur. Reproducibility tests can fail for two reasons. A process reproducibility failure occurs when the original analysis cannot be repeated because of the unavailability of data, code, information needed to recreate the code, or necessary software or tools. An outcome reproducibility failure occurs when the reanalysis obtains a different result than the one reported originally. This can occur because of an error in either the original or the reproduction study. Nosek et al. (2022)\n\nFührt die Reproduktion nicht zum selben Resultat, löst das Zweifel am Forschungsergebnis aus. Wenn die Reproduzierbarkeit am Prozess scheitert, etwa weil die Daten nicht vorhanden sind, kann kein Schluss gezogen werden, ob die Resultate stimmen.\n\nAchieving reproducibility is a basic foundation of credibility, and yet many efforts to test reproducibility reveal success rates below 100%. … Whereas an outcome reproducibility failure suggests that the original result may be wrong, a process reproducibility failure merely indicates that the original result cannot be verified. Either reason challenges credibility and increases uncertainty about the value of investing additional resources to replicate or extend the findings (Nuijten et al. 2018). Sharing data and code reduces process reproducibility failures (Kidwell et al. 2016), which can reveal more outcome reproducibility failures (Hardwicke et al. 2018, 2021; Wicherts et al. 2011). Nosek et al. (2022)\n\nDas Teilen von Daten und Datenverarbeitungsskripten erhöht demnach die Wahrscheinlichkeit, dass mögliche Fehler im Prozess gefunden werden, da auch andere Forschende die Daten/Skripts verwenden können. Das ist vorerst unangenehm, gehört aber zum wissenschaftlichen Prozess dazu. Die Normalisierung einer Fehlerkultur in diesem Bereich würde indirekt auch die Replizierbarkeit von Ergebnissen erhöhen.\n\nNeuroimaging experiments result in complicated data that can be arranged in many different ways. So far there is no consensus how to organize and share data obtained in neuroimaging experiments. Even two researchers working in the same lab can opt to arrange their data in a different way. Lack of consensus (or a standard) leads to misunderstandings and time wasted on rearranging data or rewriting scripts expecting certain structure. BIDS Website (2024)\n\n\n\n17.2.3 Tools für Reproduzierbarkeit\nFür reproduzierbare Forschung gibt es inzwischen viele gute Tools.\n\nOSF: Website der Open Science Foundation\nEine kostenfreie und unkomplizierte Möglichkeit Daten und Skripts zu teilen, und diese in Projekten abzulegen. Es lässt sich dafür sogar ein doi erstellen. Auch Formulare für eine Präregistration sind hier zu finden.\n\n\nFAIR Guiding Principles\nBeim Veröffentlichen von wissenschaftlichen Artikeln ist es empfohlen, die Daten (falls Anonymisierung möglich) sowie die Analyse-Skripte mitzuveröffentlichen.\nFür Datensätze gelten die FAIR Guiding Principles (Wilkinson et al. (2016)):\n\nF indability: Es ist klar unter welchen Umständen und wie die Daten zugänglich sind\nA ccessibility: Daten sind zugänglich bzw. es ist klar wo sie zu finden wären\nI nteroperability: Verwendbare Datenformate/strukturen\nR eusability: gute Beschreibung des Datensatzes/der enthaltenen Variablen\n\n\nHier finden Sie weitere Informationen zu FAIR.\n\n\n\nBIDS: Brain Imaging Data Structure\nFür Neuroimaging-Daten gibt es beispielsweise vorgegebene Konventionen, wie ein Datensatz und die Verarbeitungsskripts abgespeichert werden. Ein Beispiel dafür ist Brain Imaging Data Structure (BIDS). So können Datensätze mit einer für alle verständlichen Struktur veröffentlicht und geteilt werden. Gorgolewski et al. (2016)\n\nHier finden Sie weitere Informationen zu BIDS.\n\nHier sehen Sie ein Beispiel, wie ein fMRI Datensatz in BIDS Struktur umgewandelt wird:\n Gorgolewski et al. (2016)\n\n\n\n17.2.4 Coding: Best practices\nFür das Veröffentlichen von Analyseskripts eignen sich Formate wie RMarkdown in R, oder LiveScripts in MATLAB aber auch .r-Skripte sehr gut. Beim Teilen von Code erhöht sich die Reproduzierbarkeit, wenn dieser verständlich strukturiert und kommentiert ist.\nWichtig ist hier:\n\nDas Verwenden relativer Pfade (data/raw_data.csv statt C:/Users/IhrName/Desktop/Projekt/Versuch1/finaleDaten/raw_data.csv)\nKonsequentes Löschen “alter” Variablen sowie Testen, ob der Code in sich läuft\nVersionskontrolle: Entweder konsistentes Umbenennen oder mittels Github/Gitlab o.ä., Erstellen von Changelogs\nVor dem Veröffentlichen, lohnt es sich jemanden den Code ausführen lassen. So zeigt sich wo noch unklare Stellen sind, die Kommentare benötigen.\n\nBeim Kommentieren von Code sollte folgendes beachtet werden:\n\nKommentare sollten geschrieben werden, wenn der Code erstellt wird und laufend überarbeitet werden. Oft wird es sonst nicht nachgeholt.\nWenn man nicht genau kommentieren kann, was man im Code macht, dann ist evtl. der Code unklar, oder man versteht ihn noch nicht. Vielleicht kann man Variablennamen vereinfachen/präzisieren und es braucht weniger Kommentare?\nWenn Code von anderen kopiert wird, sollte die Quelle angegeben werden.",
    "crumbs": [
      "Open Science",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Good Practices in der Datenverarbeitung</span>"
    ]
  },
  {
    "objectID": "goodpractices_data.html#datenmanagement",
    "href": "goodpractices_data.html#datenmanagement",
    "title": "17  Good Practices in der Datenverarbeitung",
    "section": "17.3 Datenmanagement",
    "text": "17.3 Datenmanagement\nIn diesem Kapitel wird auf die Sicherstellung der Datenqualität, sowie die Wichtigkeit von Data collection plans und Data dictionaries eingegangen.\n\n17.3.1 Datenqualität\nDie folgenden sieben Indikatoren für gute Datenqualität wurden von C. Lewis (2024) zusammengefasst und sind hier zu finden.\n\n1. Analysierbarkeit\nDaten sollten in einem Format gespeichert werden in welchem sie analysiert werden sollen.\n\nDie Variablennamen stehen (nur!) in der ersten Zeile.\nDie restlichen Daten sind Werte in Zellen. Diese Werte sind analysierbar und Informationen sind explizit enthalten (keine leeren Felder, keine Farben als Information, nur 1 Information pro Variable).\n\n\n\n2. Interpretierbarkeit\n\nVariablennamen maschinenlesbar (kommen nur einmal vor, keine Lücken/spezielle Sonderzeichen/Farben, beginnen nicht mit einer Nummer, nicht zu lange)\nVariablennamen sind menschenlesbar (klare Bedeutung, konsistent formattiert, logisch geordnet)\nDatensätze sind in einem nicht-proprietären Format abgespeichert (z.B. .csv nicht .xlsx oder .sav)\n\n\n\n3. Vollständigkeit\n\nKeine Missings, keine Duplikationen\nEnthält alle erhobenen Variablen\nErklärung für alle Variablen in zusätzlichem File (z.B. in einem data dictionary)\n\n\n\n4. Validität\n\nVariablen entsprechen den geplanten Inhalten, d.h. Variablentypen und -werte stimmen mit Daten überein (z.B. numerisch/kategorial, range: 1-5)\nMissings sind unverwechselbar angegeben (z.B. nicht -99, sondern NA)\n\n\n\n5. Genauigkeit\n\nDaten sollten in sich übereinstimmen\nDaten sollten doppelt kontrolliert werden, wenn sie eingegeben werden\n\n\n\n6. Konsistenz\n\nVariablennamen sollten konsistent gewählt werden\nInnerhalb wie auch zwischen den Variablen sollten Werte konsistent kodiert und formattiert sein\n\n\n\n7. Anonymisiert\n\nAlle Identifikationsmerkmale, sowohl direkte (Name, Adresse, E.Mail oder IP-Adresse, etc.) sowie indirekte (Alter, Geschlecht, Geburtsdatum, etc.) sollten so anonymisiert/kodiert sein, dass keine Rückschlüsse auf Personen möglich sind\nBesondere Vorsicht bei: Offenen Fragen mit wörtlichen Antworten, extremen Werten, kleinen Stichproben/kleinen Zellgrössen (&lt;3), Diagnosen, sensiblen Studienthemen\n\nEs empfiehlt sich vor der definitiven Abspeicherung oder gar Veröffentlichung des Datensatzes, diese Punkte durchzuarbeiten und zu kontrollieren.\n\n\n\n17.3.2 Data collection plan\nDie Datenqualität kann erhöht werden, indem die obenstehenden Punkte schon bei der Planung der Datenerhebung beachtet und einbezogen werden. Wenn die Daten schon so eingegeben werden, werden die darauffolgenden Data Cleaning Schritte verkürzt und weniger Fehler passieren. Die Dateneingabe und -analyse sollte vor der Datenerhebung getestet und währenddessen immer wieder kontrolliert werden.\n\n\n17.3.3 Data dictionaries\nEin data dictionary ist eine Sammlung von Metadaten die beschreibt, welche Variablen ein Datensatz enthält und wie die Werte dieser Variable formattiert sind.\nWichtige Inhalte für die Anwendung in den Neurowissenschaftlichen Forschung sind z.B. Name, Definition, Typ, Wertebereich (values), angewandte Transformationen, Geltungsbereiche (universe) und Format fehlender Angaben (skip logic).\nBeispiel Data Dictionary\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nlabel\ntype\nvalues\nsource\ntransformations\nuniverse\nskip_logic\n\n\n\n\nparticipant identification\nid\ncharacter, factor\n“sub-001” - “sub-100”\nraw files\nanonymized variable “participant”\nall\nNA\n\n\nparticipant age\nage\ninteger\n18 - 55\nraw files\n\nall\nNA\n\n\nexperimental block\nblock\ninteger\n1, 2\nraw files\n\nall\nNA\n\n\n\n\n\n\n\n\n\nHands-on: Erstellen Data Dictionary\n\n\n\nErstellen Sie in kleinen Gruppen für eines unserer beiden Experimente ein Data Dictionary. Verwenden Sie hierfür die in den oben abgebildeten Beispielstabelle vorhandenen Spalten.\nTipp: Sie können die Tabelle auch ganz simpel in RMarkdown erstellen in dem Sie die Tabelle als Text (nicht im Codechunk) erstellen. Wie das funktioniert können Sie hier nachlesen.\n\n\n\nAusführlichere Informationen über das Erstellen von data dictionaries finden Sie z.B. hier und hier.\n\nInformationen zu Datentypen finden Sie hier oder in der untenstehenden Tabelle\nDatentypen\n\n\n\n\n\n\n\n\nDatentyp\nBeschrieb\nBeispiel\n\n\n\n\ninteger\nGanzzahliger Wert\n1, 2, 3, 29\n\n\ndouble, float\nZahl mit Kommastellen\n1.234, 89.3, 1.0\n\n\ncharacter, factor\nText oder Kategorie (nicht numerisch interpretiert)\nspeed, 0\n\n\nstring\nText oder Kategorie in “” oder ’’\n“sub-001”\n\n\ndatetime\nDatum, Zeitpunkt\n2024-04-13, 21.12.23, 00:45:45\n\n\nboolean\nEiner von 2 möglichen Werten\nFALSE, 0\n\n\n\n\n\n\n\nCacioppo, J. T., R. M. Kaplan, J. A. Krosnick, J. L. Olds, and H. Dean. 2015. “Social, Behavioral, and Economic Sciences Perspectives on Robust and Reliable Science.” Report of the Subcommittee on Replicability in Science Advisory Committee to the National Science Foundation Directorate for Social, Behavioral, and Economic Sciences.\n\n\nGoodman, Steven N., Daniele Fanelli, and John P. A. Ionnidis. 2016. “What Does Research Reproducibility Mean?” Science Translational Medicine 341. https://doi.org/10.1126/scitranslmed.aaf5027.\n\n\nGorgolewski, Krzysztof J., Tibor Auer, Vince D. Calhoun, R. Cameron Craddock, Samir Das, Eugene P. Duff, Guillaume Flandin, et al. 2016. “The Brain Imaging Data Structure, a Format for Organizing and Describing Outputs of Neuroimaging Experiments.” Scientific Data 3 (1): 160044. https://doi.org/10.1038/sdata.2016.44.\n\n\nNosek, Brian A, Tom E Hardwicke, Hannah Moshontz, Aurélien Allard, Katherine S Corker, Anna Dreber, Fiona Fidler, et al. 2022. “Replicability, Robustness, and Reproducibility in Psychological Science.” Annual Review of Psychology 73: 719–48. https://doi.org/10.1146/annurev-psych-020821-114157.\n\n\nPierré, Andrea, Tuan Pham, Jonah Pearl, Sandeep Robert Datta, Jason T. Ritt, and Alexander Fleischmann. 2024. “A Perspective on Neuroscience Data Standardization with Neurodata Without Borders.” arXiv. http://arxiv.org/abs/2310.04317.\n\n\nPlesser, Hans E. 2018. “Reproducibility Vs. Replicability: A Brief History of a Confused Terminology.” Frontiers in Neuroinformatics 11 (January): 76. https://doi.org/10.3389/fninf.2017.00076.\n\n\nWilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for Scientific Data Management and Stewardship.” Scientific Data 3 (1): 160018. https://doi.org/10.1038/sdata.2016.18.",
    "crumbs": [
      "Open Science",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Good Practices in der Datenverarbeitung</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html",
    "href": "rmarkdown.html",
    "title": "18  RStudio Projects und RMarkdown",
    "section": "",
    "text": "18.1 RStudio Projects\nRStudio Projekte ermöglichen vereinfachtes Arbeiten mit R, da alle Dateien, die in diesem Projektordner gespeichert sind, direkt verfügbar sind. So kann die aktuelle R Session abgespeichert werden und beim nächsten Öffnen kann dort weitergearbeitet werden wo man aufgehört hat. Projekte ermöglichen somit ein stabiles working directory für ein Datenanalyse-Projekt.\nEs empfiehlt sich bei RProjekten eine Einstellungsänderung (Tools&gt; Project Options...) vorzunehmen, so dass die aktuell gespeicherten Variablen bei jedem Schliessen vom Projekt gelöscht werden. Dies verhindert, dass der aktuelle Code nur aufgrund früherer Speicherung läuft.",
    "crumbs": [
      "Open Science",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>RStudio Projects und RMarkdown</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html#rstudio-projects",
    "href": "rmarkdown.html#rstudio-projects",
    "title": "18  RStudio Projects und RMarkdown",
    "section": "",
    "text": "Hands-on: Erstellen eines RStudio Projects\n\n\n\n\nÖffnen Sie RStudio.\nErstellen Sie ein neues RStudio-Project\n\nKlicken Sie dafür auf File &gt; New Project\nBenennen Sie das Project neurosci_complab_rmarkdown und speichern Sie es an einem sinnvollen Ort auf Ihrem Computer.",
    "crumbs": [
      "Open Science",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>RStudio Projects und RMarkdown</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html#r-markdown",
    "href": "rmarkdown.html#r-markdown",
    "title": "18  RStudio Projects und RMarkdown",
    "section": "18.2 R Markdown",
    "text": "18.2 R Markdown\nR Markdown ist eine simple Markup-Sprache, die es ermöglicht reproduzierbare Data Reports zu erstellen. R Markdown ist praktisch, weil Text und Code gleichzeitig verwendet werden kann und der Output des Codes angezeigt wird. Text kann verwendet werden, ohne dass vor jeder Zeile ein # zum Auskommentieren gesetzt werden. Zudem kann Text formatiert werden, also z.B. dick oder kursiv dargestellt werden oder Auflistungen beinhalten.\nDas Skript der R Markdown Files wird geknittet und so - je nach Wahl - zu einem HTML, PDF oder Word-Dokument zusammengefügt. R Markdown kann also beispielsweise eine Text-Beschreibung, Code zum Erstellen einer Grafik und auch die erstellte Grafik in einem Dokument kombinieren.\n\nMit R Markdown können auch Präsentationen, Webseiten, Bücher, Lebensläufe, Artikel oder Arbeiten erstellt werden. Hier finden Sie eine Galerie. Die Kurswebsite wurde ebenfalls in R erstellt.\n\nDas working memory innerhalb eines R Markdown Files ist der Speicherort des Files. Von dort aus können somit relative Pfade angegeben werden. Das hat den Vorteil, dass andere Benutzer denselben Projekt-Ordner auf Ihrem Computer mit unveränderten Pfaden verwenden können.\n\nWeitere Infos zu Pfaden in R Markdown.",
    "crumbs": [
      "Open Science",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>RStudio Projects und RMarkdown</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html#r-markdown-file-erstellen-und-ausführen",
    "href": "rmarkdown.html#r-markdown-file-erstellen-und-ausführen",
    "title": "18  RStudio Projects und RMarkdown",
    "section": "18.3 R Markdown File erstellen und ausführen",
    "text": "18.3 R Markdown File erstellen und ausführen\n\n\n\n\n\n\nHands-on: R Markdown File in einem Projekt erstellen\n\n\n\n\nErstellen Sie ein neues .Rmd-File (File &gt; New File &gt; R Markdown).\nGeben Sie einen Titel und Ihren Namen ein und wählen Sie HTML als Output-Format.\nSpeichern Sie dieses Dokument unter dem Namen rmarkdown_exampleab.\n\nWelches Format (Endung) hat das abgespeicherte R Markdown Skript nun in Ihrem Ordner?\n\n\n\nWeiterführende Informationen:\n👉 Einführung in die Verwendung von R/RStudio/Notebooks im Rahmen des Psychologie Studiums von Andrew Ellis und Boris Mayer Einführung in R\n👉 Sehr kompakte, praxisnahe Einführung in R Markdown von Danielle Navarro (Slidedeck in englisch) Einführung in R Markdown",
    "crumbs": [
      "Open Science",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>RStudio Projects und RMarkdown</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html#knitten",
    "href": "rmarkdown.html#knitten",
    "title": "18  RStudio Projects und RMarkdown",
    "section": "18.4 Knitten",
    "text": "18.4 Knitten\nMit Knit wird das R Markdown Skript ausgeführt und eine zusätzliche Datei wird erstellt, z.B. ein html-File.\n\n\n\n\n\n\nHands-on: Knitten\n\n\n\n\n\nFühren Sie das File mit Knit aus und vergleichen Sie das R Markdown Skript mit dem Output den Sie erhalten haben. Was fällt Ihnen auf?\n\nWas ist nicht mehr zu sehen?\nWas ist zusätzlich zu sehen?\nWas hat sich im Projekt-Ordner verändert?\n\n\n\n\n\n\n\n\n\n\nTipp\n\n\n\n\n\nIm geknitteten Dokument ist der YAML-header nicht mehr zu sehen. Ebenfalls sieht man die “Umrandungen”, also die r Einfassungen der Code-Snippets, die Markdown-Befehle zum Einfügen von Grafiken, Mathematischen Formulierungen, etc. nicht mehr.\nNeu sieht man den Output des Codes. Je nach Einstellungen Code und Fehler-/Warnmeldungen und gerenderte Inhalte (Bilder, Mathematische Formeln, etc.).\nIm Projektordner wird ein .html-File erstellt.",
    "crumbs": [
      "Open Science",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>RStudio Projects und RMarkdown</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html#yaml-header",
    "href": "rmarkdown.html#yaml-header",
    "title": "18  RStudio Projects und RMarkdown",
    "section": "18.5 YAML header",
    "text": "18.5 YAML header\nAm Anfang des R Markdown Skripts befindet sich der YAML header. Hier werden Informationen zu Titel, Autor:Innen, Datum, Outputformat, Literaturverzeichnis und Layout festgelegt.\n\nYAML: Yet Another Markdown Language\n\nDas Layout kann unter themegeändert werden. Das kann beispielsweise wie folgt aussehen:\noutput:\n  html_document:\n    theme: cosmo\nAchtung: Die Einrückungen müssen genau stimmen! Hier wurde das theme namens cosmo ausgewählt. Mögliche andere themessind z.B. default, cerulean, journal, flatly, darkly, readable, spacelab, united, cosmo, lumen, paper, sandstone, simplex, yeti.\n\n\n\n\n\n\nHands-on: Namen und Einstellungen anpassen\n\n\n\n\n\n\nGeben Sie dem Dokument einen neuen Titel z.B. R Markdown Einführung\nÄndern Sie das Layout so, dass es Ihnen gefällt.",
    "crumbs": [
      "Open Science",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>RStudio Projects und RMarkdown</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html#text-erstellen-in-r-markdown",
    "href": "rmarkdown.html#text-erstellen-in-r-markdown",
    "title": "18  RStudio Projects und RMarkdown",
    "section": "18.6 Text erstellen in R Markdown",
    "text": "18.6 Text erstellen in R Markdown\nText kann in R Markdown Files nicht nur geschrieben, sondern auch relativ simpel formatiert werden.\n👉 Hier können Sie das Cheatsheet herunterladen. Auf der rechten Seite finden Sie die Informationen für die Textformatierung.\nEs empfiehlt sich das Skript anfangs häufig zu knitten, so findet man den Fehler schneller, weil man noch weiss, was man als letztes verändert hat. Code kann aber auch einfach innerhalb der Code-Chunks überprüft werden.\n\n\n\n\n\n\nHands-on: Texte, Formeln und Bilder in R Markdown einfügen\n\n\n\n\n\n\nLöschen Sie alles bis auf den YAML-Header\nSchreiben Sie im Textbereich eine Überschrift für ein Kapitel, ein Unterkapitel und normalen Text.\nSchreiben Sie im Text etwas kursiv und etwas fett.\nErstellen Sie im Textbereich eine Liste mit 3 Punkten.\nSchreiben Sie alpha innerhalb von $, was passiert?\nFügen Sie die untenstehende Formel in den Text ein. Verwenden Sie dafür zwei Dollarzeichen am Anfang und am Ende. Was passiert?\n\n\\[\na^2 + b^2 = c^2\n\\]\n\nFügen Sie einen Link ein, knitten Sie das File und schauen Sie ob der Link funktioniert. Können Sie einen Link nur mit einem unterstrichenen Text anzeigen, so dass die Linkadresse nicht sichtbar ist?\nFügen Sie ein Bild ein. Speichern Sie dieses Bild zuerst in einem Ordner namens img im Projektordner.\n\n\n\n\n\nHier finden Sie weiterführende Hilfe zum Einfügen von Mathnotationen.\n\nMathematics in RMarkdown\nRMarkdown: The definitive Guide",
    "crumbs": [
      "Open Science",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>RStudio Projects und RMarkdown</span>"
    ]
  },
  {
    "objectID": "rmarkdown.html#code-erstellen-in-r-markdown",
    "href": "rmarkdown.html#code-erstellen-in-r-markdown",
    "title": "18  RStudio Projects und RMarkdown",
    "section": "18.7 Code erstellen in R Markdown",
    "text": "18.7 Code erstellen in R Markdown\nCode muss jeweils in einem Code-Chunk eingefügt werden. Ein Code-Chunk kann unter Code &gt; Insert Chunk eingefügt werden oder mit dem Kürzel Ctrl+Alt+ I.\nCode-Chunks werden mit ``` begonnen und beendet. In den geschweiften Klammern steht r, das bedeutet dass der Code in R geschrieben ist. In dieser Klammer kann dem Code-Chunk einen Namen gegeben und bestimmt werden, ob der Code ausgeführt und z.B. nur angezeigt werden soll, sowie ob der Output des Codes angezeigt werden soll.\nMit dem grünen Pfeil kann der Code-Chunk einzeln ausgeführt werden. Aber auch einzelne Zeilen können ausgeführt werden, genau so wie in einem .R- Skript.\n\n\n\n\n\n\nHands-on: Code in R Markdown einfügen\n\n\n\n\n\n\nErstellen Sie einen Code-Chunk, der ausgeführt, aber nicht angezeigt wird. Erstellen Sie eine Variable mit dem Namen numbers, die 10 Zahlen enthält.\nErstellen Sie ein Code-Chunk, der ausgeführt wird und dessen Output angezeigt wird. Berechnen Sie in diesem Chunk den Mittelwert und die Standardabweichung von numbers.\nErstellen Sie einen Plot mit plot(numbers).\nKnitten Sie das File, um zu überprüfen, ob alles funktioniert\n\n👉 Schauen Sie für Hilfe nochmals im Cheatsheet nach oder drücken Sie auf das Zahnrädchen-Symbol beim Code-Chunk.\nFür Fortgeschrittene:\n\nTesten Sie, ob Sie Ihr File auch zu einem PDF knitten können.\nErstellen Sie eine Tabelle\nErstellen Sie einen Glossar\nErstellen Sie ein Dokument mit Reitern oben (z.B. Data, Preprocessing, Analysis, Conclusions)\nFügen Sie interaktive Elemente ein.",
    "crumbs": [
      "Open Science",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>RStudio Projects und RMarkdown</span>"
    ]
  },
  {
    "objectID": "intro_eeg.html",
    "href": "intro_eeg.html",
    "title": "19  Einführung EEG",
    "section": "",
    "text": "19.1 Was ist EEG?\nElektroenzephalographie (EEG) ist eine nicht-invasive Methode zur Messung elektrischer Aktivität im Gehirn. Dabei werden Elektroden auf der Kopfhaut angebracht, um Spannungsänderungen zu erfassen, die durch neuronale Aktivität entstehen.",
    "crumbs": [
      "EEG",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Einführung EEG</span>"
    ]
  },
  {
    "objectID": "intro_eeg.html#was-ist-eeg",
    "href": "intro_eeg.html#was-ist-eeg",
    "title": "19  Einführung EEG",
    "section": "",
    "text": "19.1.1 Was misst EEG?\nEEG misst vor allem die summierten postsynaptischen Potentiale von großen Gruppen von Nervenzellen, insbesondere den pyramidalen Neuronen im Kortex. Diese Neuronen liegen senkrecht zur Kopfhaut und erzeugen elektrische Felder, wenn sie aktiviert werden. Die gemessenen Signale stammen nicht von einzelnen Aktionspotentialen, sondern von der koordinierten Aktivität vieler Neuronen.\n\n\n19.1.2 Wichtige Eigenschaften des EEG\n\nHohe zeitliche Auflösung im Millisekundenbereich\nNicht-invasiv und relativ kostengünstig\nMisst elektrische Aktivität direkt\nSensibel für Prozesse im Kortex, aber weniger für tieferliegende Strukturen\nInverse Problem: die Schwierigkeit, aus den EEG-Signalen auf der Kopfhaut eindeutig auf die zugrunde liegenden Quellen im Gehirn zu schließen.",
    "crumbs": [
      "EEG",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Einführung EEG</span>"
    ]
  },
  {
    "objectID": "intro_eeg.html#grundlegende-schritte-der-eeg-analyse-bis-zu-erps",
    "href": "intro_eeg.html#grundlegende-schritte-der-eeg-analyse-bis-zu-erps",
    "title": "19  Einführung EEG",
    "section": "19.2 Grundlegende Schritte der EEG-Analyse (bis zu ERPs)",
    "text": "19.2 Grundlegende Schritte der EEG-Analyse (bis zu ERPs)\n\n19.2.1 Was sind ERPs?\n\n\n\n\n\nsource: Beres, A.M. Time is of the Essence: A Review of Electroencephalography (EEG) and Event-Related Brain Potentials (ERPs) in Language Research. Appl Psychophysiol Biofeedback 42, 247–255 (2017). https://doi.org/10.1007/s10484-017-9371-3\nEvent-related potentials (ERPs) sind zeitlich mit einem bestimmten Event verknüpfte Veränderungen im EEG-Signal, die durch Mittelung über viele ähnliche Ereignisse gewonnen werden. Sie zeigen typische Wellenformen, die Rückschlüsse auf Wahrnehmung, Aufmerksamkeit oder Entscheidungsprozesse erlauben.\n\n\n19.2.2 ERP Analyse\nDie Analyse von EEG-Daten erfolgt in mehreren Schritten, um aus dem Rohsignal Event-Related Potentials (ERPs) zu extrahieren. Hier ein Überblick über die wichtigsten Analysephasen:\n\n1. Datenimport\n\nLaden der Rohdaten aus dem EEG-Aufzeichnungssystem\nEnthält Informationen zu Kanälen, Sampling-Rate und Markern (Events)\n\n\n\n2. Vorverarbeitung (Preprocessing)\n\nFilterung: Entfernen von Rauschen, z. B. tieffrequente Drift (High-Pass) oder Muskelartefakte (Low-Pass)\nRe-Referenzierung: Wahl einer gemeinsamen Referenz (z. B. Durchschnitt aller Elektroden)\nArtefaktentfernung: Ausschluss oder Korrektur von Störungen wie Augenbewegungen oder Blinzeln (z. B. mithilfe von ICA)\n\n\n\n3. Event-Markierung\n\nIdentifikation relevanter Zeitpunkte im EEG (z. B. Reizbeginn, Tastendruck)\nDiese Marker definieren die Zeitfenster für weitere Analysen\n\n\n\n\n\n\nsource: Martínez Beltrán, E.T., Quiles Pérez, M., López Bernal, S. et al. Noise-based cyberattacks generating fake P300 waves in brain–computer interfaces. Cluster Comput 25, 33–48 (2022). https://doi.org/10.1007/s10586-021-03326-z\n\n\n4. Epochierung\n\nUnterteilung des kontinuierlichen EEG in kurze Zeitabschnitte (Epochen) rund um das Event (z. B. -200 bis +800 ms)\nJede Epoche repräsentiert die Reaktion auf ein einzelnes Event\n\n\n\n5. ERP-Berechnung\n\nMitteln der Epochen über viele Trials pro Bedingung\nDas resultierende ERP zeigt zeitlich stabile, wiederholbare Hirnreaktionen auf spezifische Events\n\n\n\n6. Visualisierung und Interpretation\n\nDarstellung der ERPs als Wellenformen (z. B. über die Zeit an bestimmten Elektroden)\nIdentifikation typischer Komponenten wie P1, N1, P300 usw.\nVergleich von Bedingungen oder Gruppen mit statistischen Tests",
    "crumbs": [
      "EEG",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Einführung EEG</span>"
    ]
  },
  {
    "objectID": "modeling.html",
    "href": "modeling.html",
    "title": "20  Einführung",
    "section": "",
    "text": "20.1 Warum braucht es Modelle?\nNeurowissenschaftliche Daten werden selten nur durch einen Faktor beeinflusst. Im Gegenteil: Meist sind sehr viele verschiedene Einflussfaktoren beteiligt, und sobald diese auch noch miteinander verknüpft sind, kommen wir Menschen kognitiv sehr schnell an unsere Grenzen. Ab einer bestimmten Komplexität2 können wir nicht mehr anhand simpler Deskriptivstatistik verstehen, welcher Prozess die Daten generiert hat.\nDeshalb macht es Sinn sich Gedanken über den datengenerierenden Prozess zu machen und diesen zu modellieren. Ein Modell ist eine auf das wesentliche reduzierte Erklärung für einen Prozess. Ein passendes Modell ermöglicht es uns Daten zusammenzufassen, Abläufe zu verstehen, zukünftige Daten vorherzusagen und idealerweise sogar die Erklärung eines Prozesses. Wenn Sie den darunterliegenden datengenerierenden Prozess jedoch kennen, können Sie Daten erklären und auch zukünftige Zustände vorhersagen.",
    "crumbs": [
      "Datenmodellierung",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Einführung</span>"
    ]
  },
  {
    "objectID": "modeling.html#warum-braucht-es-modelle",
    "href": "modeling.html#warum-braucht-es-modelle",
    "title": "20  Einführung",
    "section": "",
    "text": "20.1.1 Game of Life\nEin gutes Beispiel dafür ist das Game of Life. Das Game of Life ist ein von Mathematiker John Horton Conway 1970 entworfenes Spiel bzw. Vorgehen. Hierbei besteht ein Raster mit inaktiven (weissen) sowie aktiven (schwarzen) Zellen. Die Zellinhalte entwickeln sich nach festgelegten, einfachen Regeln.\n\nEin anderes Beispiel dafür, dass wir intuitiv schnell falsch liegen, ist der Collider Bias3\n\n\nhttps://playgameoflife.com/\n\n\n\n\n\n\nHands-on: Whats going on in the game of life?\n\n\n\nSchauen Sie sich die Entwicklung der Punkte an. Ihre Aufgabe ist es den Prozess, der den Daten zugrundeliegt, zu beschreiben und erklären:\n\nWie gehen Sie vor um den zugrundeliegenden Prozess zu untersuchen?\nWie können Sie Deskriptivstatistik und Inferenzstatistik anwenden, die Sie im Studium bisher gelernt haben?\nEntdecken Sie wiederkehrende Muster?\nWelche Regeln können Sie erkennen?\nKönnen Sie, wenn Sie den Prozess anhalten, den nächsten Zustand vorhersagen?\n\n\n\nWir könnten also versuchen oben genannte Vorgehensweisen anzuwenden, um den Prozess “hinter” dem Game of Life zu verstehen:\nZuerst beobachten wir inaktive und aktive Zellen (Beobachtungsstudien) und in einem zweiten Schritt untersuchen wir die Zellen, indem wir systematisch in das Geschehen eingreifen (Experiment) in dem wir zum Start gewisse Zellen aktivieren oder deaktivieren.\nWir können versuchen aus den Daten Erkenntnisse zu gewinnen, in dem wir beispielsweise:\n\nDaten deskriptiv beschreiben: Schwarze Felder haben einen Anteil von 20%, weisse Felder einen Anteil von 80%.\nVergleichen verschiedener Gruppen/Bedingungen: Es gibt signifikant mehr weisse als schwarze Felder.\nZusammenhänge von Variablen berechnen: Je weiter unten ein Feld liegt, desto eher ist es schwarz.”\n\nWeiter fällt uns vielleicht auf, dass wir häufig folgende Konstellationen beobachten können:\n\n\n\nhttps://de.wikipedia.org/wiki/Conways_Spiel_des_Lebens\n\n\nDaher könnten wir die wiederkehrenden Muster zählen und beispielsweise über die Zeit hinweg vergleichen:\n\nWie oft taucht ein bestimmtes Muster im Verlauf auf?\nFolgen dieselben Muster immer aufeinander?\nKönnen wir in verschiedenen Bedingungen diese verschiedenen Muster häufiger auftauchen lassen?\n\nWenn Sie all dies getan haben:\n\nWie gut können Sie den nächsten Zustand des Systems vorhersagen, wenn Sie einen bestehenden Zustand kennen?\nVerstehen Sie den Prozess?\nKönnen Sie ihn erklären?\n\n\n\n\n\n\n\n\nRegeln des Game of Life\n\n\n\n\n\nDie Regeln des Game of Life lauten:\n\nEine aktive Zelle bleibt auch in der Folgegeneration aktiv (lebt weiter), wenn sie entweder zwei oder drei aktive Nachbarn hat.\nEine inaktive Zelle wird aktiv („wird geboren“/lebt in der Folgegeneration), wenn sie genau drei aktive Nachbarn hat.\n\nDaraus folgt:\n\nEine aktive Zelle mit keiner, einer oder mehr als drei aktiven Zellen um sich her wird inaktiv (“stirbt”).\nEine inaktive Zelle bleibt inaktiv, solange sie keine, eine, zwei oder mehr als drei aktive Zellen um sich her hat.\n\nHätten Sie aus diesen Regeln das Muster vorhersagen können ohne Zellen zu zählen und es aufzuzeichnen?",
    "crumbs": [
      "Datenmodellierung",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Einführung</span>"
    ]
  },
  {
    "objectID": "modeling.html#wie-wird-ein-prozess-modelliert",
    "href": "modeling.html#wie-wird-ein-prozess-modelliert",
    "title": "20  Einführung",
    "section": "20.2 Wie wird ein Prozess modelliert?",
    "text": "20.2 Wie wird ein Prozess modelliert?\n\nThere is little doubt that even before you started reading this book, you had already fit many models to data. No one who has completed an introductory statistics course can escape learning about linear regression. It turns out that every time you computed a regression line, you were actually fitting a model—namely, the regression line with its two parameters, slope and intercept—to the data. Lewandowsky & Farrell (2010, S. 71)\n\n\n20.2.1 Planetare Bewegung\nEin Beispiel dafür, wie ein datengenerierender Prozess über Jahrhunderte hinweg modelliert wurde, ist die planetare Bewegung: Menschen haben über sehr lange Zeit hinweg durch das Beobachten der Objekte im Himmel versucht zu verstehen, wie diese Bewegungen zustande kommen. Das folgende Beispiel basiert auf dem Buch von Lewandowsky & Farrell (2011).\nDie Daten die beobachtet wurden, waren die beobachtbaren Objekte im Himmel über die Zeit. Dies führte zu zwei Erkenntnissen:\n\neinige Planeten ändern plötzlich ihre Richtung\nnach einiger Zeit nehmen sie ihren ursprünglichen Weg wieder auf\n\n\n\n\n\n\n\n\n\n\n\nGeozentrisches Modell\n\n\n\n\n\n\n\nHeliozentrisches Modell\n\n\n\n\n\nWir sehen daran:\n\nDie Daten lassen sich nur mit einem Modell des zugrunde liegenden Prozesses erklären.\nModelle an sich können nicht beobachtet werden.\nEs gibt fast immer mehrere Modelle, welche die Daten erklären können\n\n\n\n\n\n\n\nHands-on: Modelle und Realität\n\n\n\nWas haben untenstehende Zitate und die Abbildung gemeinsam?\n\nAll models are wrong, some are useful. George Box (1976)\n\n\nThe map is not the territory. Gregory Bateson (1988)",
    "crumbs": [
      "Datenmodellierung",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Einführung</span>"
    ]
  },
  {
    "objectID": "modeling.html#summary",
    "href": "modeling.html#summary",
    "title": "20  Einführung",
    "section": "20.3 Summary",
    "text": "20.3 Summary\n\n“We suggest that purely verbal theorizing in cognition is increasingly inadequate in light of the growing richness of our data: whereas several decades ago decision-making tasks might have yielded only simple accuracy measures, we now have access not only to accuracy but also to the latencies of all response classes and their distributions. This richness defies verbal analysis but presents an ideal landscape for computational modeling. Indeed, we suggest that models also help avoid replication failures because the likelihood that an experiment will yield a quantitatively predicted intricate pattern of results involving multiple dependent variables by chance alone is surely lower than that a study might, by randomness alone, yield simple pairwise differences between conditions that happen to mesh with a verbally specified theoretical notion. Second, we consider the increasingly tight connection between modeling and the cognitive neurosciences to be a particularly promising arena. Measurement models, explanatory models, and cognitive architectures are now either directly neurally inspired, or they provide a conceptual bridge between behavioral data and their neural underpinnings. There is little doubt that this trend will continue in the future.” Lewandowsky & Oberauer (2018)",
    "crumbs": [
      "Datenmodellierung",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Einführung</span>"
    ]
  },
  {
    "objectID": "modeling.html#weiterführende-literatur",
    "href": "modeling.html#weiterführende-literatur",
    "title": "20  Einführung",
    "section": "20.4 Weiterführende Literatur",
    "text": "20.4 Weiterführende Literatur\n\nForstmann, B. U., & Wagenmakers, E. J. (2015). Model-based cognitive neuroscience: A conceptual introduction. In An introduction to model-based cognitive neuroscience (pp. 139-156). New York, NY: Springer New York.\nLewandowsky, S., & Farrell, S. (2010). Computational modeling in cognition: Principles and practice. Sage.\nLewandowsky, S., & Oberauer, K. (2018). Computational modeling in cognition and cognitive neuroscience. Stevens’ handbook of experimental psychology and cognitive neuroscience, 5, 1-35.\nLektüre für die Semesterferien: “Models of the Mind: How Physics, Engineering, and Mathematics Have Shaped Our Understanding of the Brain” von Grace Lindsay (2021)",
    "crumbs": [
      "Datenmodellierung",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Einführung</span>"
    ]
  },
  {
    "objectID": "modeling.html#footnotes",
    "href": "modeling.html#footnotes",
    "title": "20  Einführung",
    "section": "",
    "text": "https://andysbrainbook.readthedocs.io/en/stable/fMRI_Short_Course/fMRI_05_1stLevelAnalysis.html↩︎\nund diese ist extrem früh erreicht, wie das Beispiel Game of life mit nur wenigen Regeln zeigt↩︎\nhttps://www.the100.ci/2017/03/14/that-one-weird-third-variable-problem-nobody-ever-mentions-conditioning-on-a-collider/↩︎",
    "crumbs": [
      "Datenmodellierung",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Einführung</span>"
    ]
  },
  {
    "objectID": "sdt.html",
    "href": "sdt.html",
    "title": "21  Signalentdeckungstheorie",
    "section": "",
    "text": "21.1 %-Correct\nWeshalb ist die SDT nützlich? Dies kann am Beispiel des Masses %-Correct aufgezeigt werden. Das Berechnen von %-Correct, also dem Anteil richtiger Antworten innerhalb eines Experiments / einer Person / einer Bedingung, ist ein häufig genutztes Vorgehen, um zu quantifizieren, wie gut Personen eine Aufgabe lösen kann. Sie eignet sich jedoch nur als Mass für Sensitivität, wenn nicht zwischen der Sensitivität und der Antworttendenz unterschieden werden soll.\nFür unseren Random Dot-Datensatz können wir den Anteil richtiger Antworten jeder Versuchsperson aufgrund der gegebenen Antworten (resp) bzw. der Variable corr berechnen. Diese entspricht bei richtiger Antwort 1 und bei falscher Antwort 0. Wenn wir den Durchschnitt (mean()) der Variable corr berechnen, erhalten wir den Anteil korrekter Antworten. Wenn wir diese Zahl \\(\\cdot 100\\) rechnen, erhalten wir %-Correct.\nIm Experiment wurde die Instruktion (speed und accuracy) innerhalb der Versuchspersonen manipuliert. Es macht daher Sinn, für diese beiden Bedingungen je einen Wert pro Person zu berechnen.\nWir nehmen nicht an, dass sich im Experiment die “Sensitivität” verändert hat, also wie gut eine Person eine Aufgabe lösen kann, sondern viel mehr, dass sich ihr Entscheidungskriterium verändert hat durch die unterschiedliche Instruktion. Tatsächlich finden wir unseren Daten keinen Unterschied. Es könnte aber sein, dass eine Instruktion dazu führt, dass eine Antworttendenz stärker auf die Daten Einfluss nimmt, als in einer anderen. In diesem Fall würde vielleicht eine Antworttendenz von Personen eher rechts als links zu drücken eher Einfluss nehmen, wenn man wenig Zeit hat die Aufgabe zu lösen.\nWenn wir wissen möchten, ob die Aufgabe mit Speed-Instruktion gleich gut lösbar war wie die mit der Accuracy-Instruktion und gleichzeitig wissen möchten, ob die Personen eine Tendenz haben “rechts” oder “links” zu antworten, können wir die SDT anwenden.",
    "crumbs": [
      "Datenmodellierung",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Signalentdeckungstheorie</span>"
    ]
  },
  {
    "objectID": "sdt.html#correct",
    "href": "sdt.html#correct",
    "title": "21  Signalentdeckungstheorie",
    "section": "",
    "text": "Hands-on: %-Correct berechnen\n\n\n\n\nErstellen Sie ein neues R-Projekt namens complab_models. Erstellen Sie im Projektordner einen Ordner data.\nLaden Sie Daten des Random Dot Experiments hier herunter und speichern Sie diese im erstellten Projekt im Ordner data.\nÖffnen Sie ein neues R-Skript/RNotebook/RMarkdown signaldetection und lesen Sie die heruntergeladenen Daten ein.\nBerechnen Sie die Sensitivität für jede Versuchsperson individuell getrennt für die beiden Bedingungen speed und accuracy (vgl. Output unten).\n\n\nd_sens = |&gt;\n    ...\n    ...\n    ...\n\n\n\n# A tibble: 6 × 3\n# Groups:   id [3]\n  id      condition  sens\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n1 sub-007 accuracy   68.3\n2 sub-007 speed      66.7\n3 sub-010 accuracy  100  \n4 sub-010 speed     100  \n5 sub-011 accuracy   91.7\n6 sub-011 speed      90  \n\n\n\nDer resultierende Datensatz der berechneten Sensitivitäten ist im Long-Format. Überführen Sie den Datensatz in das Wide-Format, um die Daten einfacher verständlich zu machen (vgl. Output unten).",
    "crumbs": [
      "Datenmodellierung",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Signalentdeckungstheorie</span>"
    ]
  },
  {
    "objectID": "sdt.html#vorgehen-in-der-sdt",
    "href": "sdt.html#vorgehen-in-der-sdt",
    "title": "21  Signalentdeckungstheorie",
    "section": "21.2 Vorgehen in der SDT",
    "text": "21.2 Vorgehen in der SDT\nDie SDT ist eine statistische Entscheidungstheorie, deren zentrale Fragestellung lautet: Was ist der (unbekannte) Zustand der Welt, angesichts der verrauschten Daten?\nIn einem Modell macht es Sinn, sicherzustellen, dass das Problem einfach ist – wir beschränken die Welt auf zwei mögliche Zustände. Dies können beispielsweise sein:\n\npräsent – abwesend\nrechts – links\nneu/unbekannt – alt/bekannt (in einem Gedächtnisparadigma)\n\nWir werden das Vorgehen in der SDT anhand unserer eigenen Random Dot Experiment Daten nachvollziehen. Hierfür werden wir das Experiment aus zwei Perspektiven betrachten:\n\naus der Perspektive einer Person, welche die Aufgabe hat, Stimuli in zwei Klassen zu klassifizieren und\naus der Perspektive eines Modells, das die Leistung der Person in der Aufgabe vorhersagt.\n\n\n21.2.1 Die Perspektive der Versuchsperson\nWir betrachten ein Experiment, bei dem eine Person einen Stimulus in eine von zwei möglichen Kategorien einordnen muss. Das Random Dot Experiment beinhaltete die Stimuluskategorien Bewegung nach rechts und Bewegung nach links. Die Aufgabe der Person war es, eine binäre Klassifikation mit den Antwortoptionen rechts und links durchzuführen. Die Antwortoptionen entsprechen den beiden möglichen Zuständen der “Welt”, oder genauer gesagt, Hypothesen der Person über die möglichen Zustände der Welt.\n\nAnnahmen\n\nDie Person verarbeitet den Stimulus und gelangt zu einer internen Repräsentation des Stimulus. Diese interne Repräsentation ist nicht deterministisch, sondern variiert zufällig und ist demzufolge eine Zufallsvariable \\(X\\). Wir nehmen an, dass die interne Repräsentation normalverteilt ist.\nDie Zufallsvariable \\(X\\) repräsentiert die Information, die die Person über den Stimulus hat, also die Evidenz.\nDie Person weiss, dass \\(X\\) aus einer von zwei Verteilungen gezogen wurde, die sich nur in ihrer Lage (in ihrem Mittelwert) unterscheiden. Welche Verteilung es war, weiss die Person jedoch nicht – dies muss sie anhand eines Kriteriums entscheiden.\nDie Person hat ein Kriterium \\(k\\), das sie verwendet, um zu entscheiden, ob sich der Stimulus nach rechts oder links bewegt. Eine einfache Entscheidungsregel lautet: Wenn \\(X &gt; k\\), dann bewegen sich die Punkte nach rechts, andernfalls nach links.\n\n\n\n\nPlot adapted and modified from Vuorre (2017)2\n\n\n\n\n\n21.2.2 Die Perspektive des/der externen Beobachter*in\nDie Leistung der Versuchsperson kann durch die Wahrscheinlichkeit beschrieben werden, dass sie einen Treffer (Hit) oder einen False Alarm produziert. Diese Wahrscheinlichkeiten werden als Hit Rate und False Alarm Rate bezeichnet. Die Hit Rate ist die Wahrscheinlichkeit, dass die Person einen richtig liegt, wenn der Stimulus rechts ist. Die False Alarm Rate ist die Wahrscheinlichkeit, dass die Person einen Fehler macht, wenn der Stimulus links ist.\nDie Antworten der Versuchspersonen können in einer Tabelle zusammengefasst werden, mit vier möglichen Ergebnissen.3\n\n\n\n\nStimulus\n\n\n\n\n\nAntwort\nRechts\nLinks\n\n\nRechts\nHit\nFalse alarm (FA)\n\n\nLinks\nMiss\nCorrect rejection (CR)\n\n\n\n\nHit: Stimulus ist rechts, Antwort ist rechts\nMiss: Stimulus ist rechts, Antwort ist links\nFalse alarm: Stimulus ist links, Antwort ist rechts\nCorrect rejection: Stimulus is links, Antwort ist links\n\n\n\n\n\n\n\nHands-on: Hit, False Alarm, Miss und Correct Rejection labeln\n\n\n\nSetzen Sie diese verbale Beschreibung in R-Code um.\n\nErstellen Sie dazu im Random Dot Datensatz mit der Funktion mutate() eine Variable type. In dieser Variable soll für jeden Trial stehen, ob es sich um einen Hit, einen Miss, einen FA oder eine CR handelt (vgl. Daten unten).\n\n\nsdt &lt;- d |&gt;\n    select(id, stimulus = direction, resp) |&gt;\n    mutate(type = case_when(\n        direction == ... & resp == ... ~ ...,\n        ...\n        ...\n        ...))\n\n\n\n       id trial direction condition corrAns  resp corr        rt type\n1 sub-007     1     right     speed   right right    1 0.6537022  Hit\n2 sub-007     2      left     speed    left right    0 0.5821858   FA\n3 sub-007     3      left     speed    left  left    1 1.3868371   CR\n4 sub-007     4     right     speed   right right    1 0.9550371  Hit\n5 sub-007     5      left     speed    left right    0 0.4990127   FA\n6 sub-007     6     right     speed   right  left    0 0.6926959 Miss",
    "crumbs": [
      "Datenmodellierung",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Signalentdeckungstheorie</span>"
    ]
  },
  {
    "objectID": "sdt.html#sdt-parameter-berechnen",
    "href": "sdt.html#sdt-parameter-berechnen",
    "title": "21  Signalentdeckungstheorie",
    "section": "21.3 SDT Parameter berechnen",
    "text": "21.3 SDT Parameter berechnen\nDie beiden wichtigsten Parameter der Signal-Detektionstheorie sind \\(d'\\) und \\(c\\).\n\n\\(d'\\) (d-prime) ist ein Mass für die Sensitivität eines Beobachters bei der Unterscheidung zwischen zwei Stimulusklassen. Ein größerer \\(d'\\)-Wert zeigt eine grössere Sensitivität an. Dies bedeutet, dass die Verteilungen der beiden Stimulusklassen stärker voneinander getrennt sind und somit leichter unterscheidbar sind.\n\\(c\\) (criterion) ist ein Mass dafür, ob eine Voreingenommenheit (bias) für eine der beiden Antwortoptionen besteht. Genauer gesagt ist \\(c\\) der Abstand vom tatsächlichen Kriterium zum Punkt welcher genau zwischen den Verteilungen liegt.\n\n\n\n\n\n\n\nHands-on: Fragestellung\n\n\n\nWas bedeuten \\(d'\\) und \\(c\\) in unserem Beispiel? Welche Fragestellung(en) können wir untersuchen?\n\n\nUm \\(d'\\) und \\(c\\) zu erhalten, berechnen wir zuerst die Hit Rate und die False Alarm Rate, \\(z\\)-transformieren diese und nehmen dann die Differenz.\n\\[d' = z(Hit~Rate) - z(FA~Rate)\\]\n\n\\[c = -0.5 * (z(Hit~Rate) + z(FA~Rate))\\]\n\nRelative Häufigkeiten von Hits und False Alarms berechnen\nUm \\(d'\\) und \\(c\\) aus den beobachteten Antworthäufigkeiten zu berechnen, müssen wir zuerst die relativen Häufigkeiten der Hits (Hit Rate) und der False Alarms (FA Rate) berechnen.\nDie Hits sind die rechts-Antworten auf rechts-Stimuli. Dies bedeutet, dass wir zählen, wie oft bei einem rechts Stimulus die Antwort rechts war. Die False Alarms sind die rechts-Antworten auf links-Stimuli. Dies bedeutet, dass wir zählen, wie oft bei einem links Stimulus die Antwort rechts war.\nUm \\(d'\\) und \\(c\\) für jede Vpn in beiden Instruktions Bedingungen zu berechnen, zählen wir die verschiedenen Antworttypen (vgl. Daten unten) pro Person und Bedingung.\n\n\n\n\n\n\nRelative Häufigkeiten von Hits und False Alarms berechnen\n\n\n\n\nsdt_summary &lt;- sdt |&gt;\n    group_by(..., ...) |&gt;\n    count(...)\n\nsdt_summary\n\n\n\n# A tibble: 539 × 4\n# Groups:   id, condition [140]\n   id      condition type      n\n   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt; &lt;int&gt;\n 1 sub-007 accuracy  CR       21\n 2 sub-007 accuracy  FA        9\n 3 sub-007 accuracy  Hit      20\n 4 sub-007 accuracy  Miss     10\n 5 sub-007 speed     CR       20\n 6 sub-007 speed     FA       10\n 7 sub-007 speed     Hit      20\n 8 sub-007 speed     Miss     10\n 9 sub-010 accuracy  CR       30\n10 sub-010 accuracy  Hit      30\n# ℹ 529 more rows\n\n\n\n\nVor dem Berechnen von \\(d'\\) und \\(c\\) müssen wir den Datensatz noch formatieren: Hierzu konvertieren wir den Datensatz von long zu wide, um alle vier Antworttypen in jeweils eigenen Variablen zu speichern.\n\nsdt_summary &lt;- sdt_summary |&gt;\n    pivot_wider(names_from = type, \n                values_from = n)\nsdt_summary\n\n# A tibble: 140 × 6\n# Groups:   id, condition [140]\n   id      condition    CR    FA   Hit  Miss\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 sub-007 accuracy     21     9    20    10\n 2 sub-007 speed        20    10    20    10\n 3 sub-010 accuracy     30    NA    30    NA\n 4 sub-010 speed        30    NA    30    NA\n 5 sub-011 accuracy     26     4    29     1\n 6 sub-011 speed        26     4    28     2\n 7 sub-012 accuracy     29     1    29     1\n 8 sub-012 speed        30    NA    29     1\n 9 sub-015 accuracy     25     5    24     6\n10 sub-015 speed        22     8    25     5\n# ℹ 130 more rows\n\n\n\n\nNAs ersetzen\nWir erstellen eine Hilfsfunktion replace_NA() um alle fehlenden Werte (NA) durch 0 zu ersetzen.\n\nreplace_NA &lt;- function(x) {\n    x = ifelse(is.na(x), 0, x)\n    x\n}\n\n\nsdt_summary &lt;- sdt_summary |&gt;\n    mutate(across(c(Hit, Miss, FA, CR), replace_NA))\nsdt_summary\n\n# A tibble: 140 × 6\n# Groups:   id, condition [140]\n   id      condition    CR    FA   Hit  Miss\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n 1 sub-007 accuracy     21     9    20    10\n 2 sub-007 speed        20    10    20    10\n 3 sub-010 accuracy     30     0    30     0\n 4 sub-010 speed        30     0    30     0\n 5 sub-011 accuracy     26     4    29     1\n 6 sub-011 speed        26     4    28     2\n 7 sub-012 accuracy     29     1    29     1\n 8 sub-012 speed        30     0    29     1\n 9 sub-015 accuracy     25     5    24     6\n10 sub-015 speed        22     8    25     5\n# ℹ 130 more rows\n\n\n\n\nHit Rate und False Alarm Rate berechnen\nDie Hit Rate und die False Alarm Rate kann anhand der folgenden Formeln berechnet werden:\n\n\\[ Hit~Rate = \\frac{Hits}{Hits + Misses} \\]\n\n\\[ FA~Rate = \\frac{False Alarms}{False Alarms + Correct Rejections} \\] \n\n\n\n\n\n\nHands-on: Hit Rate und False Alarm Rate berechnen\n\n\n\n\nsdt_summary &lt;- sdt_summary |&gt;\n    mutate(hit_rate = ...,\n           fa_rate = ...)\nsdt_summary\n\n\n\n# A tibble: 140 × 8\n# Groups:   id, condition [140]\n   id      condition    CR    FA   Hit  Miss hit_rate fa_rate\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1 sub-007 accuracy     21     9    20    10    0.667  0.3   \n 2 sub-007 speed        20    10    20    10    0.667  0.333 \n 3 sub-010 accuracy     30     0    30     0    1      0     \n 4 sub-010 speed        30     0    30     0    1      0     \n 5 sub-011 accuracy     26     4    29     1    0.967  0.133 \n 6 sub-011 speed        26     4    28     2    0.933  0.133 \n 7 sub-012 accuracy     29     1    29     1    0.967  0.0333\n 8 sub-012 speed        30     0    29     1    0.967  0     \n 9 sub-015 accuracy     25     5    24     6    0.8    0.167 \n10 sub-015 speed        22     8    25     5    0.833  0.267 \n# ℹ 130 more rows\n\n\n\n\n\n\n0 und 1 ersetzen\nWir erstellen eine Hilfsfunktion correct_zero_one() mit der wir bei den Hit und False Alarm Rates alle 0 und 1 Werte durch 0.001 oder 0.999 ersetzen. Dies machen wir, damit wir bei der Berechnung der z-Werte nicht \\(\\pm \\infty\\) erhalten.\n\ncorrect_zero_one &lt;- function(x) {\n    if (identical(x, 0)) {\n        x = x + 0.001\n    } else if (identical(x, 1)) {\n        x = x - 0.001\n    }\n    x\n}\n\nFür den nächsen Schritt nutzen wir die Funktion correct_zero_one().\n\nsdt_summary &lt;- sdt_summary |&gt;\n    mutate(across(c(hit_rate, fa_rate), correct_zero_one))\nsdt_summary\n\n# A tibble: 140 × 8\n# Groups:   id, condition [140]\n   id      condition    CR    FA   Hit  Miss hit_rate fa_rate\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1 sub-007 accuracy     21     9    20    10    0.667  0.3   \n 2 sub-007 speed        20    10    20    10    0.667  0.333 \n 3 sub-010 accuracy     30     0    30     0    0.999  0.001 \n 4 sub-010 speed        30     0    30     0    0.999  0.001 \n 5 sub-011 accuracy     26     4    29     1    0.967  0.133 \n 6 sub-011 speed        26     4    28     2    0.933  0.133 \n 7 sub-012 accuracy     29     1    29     1    0.967  0.0333\n 8 sub-012 speed        30     0    29     1    0.967  0.001 \n 9 sub-015 accuracy     25     5    24     6    0.8    0.167 \n10 sub-015 speed        22     8    25     5    0.833  0.267 \n# ℹ 130 more rows\n\n\n\n\n\\(z\\)-Transformation\nAls nächstes müssen die \\(z\\)-Werte der Hit Rate und der False Alarm Rate berechnet werden. Dazu kann die Funktion qnorm()verwendet werden.\n\n\n\n\n\n\nHands-on: \\(z\\)-Transformation\n\n\n\n\nsdt_summary &lt;- sdt_summary |&gt;\n    mutate(zhr = ...,\n           zfa = ...)\nsdt_summary\n\n\n\n# A tibble: 140 × 10\n# Groups:   id, condition [140]\n   id      condition    CR    FA   Hit  Miss hit_rate fa_rate   zhr    zfa\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 sub-007 accuracy     21     9    20    10    0.667  0.3    0.431 -0.524\n 2 sub-007 speed        20    10    20    10    0.667  0.333  0.431 -0.431\n 3 sub-010 accuracy     30     0    30     0    0.999  0.001  3.09  -3.09 \n 4 sub-010 speed        30     0    30     0    0.999  0.001  3.09  -3.09 \n 5 sub-011 accuracy     26     4    29     1    0.967  0.133  1.83  -1.11 \n 6 sub-011 speed        26     4    28     2    0.933  0.133  1.50  -1.11 \n 7 sub-012 accuracy     29     1    29     1    0.967  0.0333 1.83  -1.83 \n 8 sub-012 speed        30     0    29     1    0.967  0.001  1.83  -3.09 \n 9 sub-015 accuracy     25     5    24     6    0.8    0.167  0.842 -0.967\n10 sub-015 speed        22     8    25     5    0.833  0.267  0.967 -0.623\n# ℹ 130 more rows\n\n\n\n\n\n\n\\(d'\\) und \\(c\\) berechnen\nNun können die SDT Parameter anhand der folgenden Formeln berechnet werden:\n\n\\[d' = z(Hit~Rate) - z(FA~Rate)\\]\n\n\\[c = -0.5 * (z(Hit~Rate) + z(FA~Rate))\\] \n\n\n\n\n\n\nHands-on: \\(d'\\) und \\(c\\) berechnen\n\n\n\n\nsdt_summary &lt;- sdt_summary |&gt;\n    mutate(dprime = ...,\n           c = ...) |&gt;\n    mutate(across(c(dprime, c), round, 2))\n\n\n\n# A tibble: 140 × 13\n# Groups:   id, condition [140]\n   id     condition    CR    FA   Hit  Miss hit_rate fa_rate   zhr    zfa dprime\n   &lt;chr&gt;  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 sub-0… accuracy     21     9    20    10    0.667  0.3    0.431 -0.524   0.96\n 2 sub-0… speed        20    10    20    10    0.667  0.333  0.431 -0.431   0.86\n 3 sub-0… accuracy     30     0    30     0    0.999  0.001  3.09  -3.09    6.18\n 4 sub-0… speed        30     0    30     0    0.999  0.001  3.09  -3.09    6.18\n 5 sub-0… accuracy     26     4    29     1    0.967  0.133  1.83  -1.11    2.94\n 6 sub-0… speed        26     4    28     2    0.933  0.133  1.50  -1.11    2.61\n 7 sub-0… accuracy     29     1    29     1    0.967  0.0333 1.83  -1.83    3.67\n 8 sub-0… speed        30     0    29     1    0.967  0.001  1.83  -3.09    4.92\n 9 sub-0… accuracy     25     5    24     6    0.8    0.167  0.842 -0.967   1.81\n10 sub-0… speed        22     8    25     5    0.833  0.267  0.967 -0.623   1.59\n# ℹ 130 more rows\n# ℹ 2 more variables: k &lt;dbl&gt;, c &lt;dbl&gt;\n\n\n\n\n\n\nNeuer Datensatz erstellen\nFür den finalen Datensatz wählen wir d' und c für jede Person in jeder Bedingung.\n\n\n\n\n\n\nHands-on: Datensatz erstellen\n\n\n\n\nsdt_final &lt;- sdt_summary |&gt;\n    select(...)\nsdt_final\n\n\n\n# A tibble: 140 × 4\n# Groups:   id, condition [140]\n   id      condition dprime     c\n   &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1 sub-007 accuracy    0.96  0.05\n 2 sub-007 speed       0.86  0   \n 3 sub-010 accuracy    6.18  0   \n 4 sub-010 speed       6.18  0   \n 5 sub-011 accuracy    2.94 -0.36\n 6 sub-011 speed       2.61 -0.2 \n 7 sub-012 accuracy    3.67  0   \n 8 sub-012 speed       4.92  0.63\n 9 sub-015 accuracy    1.81  0.06\n10 sub-015 speed       1.59 -0.17\n# ℹ 130 more rows",
    "crumbs": [
      "Datenmodellierung",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Signalentdeckungstheorie</span>"
    ]
  },
  {
    "objectID": "sdt.html#sdt-parameter-vergleichen",
    "href": "sdt.html#sdt-parameter-vergleichen",
    "title": "21  Signalentdeckungstheorie",
    "section": "21.4 SDT Parameter vergleichen",
    "text": "21.4 SDT Parameter vergleichen\nIn einem weiteren Schritt können die berechneten Parameter verglichen werden. So kann unterschieden werden, worauf sich die Instruktion ausgewirkt hat: Auf die Antworttendenz oder auf die Sensitivität?\nUm die SDT Parameter zwischen den Bedingungen zu vergleichen berechnen wir für \\(d'\\) und \\(c\\) den Mittelwert (mean()) und die Standardabweichung (sd()) in beiden Bedingungen (speed und accuracy).\n\n\n\n\n\n\nHands-on: Mittelwerte und SD berechnen, um Bedingungen zu vergleichen\n\n\n\n\ncs &lt;- sdt_final |&gt;\n    select(id, condition, c) |&gt;\n    Rmisc::summarySEwithin(measurevar = \"...\",\n                           withinvars = \"...\",\n                           idvar = \"...\",\n                           na.rm = FALSE,\n                           conf.interval = 0.95)\n\ndprimes &lt;- sdt_final |&gt;\n    select(id, condition, dprime) |&gt;\n    Rmisc::summarySEwithin(measurevar = \"...\",\n                           withinvars = \"...\",\n                           idvar = \"...\",\n                           na.rm = FALSE,\n                           conf.interval = 0.95)\n\n\n\nWenn alle Schritte ausgeführt wird, kann mit folgendem Code ein Plot erstellt werden:\n\nlibrary(patchwork)\n    \np_dprime &lt;- dprimes |&gt;\n    ggplot(aes(x = condition, y = dprime, group = 1)) +\n    geom_jitter(aes(condition, dprime), alpha = 0.1, data = sdt_final, width = 0.05) +\n    geom_line() +\n    geom_line(aes(condition, dprime, group = id), alpha = 0.05, data = sdt_final, width = 0.05) +\n    geom_errorbar(width = 0.1, aes(ymin = dprime - ci,\n                                   ymax = dprime + ci)) +\n    geom_point(shape = 21, size = 3, fill = \"white\") +\n    ggtitle(\"Sensitivity\") + theme_minimal()\n\np_bias &lt;- cs |&gt;\n    ggplot(aes(x = condition, y = c, group = 1)) + \n    geom_jitter(aes(condition, c), alpha = 0.1, data = sdt_final, width = 0.05) +\n    geom_hline(yintercept = 0, \n               linetype = \"dashed\",\n               color = \"grey60\") +\n    geom_line() +\n    geom_line(aes(condition, c, group = id), alpha = 0.05, data = sdt_final, width = 0.05) +\n    geom_errorbar(width = 0.1, aes(ymin = c - ci,\n                                   ymax = c + ci)) +\n    geom_point(shape = 21, size = 3, fill = \"white\") +\n    ggtitle(\"Bias\") + theme_minimal()\n\np_dprime + p_bias \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on: Was wissen wir nun?\n\n\n\nDiskutieren Sie zusammen die beiden Plots: Was können wir daran sehen?\nOb Unterschiede einer statistischen Untersuchung standhalten könnten wir z.B. mit einer ‘repeated-measures’ ANOVA untersuchen.",
    "crumbs": [
      "Datenmodellierung",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Signalentdeckungstheorie</span>"
    ]
  },
  {
    "objectID": "sdt.html#anwendungsbeispiele",
    "href": "sdt.html#anwendungsbeispiele",
    "title": "21  Signalentdeckungstheorie",
    "section": "21.5 Anwendungsbeispiele",
    "text": "21.5 Anwendungsbeispiele\nDie SDT wird in einem breiten Spektrum von Fragestellungen verwendet. Hier einige Beispiele:\n\nEntdecken von Fake-News: Batailler, C., Brannon, S. M., Teas, P. E., & Gawronski, B. (2021). A Signal Detection Approach to Understanding the Identification of Fake News. Perspectives on Psychological Science, 17(1), 78-98. https://doi.org/10.1177/1745691620986135\nGo/No-Go-Task und Alkohol: Ames, S. L., Wong, S. W., Bechara, A., Cappelli, C., Dust, M., Grenard, J. L., & Stacy, A. W. (2014). Neural correlates of a Go/NoGo task with alcohol stimuli in light and heavy young drinkers. Behavioural brain research, 274, 382-389. https://doi.org/10.1016/j.bbr.2014.08.039\nSomatic Signal Detection Task: Mirams, L., Poliakoff, E., Brown, R.J. et al. Vision of the body increases interference on the somatic signal detection task. Exp Brain Res 202, 787–794 (2010). https://doi.org/10.1007/s00221-010-2185-7\n\n\n\n\n\n\n\nHands-on: Anwendung der SDT\n\n\n\nSuchen Sie sich ein Paper mit einer SDT-Analyse oder wählen Sie eine der oben angegebenen Paper:\nLesen Sie den Abstract durch (und falls notwendig Teile des Papers):\n\nWas war das Signal? Was der Noise?\nWas entspricht \\(d'\\) und was \\(c\\)?\nWas waren die Findings?",
    "crumbs": [
      "Datenmodellierung",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Signalentdeckungstheorie</span>"
    ]
  },
  {
    "objectID": "sdt.html#weiterführende-informationen",
    "href": "sdt.html#weiterführende-informationen",
    "title": "21  Signalentdeckungstheorie",
    "section": "21.6 Weiterführende Informationen",
    "text": "21.6 Weiterführende Informationen\n\n\n\n\n\n\nWeitere Anwendungsbeispiele\n\n\n\nDas laufende Kursexperiment kann zwar mit SDT analysiert werden, es gibt jedoch weitaus spannendere Datensätze als den unseres Paradigmas:\n\nHier ein Beispiel eines Random Dot Experiments, bei dem den Personen vor den Dots Hinweisreize gezeigt wurde, in welche Richtung sich die Punktewolke am wahrscheinlichsten bewegen wird\nHier ein Beispiel der Analyse eines Datensatzes von einem Gedächtnisexperiment (Signal: schon gesehenes Gesicht, Rauschen: neues Gesicht)\n\n\n\n\nInteraktives Tutorial zu SDT: https://wise.cgu.edu/wise-tutorials/tutorial-signal-detection-theory/\nAusführlichere Einführung in die SDT: Stanislaw and Todorov (1999) und Macmillan and Creelman (2004)\nEinführung in die Verwendung von R zur Durchführung von SDT-Analysen: Knoblauch and Maloney (2012)",
    "crumbs": [
      "Datenmodellierung",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Signalentdeckungstheorie</span>"
    ]
  },
  {
    "objectID": "sdt.html#referenzen",
    "href": "sdt.html#referenzen",
    "title": "21  Signalentdeckungstheorie",
    "section": "21.7 Referenzen",
    "text": "21.7 Referenzen\n\nVuorre, Matti. 2017. “Estimating Signal Detection Models with Regression Using the Brms R Package.” PsyArXiv. https://doi.org/10.31234/osf.io/vtfc3_v1.",
    "crumbs": [
      "Datenmodellierung",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Signalentdeckungstheorie</span>"
    ]
  },
  {
    "objectID": "sdt.html#footnotes",
    "href": "sdt.html#footnotes",
    "title": "21  Signalentdeckungstheorie",
    "section": "",
    "text": "\\(z\\)-transformierte relative Häufigkeit der Treffer minus \\(z\\)-transformierte relative Häufigkeit der falschen Alarme↩︎\nhttps://vuorre.com/posts/sdt-regression/index.html#fig-sdt-example↩︎\nMeistens wird Signaldetektion im Rahmen von Signal vs. Rauschen verwendet, die beiden Verteilungen können aber wie hier auch “Bewegung nach Rechts” und “Bewegung nach links” unmfassen.↩︎",
    "crumbs": [
      "Datenmodellierung",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Signalentdeckungstheorie</span>"
    ]
  },
  {
    "objectID": "ddm.html",
    "href": "ddm.html",
    "title": "22  Drift Diffusion Modell",
    "section": "",
    "text": "22.1 DDM: Modell eines Entscheidungsprozesses\nIn unserem Random dot Experiment wurde neben der Antwort der Versuchspersonen (links, rechts) auch die Zeit (rt) gemessen, welche benötigt wurde um diese Antworten zu geben. Diese Information wurde in den vorherigen Modellen nicht gleichzeitig mit der Antwortgenauigkeit berücksichtigt.\nEin Modell mit dem solche Entscheidungsprozesse modelliert werden sind Drift-Diffusion-Modelle. Es geht davon aus, dass binäre Entscheidungen auf der Anhäufung von verrauschten Beweisen basieren, beginnend am Ausgangspunkt und endend an einer Entscheidungsschwelle, die mit einer bestimmten Entscheidung verbunden ist.\nDas Modell hat mindestens vier Parameter:\nDie Gesamtzeit für eine Reaktion ist die Zeit für die Ausbreitung vom Startpunkt bis zur Grenze plus die Non-decision time.\nIm Modell wird die Zeit in ganz kleine Schritte \\(\\Delta_t\\) unterteilt (diskrete Zeitschritte). Diese Evidenz wird in einer Entscheidungsvariable (decision variable: dv) gesammelt.\nUm nachzuvollziehen, wie sich eine Entscheidung (in unserem Beispiel: rechts und links) innerhalb eines Trials entwickelt, kann dieser Prozess in R simuliert werden.",
    "crumbs": [
      "Datenmodellierung",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Drift Diffusion Modell</span>"
    ]
  },
  {
    "objectID": "ddm.html#ddm-modell-eines-entscheidungsprozesses",
    "href": "ddm.html#ddm-modell-eines-entscheidungsprozesses",
    "title": "22  Drift Diffusion Modell",
    "section": "",
    "text": "Das Modell wurde von Roger Ratcliff entwickelt. Es hat seinen Ursprung in Modellen zu den Bewegungen von Partikeln in einer Flüssigkeit, und geht auf Arbeiten von Albert Einstein und Norbert Wiener zurück.\n\n\n\nDrift rate steht für die durchschnittliche Anzahl von Beweisen pro Zeiteinheit und ist ein Index für die Schwierigkeit der Aufgabe oder die Fähigkeit des Subjekts.\nBoundary separation stellt die Vorsicht dar; eine größere Trennung der Grenzen führt zu weniger Fehlern (wegen geringerer Auswirkung des Diffusionsrauschens innerhalb des Trials), jedoch um den Preis einer langsameren Reaktion (speed-accuracy tradeoff).\nStarting point repräsentiert die a-priori Präferenz für eine der Wahlalternativen.\nNon-decision time ist ein Verzögerungsparameter, der die Zeit für periphere Prozesse (Kodierung eines Reizes, Umwandlung der Repräsentation des Reizes in eine entscheidungsbezogene Repräsentation) und Ausführung einer Reaktion misst.\n\n\n\n\n\n\n\n\nAnnahmen des DDM\n\n\n\n\n\nDas DDM geht von folgenden Annahmen aus:\n\nBinary decision making: DDM ist ein Model für binäre Entscheidungen. Es gibt also 2 Möglichkeiten zwischen denen entschieden werden muss (in unserem Beispiel: rechts und links).\nContinuous sampling: Es wird davon ausgegangen, dass die Person den Stimulus verarbeitet und über die Zeit Evidenz akkumuliert (sequential sampling). Entscheidungen beruhen demnach auf einem kontinuierlichen Verarbeitung von Daten.\nSingle-stage processing: Entscheidungen basieren auf einer einstufigen Verarbeitung.\nParameter sind konstant. Das heisst z.B. die drift rate kann sich nicht über Zeit verändern.\n\n\n\n\n\n\n\nEntscheidungsprozessCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot() +\n    geom_hline(yintercept = c(-1, 1)) +\n    geom_hline(yintercept = 0, linetype = 3) +\n    labs(x = 'Time', y = 'Evidence (dv)') +\n    scale_y_continuous(breaks = c(-1, 0, 1),\n                       labels = c('left', '0', 'right')) +\n    theme_minimal()",
    "crumbs": [
      "Datenmodellierung",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Drift Diffusion Modell</span>"
    ]
  },
  {
    "objectID": "ddm.html#random-walk",
    "href": "ddm.html#random-walk",
    "title": "22  Drift Diffusion Modell",
    "section": "22.2 Random walk",
    "text": "22.2 Random walk\nEin Random walk ist das Resultat der Aufsummierung von Zufallszahlen. Dies kann in R selbst nachvollzogen werden:\nDazu wird ein Random walk mit 100 Zeitschritten simuliert (mit rnorm()). Es wird mit \\(0\\) begonnen und dann werden 99 normalverteilte Zufallszahlen dazuaddiert, also die kumulierte Summe berechnet (hierfür eignet sich die Funktion cumsum()).\n\nRandom walkCode\n\n\n\n\n\n\nlibrary(gganimate) # library for animation\nset.seed(546)\n\n# 0 + 100 standardnormalverteilte Zufallszahlen\nzufallszahlen_1 = c(0, rnorm(99, 0, 1))\nrandom_walk_1 = cumsum(zufallszahlen_1)\n\nd1 = tibble(t = 1:100,\n            rand_walk = random_walk_1)\n\nd1 |&gt;\n    ggplot(aes(x = t, y = rand_walk)) +\n    geom_step() +\n    geom_hline(yintercept = c(-30, 30)) +\n    geom_hline(yintercept = 0, linetype = 3) +\n    labs(x = 'Time', y = 'Random walk') +\n    scale_y_continuous(breaks = c(-30, 0, 30),\n                       labels = c('left', '0', 'right')) +\n    theme_classic() +\n    transition_reveal(nb) # animate\n\n\n\n\nDie aktuelle decision variable zu Zeitpunkt \\(t\\) als wird hier als normalverteilte Zufallszahl modelliert. Dieser Random walk hat keinen Trend, weil jeder Wert aus einer Normalverteilung mit Mittelwert \\(\\mu=0\\) gezogen wird.",
    "crumbs": [
      "Datenmodellierung",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Drift Diffusion Modell</span>"
    ]
  },
  {
    "objectID": "ddm.html#drift-rate-evidenzakkumulierung-über-die-zeit",
    "href": "ddm.html#drift-rate-evidenzakkumulierung-über-die-zeit",
    "title": "22  Drift Diffusion Modell",
    "section": "22.3 Drift rate: Evidenzakkumulierung über die Zeit",
    "text": "22.3 Drift rate: Evidenzakkumulierung über die Zeit\nWenn stattdessen aus einer Verteilung mit \\(\\mu=0.1\\) gezogen werden würde, ergäbe dies einen positiven Trend über die Zeit hinweg und es würde sich Evidenz für eine Entscheidungsrichtung ansammeln.\nDie driftrate entspricht also dem Mittelwert der Evidenz und sd deren Standardabweichung.\n\n\n\n\n\n\nHands-on: Drift rate\n\n\n\nModellieren Sie die aktuelle decision variable zu Zeitpunkt \\(t\\) als normalverteilte Zufallszahl, bei der die driftrate nicht 0 entspricht.\nVerändern Sie dafür die Werte der Variablen mean_driftrate (positive und negative Werte) und sd_driftrate im Code.\nWas passiert?\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nIst der Mittelwert für die decision variable positiv wird zunehmend Evidenz für die positive Entscheidungsoption gesammelt.\n\ndriftrate = 0.5\nsd = 0.1\n\nEin “Stück” Evidenz ist also meistens positiv:\n\nevidence = rnorm(n = 1, mean = driftrate, sd = sd)\nevidence\n\n[1] 0.4962433\n\n\n\nevidence = rnorm(n = 1, mean = driftrate, sd = sd)\nevidence\n\n[1] 0.4442915\n\n\n\nevidence = rnorm(n = 1, mean = driftrate, sd = sd)\nevidence\n\n[1] 0.7051354\n\n\nDies bedeutet, dass zum Zeitpunkt \\(t\\) die Evidenz ungefähr 0.71 beträgt. Da die Evidenz die durchschnittliche Steigung repräsentiert, wird Evidenz \\(&gt;0\\) dazu führen, dass ein Schritt in Richtung der oberen Grenze gemacht wird. Wäre die Evidenz negativ, wird ein Schritt nach unten gemacht. Da die Evidenz aus einer Normalverteilung gezogen wird, ist es also möglich, dass die Evidenz zufällig negativ wird, obwohl die drift rate, d.h. die Repräsentation der Stimulusstärke, positiv ist.\nWenn dieser Prozess nun über mehrere Zeitschritte hinweg wiederholt wird und die evidence Werte aufsummiert werden, ergibt sich die decision variable. Diese gleicht einem Random walk, hat aber einen Drift in die Richtung der durchschnittlichen Evidenz.\n\nRandom walk mit und ohne DriftCode\n\n\n\n\n\n\nset.seed(546)\n\nt = 100\n\nd3 &lt;- tibble(\n    nb = 1:t,\n    random_walk_neg = cumsum(c(0, rnorm(t-1, -0.3, 1))),\n    random_walk_neutral = random_walk_1,\n    random_walk_pos = cumsum(c(0, rnorm(t-1, 0.3, 1)))\n    ) |&gt;\n    pivot_longer(cols = c(random_walk_neg, random_walk_neutral, random_walk_pos),\n                 names_to = \"name\",\n                 values_to = \"value\")\n\n# aggregate dataset and plot\nd3 |&gt;\n    ggplot(aes(x = nb, y = value, color = name)) +\n    geom_step() +\n    geom_hline(yintercept = c(-30, 30)) +\n    geom_hline(yintercept = 0, linetype = 3) +\n    lims(y = c(-30, 30)) +\n    labs(x = 'Time', y = 'Random walk', color = '') +\n    scale_y_continuous(breaks = c(-30, 0, 30),\n                       labels = c('left', '0', 'right')) +\n    scale_color_manual(labels = c('negative drift', 'no drift', 'positive drift'), values = c(\"tomato4\", \"black\", \"skyblue\")) +\n    theme_classic()\n\n\n\n\n\n22.3.1 Evidenzakkumulierung in R modellieren\nDie Evidenzakkumulierung kann als Iterationen über einzelne Zeitschritte hinweg modelliert werden. In R kann dies mit einem for Loop gemacht werden.\n\ndriftrate = 0.5\nsd = 0.1\n\nn_steps = 10\nevidence = rep(NA, n_steps)\n\ndv = rep(NA, n_steps)\n\ntime_steps = 1:n_steps\n\n# Ersten Wert aus der Verteilung ziehen\nevidence[1] = rnorm(1, mean = driftrate, sd = sd)\ndv[1] = evidence[1]\n\n# Für jeden weitern Zeitpunkt eine Zufallszahl ziehen und zur kumulierten DV addieren\nfor (t in 2:n_steps) {\n    evidence[t] = rnorm(1, mean = driftrate, sd = sd)\n    dv[t] = dv[t-1] + evidence[t]}\n\n\n\n\n\n\n\nHands-on für Interessierte: Funktion erstellen\n\n\n\n\nFunktion erstellenBeispielscode\n\n\n\nErstellen Sie aus dem obigen Code eine custom function:\n\n\nWie soll die Funktion heissen? (-&gt; name)\nWas sind Inputs der Funktion? (-&gt; ())\nWas soll die Funktion tun? (-&gt; {})\n\n\n# Zur Erinnerung die Struktur einer custom Funktion\n\nname = function(){\n    ...\n    ...\n}\n\n\nDer Output der Funktion soll ein data frame sein mit evidence und dv.\n\n\ndata_ddm &lt;- name()\n\n\nMachen Sie eine Abbildung dieser Daten\n\n\nupper = ... # upper boundary\nlower = ... # lower boundary\n\ndata_ddm |&gt;\n    ggplot() +\n    ...\n    ...\n    geom_hline(yintercept = c(upper, loewr)) +\n    geom_hline(yintercept = 0, linetype = 3) +\n    labs(x = 'Time', y = 'Evidence (dv)') +\n    scale_y_continuous(breaks = c(upper, 0, lower),\n                       labels = c('right', '0', 'left')) +\n    theme_classic()\n\n\n\n\nddm_function = function(driftrate = 0.3,\n                sd = 0.5,\n                n_steps = 100){\n    evidence = rep(NA, n_steps)\n    dv = rep(NA, n_steps)\n\n    time_steps = 1:n_steps\n\n    # Ersten Wert aus der Verteilung ziehen\n    evidence[1] = rnorm(1, mean = driftrate, sd = sd)\n    dv[1] = evidence[1]\n\n    # Für jeden weiteren Zeitpunkt eine Zufallszahl ziehen und zur kumulierten DV addieren\n    for (t in 2:n_steps) {\n        evidence[t] = rnorm(1, mean = driftrate, sd = sd)\n        dv[t] = dv[t-1] + evidence[t]}\n    \n    return(tibble(t = 1:n_steps, evidence, dv))\n}\n\n\ndata_ddm &lt;- ddm_function()\n\n\nupper = 20 # upper boundary\nlower = -20 # lower boundary\n\ndata_ddm |&gt;\n    ggplot(aes(x = t, y = dv)) +\n    geom_step() +\n    geom_hline(yintercept = c(upper, lower)) +\n    geom_hline(yintercept = 0, linetype = 3) +\n    labs(x = 'Time', y = 'Evidence (dv)') +\n    scale_y_continuous(breaks = c(upper, 0, lower),\n                       labels = c('right', '0', 'left')) +\n    theme_classic()",
    "crumbs": [
      "Datenmodellierung",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Drift Diffusion Modell</span>"
    ]
  },
  {
    "objectID": "ddm.html#non-decision-time-bias-und-boundaries",
    "href": "ddm.html#non-decision-time-bias-und-boundaries",
    "title": "22  Drift Diffusion Modell",
    "section": "22.4 Non-decision time, bias und boundaries",
    "text": "22.4 Non-decision time, bias und boundaries\nDie Decision Variable dv repräsentiert nun die kumulierten Evidenz, aufgrund dessen das Gehirn eine Entscheidung treffen kann. Wenn der Wert der decision variable grösser als die obere Grenze oder kleiner als die untere Grenze wird, wird die Evidenzakkumulierung abgebrochen, und eine Entscheidung getroffen.\nWir können nun noch die non-decision time und den Anfangspunkt (bias) der Evidenzakkumulierung hinzufügen.\nDie non-decision time beschreibt die Zeit, welche nicht der Evidenzakkumulierung dient. Vor dem Entscheidungsprozess ist das z.B. das Ausrichten des Blicks auf die Aufgabe, nach dem Entscheidungsprozes ist dies z.B. die Ausführung des Tastendrucks.\nDer bias, also der Anfangspunkt ist ein sehr wichtiger Parameter der beeinflusst, ob schon vor dem Entscheidungsprozess zu einer gewissen Antwort tendiert wird (Antworttendenz). Wenn beispielsweise der Anfangspunkt unterhalb der Mitte liegt, braucht es weniger Evidenz um die untere Grenze zu überschreiten und mehr Evidenz für die obere Grenze zu überschreiten.\nDie boundaries beeinflussen, wie viel Evidenz ausreicht, um eine Entscheidung zu treffen. Will man sich ganz sicher sein, sind die boundaries weiter auseinander.\n\nModel ParametersFunction\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nBedeutung\nAnwendung\n\n\n\n\ndrift rate\nQualität der Evidenz pro Zeiteinheit\nTask Schwierigkeit, Fähigkeit\n\n\nbias\nAnfangspunkt der Evidenzakkumulierung\nA priori Präferenz für eine der beiden Alternativen\n\n\nboundary separation\nVorsicht (caution)\nSpeed-Accuracy Trade-off\n\n\nnon-decision time\nVerzögerung\nPeriphere Prozesse\n\n\n\n\n\n\n\n\ndrift_diffusion = function(bias = 0.5, # z\n                           driftrate = 0.8, # v\n                           decision_boundary = 2, # a\n                           ndt = 0.5, # t0\n                           diffvar = 0.1, \n                           dt = 0.001, # t step duration\n                           max_time = 6) {\n    \n    assertthat::assert_that(diffvar &gt; 0)\n    \n    # rescale bias so that 0.5 lies halfway between upper and lower bound\n    bias = as.numeric(2 * decision_boundary * bias - decision_boundary)\n    \n    # initialize time_steps and dv\n    time_steps = max_time/dt\n    dv = array(dim = time_steps)\n    \n    # start accumulating from bias (starting point)\n    dv[1] = rnorm(1, mean = bias, sd = sqrt(dt))\n    \n    for (j in 2:time_steps) {\n        \n        # non-decision time\n        if (j &lt;= ndt/dt) {\n            dv[j] = dv[j-1]\n        }\n        else {\n            error = rnorm(1, 0, sqrt(diffvar * dt))\n            dv[j] = dv[j-1] + driftrate * dt + error  # Cobb & Zacks (1985), Eq. 1.14\n            if (abs(dv[j]) &gt; decision_boundary) {\n                dv[j] = dplyr::if_else(dv[j] &gt; 0,\n                                       min(dv[j], decision_boundary),\n                                       max(dv[j], -decision_boundary))\n                break()\n            }\n        }\n    }\n    d = dplyr::tibble(time = round(seq_along(dv) * dt, 3),\n                      dv = dv,\n                      steps = seq_along(dv),\n                      driftrate = driftrate,\n                      decision_boundary = decision_boundary,\n                      bias = bias,\n                      ndt = ndt)\n    return(d)\n}\n\n\n\n\n\n\n\n\n\n\nHands-on: DDM Parameter\n\n\n\nVerändern Sie im Code die Werte der Variablen\n\nmean_driftrate (positive und negative Werte)\nsd_driftrate\nbias\nboundary\ntimesteps.\n\nWas passiert?\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Datenmodellierung",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Drift Diffusion Modell</span>"
    ]
  },
  {
    "objectID": "ddm.html#auswirkungen-der-parameter",
    "href": "ddm.html#auswirkungen-der-parameter",
    "title": "22  Drift Diffusion Modell",
    "section": "22.5 Auswirkungen der Parameter",
    "text": "22.5 Auswirkungen der Parameter\nUm den Effekt dieser Parameter zu visualisieren, können Trials mit unterschiedlichen Parameterwerten geplottet werden.\n\n22.5.1 Drift rate\nWenn die drift rate viel grösser als \\(0\\) ist, also \\(&gt;&gt; 0\\), wird die obere Entscheidungsgrenze (decision boundary) schnell erreicht. Zudem wird es nur wenige Fehler geben. Ist die drift rate kleiner, aber immer noch \\(&gt; 0\\), wird die durschnittliche Zeit länger, um eine korrekte Antwort zu geben.\n\nHohe vs. tiefe drift rateCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(829)\n\nslow = drift_diffusion(driftrate = 0.8) |&gt; mutate(type = \"slow\")\nfast = drift_diffusion(driftrate = 1.2) |&gt; mutate(type = \"fast\")\n\nfastslow = bind_rows(fast, slow) \n\nfastslow |&gt; \n    ggplot(aes(time, dv, color = type)) +\n    geom_hline(yintercept = 0, linetype = 3) +\n    geom_line() +\n    scale_color_manual(values = c(\"skyblue3\", \"skyblue\")) +\n    geom_hline(yintercept = c(-2, 2)) +\n    theme_classic()\n\n\n\n\n\n\n22.5.2 Bias\nWenn der bias \\(&gt;0.5\\) ist, wird die obere Entscheidungsgrenze schneller erreicht. Hier gibt es nun eine Interaktion mit der drift rate—ist diese klein, und der bias \\(&lt;0.5\\), ist die Chance, schnelle Fehler zu machen erhöht.\n\nStarting point (bias)Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(29)\n\nunbiased = drift_diffusion(bias = 0.5) |&gt; mutate(type = \"unbiased\")\nupbiased = drift_diffusion(bias = 0.7) |&gt; mutate(type = \"upbiased\")\ndownbiased = drift_diffusion(bias = 0.3) |&gt; mutate(type = \"downbiased\")\n\nbias = bind_rows(unbiased, upbiased, downbiased) \n\nbias |&gt; \n    ggplot(aes(time, dv, color = type)) +\n    geom_hline(yintercept = 0, linetype = 3) +\n    geom_line() +\n    scale_color_manual(values = c(\"skyblue\",\"skyblue3\", \"skyblue4\")) +\n    geom_hline(yintercept = c(-2, 2)) +\n    theme_classic()\n\n\n\n\n\n\n22.5.3 Boundary separation\nLiegen die Grenzen weiter auseinander, braucht es mehr akkumulierte Evidenz, um eine der Grenzen zu erreichen. Dies führt dazu, dass weniger Fehler gemacht werden, da die zufällige Fluktuation über längere Zeit hinweg einen weniger starken Einfluss hat. Deshalb kann eine Verschiebung der Grenzen den Speed-Accuracy Trade-off erklären.\n\nDecision boundariesCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(90)\n\ncarefree = drift_diffusion(decision_boundary = 1.6) |&gt; mutate(type = \"carefree\")\ncautious = drift_diffusion(decision_boundary = 2.1) |&gt; mutate(type = \"cautious\")\n\ncautiouscareless = bind_rows(carefree, cautious) \n\ndecision_boundaries = tribble(~type, ~decision_boundary,\n                               \"carefree\", 1.6,\n                               \"cautious\", 2.1)\ncautiouscareless |&gt; \n    ggplot(aes(time, dv, color = type)) +\n    geom_hline(yintercept = 0, linetype = 3) +\n    geom_line() +\n    scale_color_manual(values = c(\"skyblue\", \"skyblue4\")) +\n    geom_hline(aes(yintercept = decision_boundary, color = type), data = decision_boundaries) +\n    geom_hline(aes(yintercept = -decision_boundary, color = type), data = decision_boundaries) +\n    theme_classic()\n\n\n\n\n\n\n22.5.4 Non-decision time\nEine Veränderung der non-decision time hat eine Auswirkung auf die durschnittliche Reaktionszeit, hat aber keinen Einfluss auf die Fehlerrate.\n\nNon-decision timeCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(4534)\n\nlongndt = drift_diffusion(ndt = 0.7) |&gt; mutate(type = \"longndt\")\nshortndt = drift_diffusion(ndt = 0.2) |&gt; mutate(type = \"shortndt\")\n\nndt = bind_rows(longndt, shortndt) \n\nndts = tribble(~type, ~ndt,\n                \"longndt\", 0.7,\n                \"shortndt\", 0.2)\n\nndt |&gt; \n    ggplot(aes(time, dv, color = type)) +\n    geom_hline(yintercept = 0, linetype = 3) +\n    geom_line() +\n    scale_color_manual(values = c(\"skyblue\", \"skyblue4\")) +\n    geom_vline(aes(xintercept = ndt, color = type), data = ndts) +\n    geom_hline(yintercept = c(-2, 2)) +\n    theme_classic()\n\n\n\n\n\n\n\n\n\n\nHands-on: Interpretation der Random Dot DDM Parameter\n\n\n\n\nDDM Random Dot ExperimentCodeDaten\n\n\nWie können die DDM Parameter interpretiert werden?\nBeachten Sie hierzu auch die 95% credible intervals.\n\n\n\n\n\nparameter\nestimate\n95%CrI (lower)\n95%CrI (upper)\n\n\n\n\ndrift rate accuracy\n0.02\n-0.01\n0.05\n\n\ndrift rate speed\n0.03\n0.00\n0.06\n\n\nboundary accuracy\n2.85\n2.81\n2.88\n\n\nboundary speed\n2.46\n2.43\n2.49\n\n\nbias accuracy\n0.50\n0.49\n0.51\n\n\nbias speed\n0.49\n0.48\n0.50\n\n\nndt\n0.04\n0.03\n0.04\n\n\n\n\n\n\n\n\nlibrary(brms)\nlibrary(cmdstanr)\n\nd = read_csv('data/sdt_random_dot_clean.csv') |&gt;\n    mutate(resp = case_match(resp,\n                             'right' ~ 'upper',\n                             'left' ~ 'lower'))\n\n# Vereinfachte Schätzung der DDM Parameter (ohne Einbezug der Messwiederholung!)\nfit = brm(bf(rt | dec(resp) ~ 0 + condition,\n             bias ~ 0 + condition,\n             bs ~ 0 + condition\n           #ndt ~ 0 + condition\n),\n         # inits = 0.1,\n          data = d,\n          family = wiener(link_bs = \"identity\",\n                          link_ndt = \"identity\",\n                          link_bias = \"identity\"),\n          cores = parallel::detectCores(),\n          chains = 4,\n          backend = \"cmdstanr\")\n\n\n\nDie Daten stammen aus dem Random Dot Experiment FS25.\nDatensatz",
    "crumbs": [
      "Datenmodellierung",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Drift Diffusion Modell</span>"
    ]
  },
  {
    "objectID": "ddm.html#diffusions-modell-in-der-forschung",
    "href": "ddm.html#diffusions-modell-in-der-forschung",
    "title": "22  Drift Diffusion Modell",
    "section": "22.6 Diffusions Modell in der Forschung",
    "text": "22.6 Diffusions Modell in der Forschung\nWeil mit dem Diffusionsmodell verschiedene Aspekte des Entscheidungsprozesses spezifisch modelliert und unterschieden werden können, wird dieses Modell häufig in der Forschung verwendet. So können detaillierte Einsichten in den Entscheidungsprozess gewonnen werden. Hier ein paar Beispiele:\n\nUntersuchung der kognitiven Eigenschaften bei ADHS Review\nUntersuchung des Entscheidungsverhaltens im Zusammenhang mit Abhängigkeit (Tabak, Alkohol und Glücksspiel)\nUntersuchung des Entscheidungsverhaltens im Zusammenhang mit Depression, und Angst.\nUntersuchung von verändertem Entscheidungsverhalten aufgrund von strukturellen oder funktionalen Veränderungen des Gehirns z.B. bei Parkinson.",
    "crumbs": [
      "Datenmodellierung",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Drift Diffusion Modell</span>"
    ]
  },
  {
    "objectID": "ddm.html#footnotes",
    "href": "ddm.html#footnotes",
    "title": "22  Drift Diffusion Modell",
    "section": "",
    "text": "Bogacz, Rafal; Eric Brown, Jeff Moehlis, Philip Holmes, Jonathan D. Cohen (2006). “The Physics of Optimal Decision Making: A Formal Analysis of Models of Performance in Two-Alternative Forced-Choice Tasks”. Psychological Review. 113 (4): 700–765. https://doi.org/10.1037/0033-295X.113.4.700↩︎",
    "crumbs": [
      "Datenmodellierung",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Drift Diffusion Modell</span>"
    ]
  },
  {
    "objectID": "webrconsole.html",
    "href": "webrconsole.html",
    "title": "WebR Konsole",
    "section": "",
    "text": "In der WebR-Konsole können Sie R-Code ausführen. Erstellte Variablen werden gespeichert, so lange das Browserfenster nicht geschlossen wird.\n\n Konsole Tipp Lösung\n\n\nGeben Sie hier Code ein.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPackages\nLaden Sie zuerst das {tidyverse} mit library(tidyverse).\nDatensätze\nEs stehen Ihnen folgende Datensätze zur Verfügung:\n\ncars\niris\n\nEs können weitere Datensätze durch das Laden von Packages genutzt werden:\n\npenguins aus {palmerpenguins}\n\n\n# Laden vom penguins-Datensatz aus dem {palmerpenguins} Package\nlibrary(palmerpenguins)\nd &lt;- penguins\n\n\n\n\nlibrary(tidyverse)\n\nglimpse(cars)\n\nRows: 50\nColumns: 2\n$ speed &lt;dbl&gt; 4, 4, 7, 7, 8, 9, 10, 10, 10, 11, 11, 12, 12, 12, 12, 13, 13, 13…\n$ dist  &lt;dbl&gt; 2, 10, 4, 22, 16, 10, 18, 26, 34, 17, 28, 14, 20, 24, 28, 26, 34…\n\nplot(cars)",
    "crumbs": [
      "Anhang",
      "WebR Konsole"
    ]
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License and Authorship",
    "section": "",
    "text": "The complab course was initially conceptualized and written by Andrew Ellis (2021-2023), extended by Gerda Wyssen (2021-2025), Daniel Fitze (2024), and Enea Weber (2025). The course experiments (Complab 2024-2025) and their descriptions were created by Rebekka Borer.\nPrevious course material:\n\nComplab 2021\n\nComplab 2022\nComplab 2023\nComplab 2024\n\nWe thank all current and previous students who gave valuable feedback on the course and material!\nCite as: Wyssen, G., Ellis, A., Weber, E., Fitze, D. (2025). Neurowissenschaft im Computerlab. Skript FS 2025. https://kogpsy.github.io/neuroscicomplabFS25.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.",
    "crumbs": [
      "Anhang",
      "License and Authorship"
    ]
  }
]